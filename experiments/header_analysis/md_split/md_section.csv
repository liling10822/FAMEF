,Name,Header,Content,Level
0,GPRPy, GPRPy,,1
1,GPRPy, Simplemost installation,"Open-source Ground Penetrating Radar processing and visualization software.

![Profile GUI](profileGUI.png)

![CMP/WARR GUI](CWGUI.png)


",2
2,GPRPy, Running the software,"**In the following instructions, if you use Windows, use the comands `python` and `pip`. If you use Mac or Linux, use the commands `python3` and `pip3` instead.**

1) Download the GPRPy software from 
   [https://github.com/NSGeophysics/GPRPy/archive/master.zip](https://github.com/NSGeophysics/GPRPy/archive/master.zip). <br/>
   Save the file somewhere on your computer and extract the zip folder. <br/>
   As an **alternative**, you can install git from [https://git-scm.com/](https://git-scm.com/), then run in a command prompt:<br/>
   `git clone https://github.com/NSGeophysics/GPRPy.git`<br/>
   The advantage of the latter is that you can easily update your software by running from the GPRPy folder in a command prompt:<br/>
   `git pull origin master`

2) Install Python 3.7 for example from [https://conda.io/miniconda.html](https://conda.io/miniconda.html)

3) Once the installation finished, open a command prompt that can run Python <br/>
   On Windows: click on Start, then enter ""Anaconda Prompt"", without the quotation marks into the ""Search programs and files"" field. On Mac or Linux, open the regular terminal.

4) In the command prompt, change to the directory  where you downloaded the GPRPy files.
   This is usually through a command like for example<br/>
   `cd Desktop\GPRPy`<br/>
   if you downloaded GPRPy directly onto your desktop. Then type the following and press enter afterward:<br/>
   `python installMigration.py`<br/>
   Then type the following and press enter afterward:<br/>
   `pip install .`<br/>
   **don't forget the period ""."" at the end of the `pip install` command**


",2
3,GPRPy, Running automatically generated scripts,"After installation, you can run the script from the Anaconda Prompt (or your Python-enabled prompt) by running either<br/>
`gprpy`<br/>
or<br/>
`python -m gprpy`

The first time you run GPRPy it could take a while to initialize. GPRPy will ask you if you want to run the profile [p] or WARR / CMP [c] user interface. Type<br/>
`p`<br/>
and then enter for profile, or<br/>
`c`<br/>
and then enter for CMP / WARR.

You can also directly select one by running either<br/>
`gprpy p`<br/>
or<br/>
`gprpy c`<br/>
or<br/>
`python -m gprpy p`<br/>
or<br/>
`python -m gprpy c`


",2
4,GPRPy, In case of trouble,"To run automatically generated scripts, open the command prompt that can run python (for example Anaconda Prompt), switch to the folder with the automatically generated script and run<br/>
`python myscriptname.py`<br/>
where myscriptname.py is the name of your automatically generated script.  


",2
5,GPRPy, Uninstalling GPRPy,"If you have several versions of python installed, for example on a Mac or Linux system, replace, in the commands shown earlier,
`python` with `python3`<br/>
and<br/>
`pip` with `pip3`

If you have any troubles getting the software running, please send me an email or open an issue on GitHub and I will help you getting it running.


",2
6,GPRPy, News,"To uninstall GPRPy, simply run, in the (Anaconda) command prompt<br/>
`pip uninstall gprpy`

",2
7,SRN-Deblur, Scale-recurrent Network for Deep Image Deblurring,,1
8,SRN-Deblur, Our results on real data,"by [Xin Tao](http://www.xtao.website), Hongyun Gao, [Xiaoyong Shen](http://xiaoyongshen.me/), [Jue Wang](http://juew.org), [Jiaya Jia](http://www.cse.cuhk.edu.hk/leojia/). ([pdf](http://www.xtao.website/projects/srndeblur/srndeblur_cvpr18.pdf))

",3
9,SRN-Deblur, Results on the testing dataset,"<img src=""./imgs/teaser.jpg"" width=""100%"" alt=""Real Photo"">

",3
10,SRN-Deblur, More cases on real photos from previous papers:,"<img src=""./imgs/comp_sota.jpg"" width=""100%"" alt=""Testing Dataset"">

",3
11,SRN-Deblur, Prerequisites,"<img src=""./imgs/comp_real.jpg"" width=""100%"" alt=""More Cases"">

",2
12,SRN-Deblur, Installation,"- Python2.7
- Scipy
- Scikit-image
- numpy
- Tensorflow 1.4 with NVIDIA GPU or CPU (cpu testing is very slow)

",2
13,SRN-Deblur, Testing,"Clone this project to your machine. 

```bash
git clone https://github.com/jiangsutx/SRN-Deblur.git
cd SRN-Deblur
```

",2
14,SRN-Deblur, Evaluation,"Download pretrained models through: `download_model.sh` inside `checkpoints/`.

To test blur images in a folder, just use arguments 
`--input_path=<TEST_FOLDER>` and save the outputs to `--output_path=<OUTPUT_FOLDER>`.
For example:

```bash
python run_model.py --input_path=./testing_set --output_path=./testing_res
```

If you have a GPU, please include `--gpu` argument, and add your gpu id to your command. 
Otherwise, use `--gpu=-1` for CPU. 

```bash
python run_model.py --gpu=0
```

To test the model, pre-defined height and width of tensorflow 
placeholder should be assigned. 
Our network requires the height and width be multiples of `16`. 
When the gpu memory is enough, the height and width could be assigned to 
the maximum to accommodate all the images. 

Otherwise, the images will be downsampled by the largest scale factor to 
be fed into the placeholder. And results will be upsampled to the original size.

According to our experience, `--height=720` and `--width=1280` work well 
on a Gefore GTX 1050 TI with 4GB memory. For example, 

```bash
python run_model.py --height=720 --width=1280
```

",3
15,SRN-Deblur, Training,"The quantitative results of **PSNR** and **SSIM** in the paper is 
calculated using MATLAB built-in function `psnr()` and `ssim()` based 
on the generated color results.

",2
16,SRN-Deblur, Models,"We trained our model using the dataset from 
[DeepDeblur_release](https://github.com/SeungjunNah/DeepDeblur_release). 
Please put the dataset into `training_set/`. And the provided `datalist_gopro.txt` 
can be used to train the model. 

Hyper parameters such as batch size, learning rate, epoch number can be tuned through command line:

```bash
python run_model.py --phase=train --batch=16 --lr=1e-4 --epoch=4000
```

",2
17,SRN-Deblur, How to choose,"We provided 3 models (training settings) for testing:
1. `--model=lstm`: This model implements exactly the same structure in our paper.
Current released model weights should produce `PSNR=30.19, SSIM=0.9334` on GOPRO testing dataset.
2. `--model=gray`: According to our further experiments after paper acceptance, we are able
to get a slightly better model by tuning parameters, even without LSTM. 
This model should produce visually sharper and quantitatively better results. 
3. `--model=color`: Previous models are trained on gray images, and may produce color
ringing artifacts. So we train a model directly based on RGB images. 
This model keeps better color consistency, but the results are less sharp.
",3
18,SRN-Deblur, Reference,"If you would like to compare performance against our method, you can use 
model `gray` and `lstm`. 
If you want to restore blurry images you can try `gray` and `color`. 
And `color` is very useful in low-light noisy images.  

",2
19,SRN-Deblur, Contact,"If you use any part of our code, or SRN-Deblur is useful for your research, please consider citing: 

```bibtex
@inproceedings{tao2018srndeblur,
  title={Scale-recurrent Network for Deep Image Deblurring},
  author={Tao, Xin and Gao, Hongyun and Shen, Xiaoyong and Wang, Jue and Jia, Jiaya},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
}
```

",2
20,SRN-Deblur, Reference,"We are glad to hear if you have any suggestions and questions.

Please send email to xtao@cse.cuhk.edu.hk

",2
21,sentinelsat, connect to the API,"===========

.. image:: https://badge.fury.io/py/sentinelsat.svg
    :target: http://badge.fury.io/py/sentinelsat
    :alt: PyPI package

.. image:: https://travis-ci.com/sentinelsat/sentinelsat.svg?branch=master
    :target: https://travis-ci.com/sentinelsat/sentinelsat
    :alt: Travis-CI

.. image:: https://codecov.io/gh/sentinelsat/sentinelsat/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/sentinelsat/sentinelsat
    :alt: codecov.io code coverage

.. image:: https://readthedocs.org/projects/sentinelsat/badge/?version=stable
    :target: http://sentinelsat.readthedocs.io/en/stable/?badge=stable
    :alt: Documentation

.. image:: https://img.shields.io/badge/gitter-join_chat-1dce73.svg?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4NCjxzdmcgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB4PSIwIiB5PSI1IiBmaWxsPSIjZmZmIiB3aWR0aD0iMSIgaGVpZ2h0PSI1Ii8%2BPHJlY3QgeD0iMiIgeT0iNiIgZmlsbD0iI2ZmZiIgd2lkdGg9IjEiIGhlaWdodD0iNyIvPjxyZWN0IHg9IjQiIHk9IjYiIGZpbGw9IiNmZmYiIHdpZHRoPSIxIiBoZWlnaHQ9IjciLz48cmVjdCB4PSI2IiB5PSI2IiBmaWxsPSIjZmZmIiB3aWR0aD0iMSIgaGVpZ2h0PSI0Ii8%2BPC9zdmc%2B&logoWidth=8
    :target: https://gitter.im/sentinelsat/
    :alt: gitter.im chat

.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.595961.svg
   :target: https://doi.org/10.5281/zenodo.595961
   :alt: Zenodo DOI

Sentinelsat makes searching, downloading and retrieving the metadata of `Sentinel
<http://www.esa.int/Our_Activities/Observing_the_Earth/Copernicus/Overview4>`_
satellite images from the
`Copernicus Open Access Hub <https://scihub.copernicus.eu/>`_ easy.

It offers an easy-to-use command line interface

.. code-block:: bash

  sentinelsat -u <user> -p <password> -g <search_polygon.geojson> --sentinel 2 --cloud 30

and a powerful Python API.

.. code-block:: python

  from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt

  api = SentinelAPI('user', 'password')
  footprint = geojson_to_wkt(read_geojson('search_polygon.geojson'))
  products = api.query(footprint,
                       producttype='SLC',
                       orbitdirection='ASCENDING')
  api.download_all(products)



Documentation is published at http://sentinelsat.readthedocs.io/.

Installation
============

Install ``sentinelsat`` through pip:

.. code-block:: bash

    pip install sentinelsat

Usage
=====

Sentinelsat provides a Python API and a command line interface to search,
download and retrieve the metadata for Sentinel products.

Python Library
--------------

.. code-block:: python

  from sentinelsat.sentinel import SentinelAPI, read_geojson, geojson_to_wkt
  from datetime import date

  ",1
22,sentinelsat, download single scene by known product id,"  api = SentinelAPI('user', 'password', 'https://scihub.copernicus.eu/dhus')

  ",1
23,sentinelsat," search by polygon, time, and Hub query keywords","  api.download(<product_id>)

  ",1
24,sentinelsat, download all results from the search,"  footprint = geojson_to_wkt(read_geojson('map.geojson'))
  products = api.query(footprint,
                       date = ('20151219', date(2015, 12, 29)),
                       platformname = 'Sentinel-2',
                       cloudcoverpercentage = (0, 30))

  ",1
25,sentinelsat, GeoJSON FeatureCollection containing footprints and metadata of the scenes,"  api.download_all(products)

  ",1
26,sentinelsat, GeoPandas GeoDataFrame with the metadata of the scenes and the footprints as geometries,"  api.to_geojson(products)

  ",1
27,sentinelsat," Get basic information about the product: its title, file size, MD5 sum, date, footprint and","  api.to_geodataframe(products)

  ",1
28,sentinelsat, its download url,  ,1
29,sentinelsat, Get the product's full metadata available on the server,"  api.get_product_odata(<product_id>)

  ",1
30,sentinelsat, same result as query above,"  api.get_product_odata(<product_id>, full=True)

Valid search query keywords can be found at the `Copernicus Open Access Hub documentation
<https://scihub.copernicus.eu/userguide/3FullTextSearch>`_.

Command Line Interface
----------------------

A basic search query consists of a search area geometry as well as the username and
password to access the Copernicus Open Access Hub.

.. code-block:: bash

  sentinelsat -u <user> -p <password> -g <geojson>

Search areas are provided as GeoJSON files, which can be created with
`QGIS <http://qgis.org/en/site/>`_ or `geojson.io <http://geojson.io>`_.
If you do not specify a start and end date only products published in the last
24 hours will be queried.

Example
^^^^^^^

Search and download all Sentinel-1 scenes of type SLC, in descending
orbit, for the year 2015.

.. code-block:: bash

  sentinelsat -u <user> -p <password> -g <search_polygon.geojson> -s 20150101 -e 20151231 -d \
  --producttype SLC -q ""orbitdirection=Descending"" \
  --url ""https://scihub.copernicus.eu/dhus""

Username, password and DHuS URL can also be set via environment variables for convenience.

.. code-block:: bash
 
  ",1
31,RDN, Residual Dense Network for Image Super-Resolution,,1
32,RDN, Contents,"This repository is for RDN introduced in the following paper

[Yulun Zhang](http://yulunzhang.com/), [Yapeng Tian](http://yapengtian.org/), [Yu Kong](http://www1.ece.neu.edu/~yukong/), [Bineng Zhong](https://scholar.google.de/citations?user=hvRBydsAAAAJ&hl=en), and [Yun Fu](http://www1.ece.neu.edu/~yunfu/), ""Residual Dense Network for Image Super-Resolution"", CVPR 2018 (spotlight), [[arXiv]](https://arxiv.org/abs/1802.08797) 

[Yulun Zhang](http://yulunzhang.com/), [Yapeng Tian](http://yapengtian.org/), [Yu Kong](http://www1.ece.neu.edu/~yukong/), [Bineng Zhong](https://scholar.google.de/citations?user=hvRBydsAAAAJ&hl=en), and [Yun Fu](http://www1.ece.neu.edu/~yunfu/), ""Residual Dense Network for Image Restoration"", arXiv 2018, [[arXiv]](https://arxiv.org/abs/1812.10477) 


The code is built on [EDSR (Torch)](https://github.com/LimBee/NTIRE2017) and tested on Ubuntu 14.04 environment (Torch7, CUDA8.0, cuDNN5.1) with Titan X/1080Ti/Xp GPUs.

Other implementations: [PyTorch_version](https://github.com/thstkdgus35/EDSR-PyTorch) has been implemented by Nguyễn Trần Toàn (trantoan060689@gmail.com) and merged into [EDSR_PyTorch](https://github.com/thstkdgus35/EDSR-PyTorch). [TensorFlow_version](https://github.com/hengchuan/RDN-TensorFlow) by hengchuan.

",2
33,RDN, Introduction,"1. [Introduction](#introduction)
2. [Train](#train)
3. [Test](#test)
4. [Results](#results)
5. [Citation](#citation)
6. [Acknowledgements](#acknowledgements)

",2
34,RDN, Train,"A very deep convolutional neural network (CNN) has recently achieved great success for image super-resolution (SR) and offered hierarchical features as well. However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in image SR. We fully exploit the hierarchical features from all the convolutional layers. Speciﬁcally, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Experiments on benchmark datasets with different degradation models show that our RDN achieves favorable performance against state-of-the-art methods.

![RDB](/Figs/RDB.png)
Figure 1. Residual dense block (RDB) architecture.
![RDN](/Figs/RDN.png)
Figure 2. The architecture of our proposed residual dense network (RDN).

",2
35,RDN, Prepare training data ,,3
36,RDN, Begin to train,"1. Download DIV2K training data (800 training + 100 validtion images) from [DIV2K dataset](https://data.vision.ee.ethz.ch/cvl/DIV2K/) or [SNU_CVLab](https://cv.snu.ac.kr/research/EDSR/DIV2K.tar).

2. Place all the HR images in 'Prepare_TrainData/DIV2K/DIV2K_HR'.

3. Run 'Prepare_TrainData_HR_LR_BI/BD/DN.m' in matlab to generate LR images for BI, BD, and DN models respectively.

4. Run 'th png_to_t7.lua' to convert each .png image to .t7 file in new folder 'DIV2K_decoded'.

5. Specify the path of 'DIV2K_decoded' to '-datadir' in 'RDN_TrainCode/code/opts.lua'.

For more informaiton, please refer to [EDSR(Torch)](https://github.com/LimBee/NTIRE2017).

",3
37,RDN, Test,"1. (optional) Download models for our paper and place them in '/RDN_TrainCode/experiment/model'.

    All the models can be downloaded from [Dropbox](https://www.dropbox.com/sh/ngcvqdas167gol2/AAAdJe9w6s2fpo_KEGZe7d4Ra?dl=0) or [Baidu](https://pan.baidu.com/s/116FAzKnaJnAdxY_B6ENp_A).

2. Cd to 'RDN_TrainCode/code', run the following scripts to train models.

    **You can use scripts in file 'TrainRDN_scripts' to train models for our paper.**

    ```bash
    #notes: BI, scale 2, 3, 4
    #notes: BIX2F64D18C6G64P48, input=48x48, output=96x96
    th main.lua -scale 2 -netType RDN -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true

    #notes: BIX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX2.t7
    th main.lua -scale 3 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true  -preTrained ../experiment/model/RDN_BIX2.t7

    #notes: BIX4F64D18C6G64P32, input=32x32, output=128x128, fine-tune on RDN_BIX2.t7
    th main.lua -scale 4 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 128 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true -nEpochs 1000 -preTrained ../experiment/model/RDN_BIX2.t7 

    #notes: BD, scale 3
    #notes: BDX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX3.t7
    th main.lua -scale 3 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BD -splitBatch 4 -trainOnly true -nEpochs 200 -preTrained ../experiment/model/RDN_BIX3.t7

    #notes: DN, scale 3
    #notes: DNX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX3.t7
    th main.lua -scale 3 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel DN -splitBatch 4 -trainOnly true  -nEpochs 200 -preTrained ../experiment/model/RDN_BIX3.t7
    ```
    Only RDN_BIX2.t7 was trained using 48x48 input patches. All other models were trained using 32x32 input patches in order to save training time.
    However, smaller input patch size in training would lower the performance to some degree. We also set '-trainOnly true' to save GPU memory.
",2
38,RDN, Quick start,,3
39,RDN, The whole test pipeline,"1. Download models for our paper and place them in '/RDN_TestCode/model'.

    All the models can be downloaded from [Dropbox](https://www.dropbox.com/sh/ngcvqdas167gol2/AAAdJe9w6s2fpo_KEGZe7d4Ra?dl=0) or [Baidu](https://pan.baidu.com/s/116FAzKnaJnAdxY_B6ENp_A).

2. Run 'TestRDN.lua'

    **You can use scripts in file 'TestRDN_scripts' to produce results for our paper.**

    ```bash
    #notes: No self-ensemble: RDN
    #notes: BI degradation model, X2, X3, X4
    th TestRDN.lua -model RDN_BIX2 -degradation BI -scale 2 -selfEnsemble false -dataset Set5
    th TestRDN.lua -model RDN_BIX3 -degradation BI -scale 3 -selfEnsemble false -dataset Set5
    th TestRDN.lua -model RDN_BIX4 -degradation BI -scale 4 -selfEnsemble false -dataset Set5
    #notes: BD degradation model, X3
    th TestRDN.lua -model RDN_BDX3 -degradation BD -scale 3 -selfEnsemble false -dataset Set5
    #notes: DN degradation model, X3
    th TestRDN.lua -model RDN_DNX3 -degradation DN -scale 3 -selfEnsemble false -dataset Set5


    #notes: With self-ensemble: RDN+
    #notes: BI degradation model, X2, X3, X4
    th TestRDN.lua -model RDN_BIX2 -degradation BI -scale 2 -selfEnsemble true -dataset Set5
    th TestRDN.lua -model RDN_BIX3 -degradation BI -scale 3 -selfEnsemble true -dataset Set5
    th TestRDN.lua -model RDN_BIX4 -degradation BI -scale 4 -selfEnsemble true -dataset Set5
    #notes: BD degradation model, X3
    th TestRDN.lua -model RDN_BDX3 -degradation BD -scale 3 -selfEnsemble true -dataset Set5
    #notes: DN degradation model, X3
    th TestRDN.lua -model RDN_DNX3 -degradation DN -scale 3 -selfEnsemble true -dataset Set5
    ```

",3
40,RDN, Results,"1. Prepare test data.

    Place the original test sets (e.g., Set5, other test sets are available from [GoogleDrive](https://drive.google.com/drive/folders/1xyiuTr6ga6ni-yfTP7kyPHRmfBakWovo?usp=sharing) or [Baidu](https://pan.baidu.com/s/1yBI_-rknXT2lm1UAAB_bag)) in 'OriginalTestData'.

    Run 'Prepare_TestData_HR_LR.m' in Matlab to generate HR/LR images with different degradation models.
2. Conduct image SR. 

    See **Quick start**
3. Evaluate the results.

    Run 'Evaluate_PSNR_SSIM.m' to obtain PSNR/SSIM values for paper.



",2
41,RDN, Citation,"![PSNR_SSIM_BI](/Figs/PSNR_SSIM_BI.png)
Table 1. Benchmark results with BI degradation model. Average PSNR/SSIM values for scaling factor ×2, ×3, and ×4.

![PSNR_SSIM_BD_DN](/Figs/PSNR_SSIM_BD_DN.png)
Table 2. Benchmark results with BD and DN degradation models. Average PSNR/SSIM values for scaling factor ×3.

",2
42,RDN, Acknowledgements,"If you find the code helpful in your resarch or work, please cite the following papers.
```
@InProceedings{Lim_2017_CVPR_Workshops,
  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  month = {July},
  year = {2017}
}

@inproceedings{zhang2018residual,
    title={Residual Dense Network for Image Super-Resolution},
    author={Zhang, Yulun and Tian, Yapeng and Kong, Yu and Zhong, Bineng and Fu, Yun},
    booktitle={CVPR},
    year={2018}
}

@article{zhang2018rdnir,
    title={Residual Dense Network for Image Restoration},
    author={Zhang, Yulun and Tian, Yapeng and Kong, Yu and Zhong, Bineng and Fu, Yun},
    booktitle={arXiv},
    year={2018}
}

```
",2
43,Fiona," Open a file for reading. We'll call this the ""source.""","Fiona
=====

Fiona is OGR's neat and nimble API for Python programmers.

.. image:: https://travis-ci.org/Toblerity/Fiona.png?branch=master
   :target: https://travis-ci.org/Toblerity/Fiona

.. image:: https://ci.appveyor.com/api/projects/status/github/Toblerity/Fiona?svg=true
   :target: https://ci.appveyor.com/project/sgillies/fiona/branch/master

.. image:: https://coveralls.io/repos/Toblerity/Fiona/badge.png
   :target: https://coveralls.io/r/Toblerity/Fiona

Fiona is designed to be simple and dependable. It focuses on reading and
writing data in standard Python IO style and relies upon familiar Python types
and protocols such as files, dictionaries, mappings, and iterators instead of
classes specific to OGR. Fiona can read and write real-world data using
multi-layered GIS formats and zipped virtual file systems and integrates
readily with other Python GIS packages such as pyproj_, Rtree_, and Shapely_.
Fiona is supported only on CPython versions 2.7 and 3.4+.

For more details, see:

* Fiona `home page <https://github.com/Toblerity/Fiona>`__
* `Docs and manual <https://fiona.readthedocs.io/>`__
* `Examples <https://github.com/Toblerity/Fiona/tree/master/examples>`__
* Main `user discussion group <https://fiona.groups.io/g/main>`__
* `Developers discussion group <https://fiona.groups.io/g/dev>`__

Usage
=====

Collections
-----------

Records are read from and written to ``file``-like `Collection` objects
returned from the ``fiona.open()`` function.  Records are mappings modeled on
the GeoJSON format. They don't have any spatial methods of their own, so if you
want to do anything fancy with them you will probably need Shapely or something
like it. Here is an example of using Fiona to read some records from one data
file, change their geometry attributes, and write them to a new data file.

.. code-block:: python

    import fiona

    ",1
44,Fiona," The file we'll write to, the ""destination"", must be initialized","    with fiona.open('tests/data/coutwildrnp.shp') as src:

        ",1
45,Fiona," with a coordinate system, a format driver name, and",        ,1
46,Fiona, a record schema.  We can get initial values from the open,        ,1
47,Fiona, collection's ``meta`` property and then modify them as,        ,1
48,Fiona, desired.,        ,1
49,Fiona," Open an output file, using the same format driver and","        meta = src.meta
        meta['schema']['geometry'] = 'Point'

        ",1
50,Fiona, coordinate reference system as the source. The ``meta``,        ,1
51,Fiona, mapping fills in the keyword parameters of fiona.open().,        ,1
52,Fiona, Process only the records intersecting a box.,"        with fiona.open('test_write.shp', 'w', **meta) as dst:

            ",1
53,Fiona, Get a point on the boundary of the record's,"            for f in src.filter(bbox=(-107.0, 37.0, -105.0, 39.0)):

                ",1
54,Fiona, geometry.,                ,1
55,Fiona, Write the record out.,"                f['geometry'] = {
                    'type': 'Point',
                    'coordinates': f['geometry']['coordinates'][0][0]}

                ",1
56,Fiona, The destination's contents are flushed to disk and the file is,"                dst.write(f)

    ",1
57,Fiona, closed when its ``with`` block ends. This effectively,    ,1
58,Fiona, executes ``dst.flush(); dst.close()``.,    ,1
59,Fiona, Output:,"Reading Multilayer data
-----------------------

Collections can also be made from single layers within multilayer files or
directories of data. The target layer is specified by name or by its integer
index within the file or directory. The ``fiona.listlayers()`` function
provides an index ordered list of layer names.

.. code-block:: python

    for layername in fiona.listlayers('tests/data'):
        with fiona.open('tests/data', layer=layername) as src:
            print(layername, len(src))

    ",1
60,Fiona," (u'coutwildrnp', 67)",    ,1
61,Fiona, Output:,"Layer can also be specified by index. In this case, ``layer=0`` and
``layer='test_uk'`` specify the same layer in the data file or directory.

.. code-block:: python

    for i, layername in enumerate(fiona.listlayers('tests/data')):
        with fiona.open('tests/data', layer=i) as src:
            print(i, layername, len(src))

    ",1
62,Fiona," (0, u'coutwildrnp', 67)",    ,1
63,Fiona, Output:,"Writing Multilayer data
-----------------------

Multilayer data can be written as well. Layers must be specified by name when
writing.

.. code-block:: python

    with open('tests/data/cowildrnp.shp') as src:
        meta = src.meta
        f = next(src)

    with fiona.open('/tmp/foo', 'w', layer='bar', **meta) as dst:
        dst.write(f)

    print(fiona.listlayers('/tmp/foo'))

    with fiona.open('/tmp/foo', layer='bar') as src:
        print(len(src))
        f = next(src)
        print(f['geometry']['type'])
        print(f['properties'])

        ",1
64,Fiona, [u'bar'],        ,1
65,Fiona, 1,        ,1
66,Fiona, Polygon,        ,1
67,Fiona," OrderedDict([(u'PERIMETER', 1.22107), (u'FEATURE2', None), (u'NAME', u'Mount Naomi Wilderness'), (u'FEATURE1', u'Wilderness'), (u'URL', u'http://www.wilderness.net/index.cfm?fuse=NWPS&sec=wildView&wname=Mount%20Naomi'), (u'AGBUR', u'FS'), (u'AREA', 0.0179264), (u'STATE_FIPS', u'49'), (u'WILDRNP020', 332), (u'STATE', u'UT')])",        ,1
68,Fiona, Output:,"A view of the /tmp/foo directory will confirm the creation of the new files.

.. code-block:: console

    $ ls /tmp/foo
    bar.cpg bar.dbf bar.prj bar.shp bar.shx

Collections from archives and virtual file systems
--------------------------------------------------

Zip and Tar archives can be treated as virtual filesystems and Collections can
be made from paths and layers within them. In other words, Fiona lets you read
and write zipped Shapefiles.

.. code-block:: python

    for i, layername in enumerate(
            fiona.listlayers('zip://tests/data/coutwildrnp.zip'):
        with fiona.open('zip://tests/data/coutwildrnp.zip', layer=i) as src:
            print(i, layername, len(src))

    ",1
69,Fiona," (0, u'coutwildrnp', 67)",    ,1
70,Fiona, Output:,"Fiona can also read from more exotic file systems. For instance, a
zipped shape file in S3 can be accessed like so:

.. code-block:: python

   with fiona.open('zip+s3://mapbox/rasterio/coutwildrnp.zip') as src:
       print(len(src))

   ",1
71,Fiona, 67,   ,1
72,hmr, End-to-end Recovery of Human Shape and Pose,,1
73,hmr, Requirements,"Angjoo Kanazawa, Michael J. Black, David W. Jacobs, Jitendra Malik
CVPR 2018

[Project Page](https://akanazawa.github.io/hmr/)
![Teaser Image](https://akanazawa.github.io/hmr/resources/images/teaser.png)

",3
74,hmr, Installation,"- Python 2.7
- [TensorFlow](https://www.tensorflow.org/) tested on version 1.3, demo alone runs with TF 1.12

",3
75,hmr, Linux Setup with virtualenv,,4
76,hmr, Install TensorFlow,"```
virtualenv venv_hmr
source venv_hmr/bin/activate
pip install -U pip
deactivate
source venv_hmr/bin/activate
pip install -r requirements.txt
```
",4
77,hmr, Windows Setup with python 3 and Anaconda,"With GPU:
```
pip install tensorflow-gpu==1.3.0
```
Without GPU:
```
pip install tensorflow==1.3.0
```

",3
78,hmr, if you need to get chumpy ,"This is only partialy tested.
```
conda env create -f hmr.yml
```
",4
79,hmr, Demo,"https://github.com/mattloper/chumpy/tree/db6eaf8c93eb5ae571eb054575fb6ecec62fd86d


",3
80,hmr, Webcam Demo (thanks @JulesDoe!),"1. Download the pre-trained models
```
wget https://people.eecs.berkeley.edu/~kanazawa/cachedir/hmr/models.tar.gz && tar -xf models.tar.gz
```

2. Run the demo
```
python -m demo --img_path data/coco1.png
python -m demo --img_path data/im1954.jpg
```

Images should be tightly cropped, where the height of the person is roughly 150px.
On images that are not tightly cropped, you can run
[openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) and supply
its output json (run it with `--write_json` option).
When json_path is specified, the demo will compute the right scale and bbox center to run HMR:
```
python -m demo --img_path data/random.jpg --json_path data/random_keypoints.json
```
(The demo only runs on the most confident bounding box, see `src/util/openpose.py:get_bbox`)

",3
81,hmr, Training code/data,"1. Download pre-trained models like above.
2. Run webcam Demo
2. Run the demo
```
python -m demo --img_path data/coco1.png
python -m demo --img_path data/im1954.jpg
```

",3
82,hmr, Citation,"Please see the [doc/train.md](https://github.com/akanazawa/hmr/blob/master/doc/train.md)!

",3
83,hmr, Opensource contributions,"If you use this code for your research, please consider citing:
```
@inProceedings{kanazawaHMR18,
  title={End-to-end Recovery of Human Shape and Pose},
  author = {Angjoo Kanazawa
  and Michael J. Black
  and David W. Jacobs
  and Jitendra Malik},
  booktitle={Computer Vision and Pattern Regognition (CVPR)},
  year={2018}
}
```

",3
84,apsg, APSG - python package for structural geologists,,1
85,apsg, Installation,"[![GitHub version](https://badge.fury.io/gh/ondrolexa%2Fapsg.svg)](https://badge.fury.io/gh/ondrolexa%2Fapsg)
[![Build Status](https://travis-ci.org/ondrolexa/apsg.svg?branch=master)](https://travis-ci.org/ondrolexa/apsg)
[![Documentation Status](https://readthedocs.org/projects/apsg/badge/?version=stable)](https://apsg.readthedocs.io/en/stable/?badge=stable)
[![DOI](https://zenodo.org/badge/24879346.svg)](https://zenodo.org/badge/latestdoi/24879346)

APSG defines several new python classes to easily manage, analyze and
visualize orientational structural geology data.

",2
86,apsg, PyPI,,3
87,apsg, Upgrading via pip,"To install APSG, just execute
```
pip install apsg
```
Alternatively, you download the package manually from the Python Package Index [https://pypi.org/project/apsg](https://pypi.org/project/apsg), unzip it, navigate into the package, and use the command:
```
python setup.py install
```
",4
88,apsg, Installing APSG from the source distribution,"To upgrade an existing version of APSG from PyPI, execute
```
pip install apsg --upgrade --no-deps
```
Please note that the dependencies (Matplotlib, NumPy and SciPy) will also be upgraded if you omit the `--no-deps` flag; use the `--no-deps` (""no dependencies"") flag if you don't want this.

",4
89,apsg, Conda,"In rare cases, users reported problems on certain systems with the default pip installation command, which installs APSG from the binary distribution (""wheels"") on PyPI. If you should encounter similar problems, you could try to install APSG from the source distribution instead via
```
pip install --no-binary :all: apsg
```
Also, I would appreciate it if you could report any issues that occur when using `pip install apsg` in hope that we can fix these in future releases.

",3
90,apsg, Current release info,"The APSG package is also available through `conda-forge`.

",4
91,apsg, Installing apsg,"| Name | Downloads | Version | Platforms |
| --- | --- | --- | --- |
| [![Conda Recipe](https://img.shields.io/badge/recipe-apsg-green.svg)](https://anaconda.org/conda-forge/apsg) | [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/apsg.svg)](https://anaconda.org/conda-forge/apsg) | [![Conda Version](https://img.shields.io/conda/vn/conda-forge/apsg.svg)](https://anaconda.org/conda-forge/apsg) | [![Conda Platforms](https://img.shields.io/conda/pn/conda-forge/apsg.svg)](https://anaconda.org/conda-forge/apsg) |

",4
92,apsg, Master version,"Installing `apsg` from the `conda-forge` channel can be achieved by adding `conda-forge` to your channels with:

```
conda config --add channels conda-forge
```

Once the `conda-forge` channel has been enabled, `apsg` can be installed with:

```
conda install apsg
```

It is possible to list all of the versions of `apsg` available on your platform with:

```
conda search apsg --channel conda-forge
```

",3
93,apsg, Getting started,"The APSG version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing
```
pip install git+git://github.com/ondrolexa/apsg.git
```
Or, you can fork the GitHub repository from [https://github.com/ondrolexa/apsg](https://github.com/ondrolexa/apsg) and install APSG from your local drive via
```
python setup.py install
```

",2
94,apsg, Documentation,"You can see APSG in action in accompanied Jupyter notebook [http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/apsg_tutorial.ipynb](http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/apsg_tutorial.ipynb)

And for fun check how simply you can animate stereonets
[http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/animation_example.ipynb](http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/animation_example.ipynb)

",2
95,apsg, Contributing,"Explore the full features of APSG. You can find detailed documentation [here](https://apsg.readthedocs.org).

",2
96,apsg, Donate,"Most discussion happens on [Github](https://github.com/ondrolexa/apsg). Feel free to open [an issue](https://github.com/ondrolexa/apsg/issues/new) or comment on any open issue or pull request. Check ``CONTRIBUTING.md`` for more details.

",2
97,apsg, License,"APSG is an open-source project, available for you for free. It took a lot of time and resources to build this software. If you find this software useful and want to support its future development please consider donating me.

[![Donate via PayPal](https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=QTYZWVUNDUAH8&item_name=APSG+development+donation&currency_code=EUR&source=url)

",2
98,mplleaflet, mplleaflet,,1
99,mplleaflet, Examples,"mplleaflet is a Python library that converts a [matplotlib](http://matplotlib.org) plot into a webpage
containing a pannable, zoomable [Leaflet](http://leafletjs.com) map. It can also [embed the Leaflet map in an IPython notebook](http://nbviewer.ipython.org/github/jwass/mplleaflet/blob/master/examples/NYC%20Boroughs.ipynb). The goal of mplleaflet is to enable use of Python and matplotlib for visualizing geographic data on [slippy maps](http://wiki.openstreetmap.org/wiki/Slippy_Map) without having to write any Javascript or HTML. You also don't need to worry about choosing the base map content i.e., coastlines, roads, etc.

*Only one line of code is needed to convert a plot into a web map.*
`mplleaflet.show()`

The library is heavily inspired by [mpld3](https://github.com/jakevdp/mpld3) and uses  [mplexporter](https://github.com/mpld3/mplexporter) to do most of the heavy lifting to walk through Figure objects.

",2
100,mplleaflet, Basic usage,,3
101,mplleaflet, IPython Notebook embedding,"The simplest use is to just create your plot using matplotlib commands and call `mplleaflet.show()`.

```
>>> import matplotlib.pyplot as plt
... #notes: Load longitude, latitude data
>>> plt.hold(True)
#notes: Plot the data as a blue line with red squares on top
#notes: Just plot longitude vs. latitude
>>> plt.plot(longitude, latitude, 'b') #notes: Draw blue line
>>> plt.plot(longitude, latitude, 'rs') #notes: Draw red squares
```
![matplotlib x,y plot](examples/images/simple_plot.png)

Normally, displaying data as longitude, latitude will cause a cartographer to cry. That's totally fine with mplleaflet, Leaflet will project your data properly.

```
#notes: Convert to interactive Leaflet map
>>> import mplleaflet
>>> mplleaflet.show()
```

[Click to view final web page](http://htmlpreview.github.io/?https://github.com/jwass/mplleaflet/master/examples/readme_example.html)

![Leaflet map preview](examples/images/simple_plot_map_preview.jpg)

Disclaimer: Displaying data in spherical mercator might also cause a cartographer to cry.

`show()` allows you to specify different tile layer URLs, CRS/EPSG codes, output files, etc. 

",3
102,mplleaflet, Other examples,"Just use  `mplleaflet.display()` to embed the interactive Leaflet map in an IPython notebook.
[Click here to see a live example.](http://nbviewer.ipython.org/github/jwass/mplleaflet/blob/master/examples/NYC%20Boroughs.ipynb)

",3
103,mplleaflet, Why mplleaflet?,"* [basic_plot.py](examples/basic_plot.py): Simple line/point plotting. [View the map](http://htmlpreview.github.io/?https://github.com/jwass/mplleaflet/master/examples/basic_plot.html).
* [quiver.py](examples/quiver.py): Demonstrates use of quiver() to plot 2-D arrows. [View the map](http://htmlpreview.github.io/?https://github.com/jwass/mplleaflet/master/examples/quiver.html).
* [contour.py](examples/contour.py): Compute contour curves. This example demonstrates plotting in a different CRS and letting mplleaflet convert the output. [View the map](http://htmlpreview.github.io/?https://github.com/jwass/mplleaflet/master/examples/contour.html).
* [Embedded IPython notebook example](http://nbviewer.ipython.org/github/jwass/mplleaflet/blob/master/examples/NYC%20Boroughs.ipynb)

",2
104,mplleaflet, Installation,"Other Python libraries, [basemap](http://matplotlib.org/basemap/) and
[folium](https://github.com/wrobstory/folium), exist to create maps in Python. However mplleaflet allows you to leverage  all matplotlib capability without having to set up the background basemap. You can use `plot()` to style points and lines, and you can also use more complex functions like `contour()`, `quiver()`, etc. Furthermore, with mplleaflet you no longer have to worry about setting up the basemap. Displaying continents or roads is determined automatically by the zoom level required to view the physical size of the data. You should use a different library if you need fine control over the basemap, or need a geographic projection other than spherical mercator.

",2
105,mplleaflet, Development,"Install `mplleaflet` from PyPI using `$ pip install mplleaflet`.

",2
106,mplleaflet, Dependencies,"If developing for `mplleaflet`, `mplexporter` is a git submodule with its
Python package files placed under the `mplleaflet` package. The Makefile
copies the files into the appropriate location.

```
$ git submodule init
$ git submodule update
$ make
$ pip install -e .

```

",2
107,pylops, Objective,"[![PyPI version](https://badge.fury.io/py/pylops.svg)](https://badge.fury.io/py/pylops)
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/pylops/badges/version.svg)](https://anaconda.org/conda-forge/pylops)
[![Build Status](https://travis-ci.org/equinor/pylops.svg?branch=master)](https://travis-ci.org/equinor/pylops)
[![AzureDevOps Status](https://dev.azure.com/MRAVA/PyLops/_apis/build/status/equinor.pylops?branchName=master)](https://dev.azure.com/MRAVA/PyLops/_build/latest?definitionId=1&branchName=master)
[![Documentation Status](https://readthedocs.org/projects/pylops/badge/?version=latest)](https://pylops.readthedocs.io/en/latest/?badge=latest)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/f24504b617ad40f188b73afc0722b5b8)](https://www.codacy.com/app/mrava87/pylops?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=equinor/pylops&amp;utm_campaign=Badge_Grade)
[![Codacy Coverage](https://api.codacy.com/project/badge/Coverage/f24504b617ad40f188b73afc0722b5b8)](https://www.codacy.com/app/mrava87/pylops?utm_source=github.com&utm_medium=referral&utm_content=equinor/pylops&utm_campaign=Badge_Coverage)
[![OS-support](https://img.shields.io/badge/OS-linux,osx-850A8B.svg)](https://github.com/equinor/pylops)
[![Slack Status](https://img.shields.io/badge/chat-slack-green.svg)](https://pylops.slack.com)

",2
108,pylops, Project structure,"This Python library is inspired by the MATLAB [Spot – A Linear-Operator Toolbox](http://www.cs.ubc.ca/labs/scl/spot/) project.

Linear operators and inverse problems are at the core of many of the most used algorithms
in signal processing, image processing, and remote sensing. When dealing with small-scale problems,
the Python numerical scientific libraries [numpy](http://www.numpy.org)
and [scipy](https://www.scipy.org/scipylib/index.html) allow to perform many
of the underlying matrix operations (e.g., computation of matrix-vector products and manipulation of matrices)
in a simple and compact way.

Many useful operators, however, do not lend themselves to an explicit matrix
representation when used to solve large-scale problems. PyLops operators, on the other hand, still represent a matrix
and can be treated in a similar way, but do not rely on the explicit creation of a dense (or sparse) matrix itself. Conversely,
the forward and adjoint operators are represented by small pieces of codes that mimic the effect of the matrix
on a vector or another matrix.

Luckily, many iterative methods (e.g. cg, lsqr) do not need to know the individual entries of a matrix to solve a linear system.
Such solvers only require the computation of forward and adjoint matrix-vector products as done for any of the PyLops operators.

Here is a simple example showing how a dense first-order first derivative operator can be created,
applied and inverted using numpy/scipy commands:
```python
import numpy as np
from scipy.linalg import lstsq

nx = 7
x = np.arange(nx) - (nx-1)/2

D = np.diag(0.5*np.ones(nx-1), k=1) - \
    np.diag(0.5*np.ones(nx-1), k=-1)
D[0] = D[-1] = 0 #notes: take away edge effects

#notes: y = Dx
y = np.dot(D,x)
#notes: x = D'y
xadj = np.dot(D.T,y)
#notes: xinv = D^-1 y
xinv = lstsq(D, y)[0]
```
and similarly using PyLops commands:
```python
from pylops import FirstDerivative

Dlop = FirstDerivative(nx, dtype='float64')

#notes: y = Dx
y = Dlop*x
#notes: x = D'y
xadj = Dlop.H*y
#notes: xinv = D^-1 y
xinv = Dlop / y
```

Note how this second approach does not require creating a dense matrix, reducing both the memory load and the computational cost of
applying a derivative to an input vector x. Moreover, the code becomes even more compact and espressive than in the previous case
letting the user focus on the formulation of equations of the forward problem to be solved by inversion.


",2
109,pylops, Getting started,"This repository is organized as follows:
* **pylops**:       python library containing various linear operators and auxiliary routines
* **pytests**:    set of pytests
* **testdata**:   sample datasets used in pytests and documentation
* **docs**:       sphinx documentation
* **examples**:   set of python script examples for each linear operator to be embedded in documentation using sphinx-gallery
* **tutorials**:  set of python script tutorials to be embedded in documentation using sphinx-gallery

",2
110,pylops, From PyPi,"You need **Python 3.6.4 or greater**.

",4
111,pylops, From Conda-forge,"If you want to use PyLops within your codes,
install it in your Python environment by typing the following command in your terminal:

```
pip install pylops
```

Open a python terminal and type:

```
import pylops
```

If you do not see any error, you should be good to go, enjoy!

",4
112,pylops, From Docker,"Alternatively, you can install PyLops using the conda-forge distribution by typing the following command in your terminal:

```
conda install -c conda-forge pylops
```

",4
113,pylops, Contributing,"If you simply want to try PyLops but do not have Python in your
local machine, you can use our [Docker](https://www.docker.com) image. After installing Docker in your computer,
type the following command in your terminal (note that this will take some time the first time
you type it as you will download and install the docker image):

```
docker run -it -v /path/to/local/folder:/home/jupyter/notebook -p 8888:8888 mrava87/pylops:notebook
```

This will give you an address that you can put in your browser and will open a jupyter-notebook enviroment with PyLops
and other basic Python libraries installed. Here `/path/to/local/folder` is the absolute path of a local folder
on your computer where you will create a notebook (or containing notebooks that you want to continue working on). Note that
anything you do to the notebook(s) will be saved in your local folder.

A larger image with Conda distribution is also available. Simply use `conda_notebook` instead of `notebook` in the
previous command.

",2
114,pylops, 1. Fork and clone the repository,"*Feel like contributing to the project? Adding new operators or tutorial?*

We advise using the [Anaconda Python distribution](https://www.anaconda.com/download)
to ensure that all the dependencies are installed via the `Conda` package manager. Follow
the following instructions and read carefully the [CONTRIBUTING](CONTRIBUTING.md) file before getting started.

",3
115,pylops, 2. Install PyLops in a new Conda environment,"Execute the following command in your terminal:

```
git clone https://github.com/your_name_here/pylops.git
```

",3
116,pylops, Documentation,"To ensure that further development of PyLops is performed within the same environment (i.e., same dependencies) as
that defined by ``requirements-dev.txt`` or ``environment-dev.yml`` files, we suggest to work off a new Conda enviroment.

The first time you clone the repository run the following command:
```
make dev-install_conda
```
To ensure that everything has been setup correctly, run tests:
```
make tests
```
Make sure no tests fail, this guarantees that the installation has been successfull.

Remember to always activate the conda environment every time you open a new terminal by typing:
```
source activate pylops
```

",2
117,pylops, History,"The official documentation of PyLops is available [here](https://pylops.readthedocs.io/).

Visit this page to get started learning about different operators and their applications as well as how to
create new operators yourself and make it to the ``Contributors`` list.

Moreover, if you have installed PyLops using the *developer environment* you can also build the documentation locally by
typing the following command:
```
make doc
```
Once the documentation is created, you can make any change to the source code and rebuild the documentation by
simply typing
```
make docupdate
```
Note that if a new example or tutorial is created (and if any change is made to a previously available example or tutorial)
you are required to rebuild the entire documentation before your changes will be visible.


",2
118,pylops, Contributors,"PyLops was initially written and it is currently maintained by [Equinor](https://www.equinor.com).
It is a flexible and scalable python library for large-scale optimization with linear
operators that can be tailored to our needs, and as contribution to the free software community.


",2
119,tensorflow-magenta, Getting Started,"<img src=""magenta-logo-bg.png"" height=""75"">

[![Build Status](https://travis-ci.org/tensorflow/magenta.svg?branch=master)](https://travis-ci.org/tensorflow/magenta)
 [![PyPI version](https://badge.fury.io/py/magenta.svg)](https://badge.fury.io/py/magenta)

**Magenta** is a research project exploring the role of machine learning
in the process of creating art and music.  Primarily this
involves developing new deep learning and reinforcement learning
algorithms for generating songs, images, drawings, and other materials. But it's also
an exploration in building smart tools and interfaces that allow
artists and musicians to extend (not replace!) their processes using
these models.  Magenta was started by some researchers and engineers
from the [Google Brain team](https://research.google.com/teams/brain/),
but many others have contributed significantly to the project. We use
[TensorFlow](https://www.tensorflow.org) and release our models and
tools in open source on this GitHub.  If you’d like to learn more
about Magenta, check out our [blog](https://magenta.tensorflow.org),
where we post technical details.  You can also join our [discussion
group](https://groups.google.com/a/tensorflow.org/forum/#!forum/magenta-discuss).

This is the home for our Python TensorFlow library. To use our models in the browser with [TensorFlow.js](https://js.tensorflow.org/), head to the [Magenta.js](https://github.com/tensorflow/magenta-js) repository.

",2
120,tensorflow-magenta, Installation,"* [Installation](#installation)
* [Using Magenta](#using-magenta)
* [Playing a MIDI Instrument](#playing-a-midi-instrument)
* [Development Environment (Advanced)](#development-environment)

",2
121,tensorflow-magenta, Automated Install (w/ Anaconda),"Magenta maintains a [pip package](https://pypi.python.org/pypi/magenta) for easy
installation. We recommend using Anaconda to install it, but it can work in any
standard Python environment. We support both Python 2 (>= 2.7) and Python 3 (>= 3.5).
These instructions will assume you are using Anaconda.

Note that if you want to enable GPU support, you should follow the [GPU Installation](#gpu-installation) instructions below.

",3
122,tensorflow-magenta, Manual Install (w/o Anaconda),"If you are running Mac OS X or Ubuntu, you can try using our automated
installation script. Just paste the following command into your terminal.

```bash
curl https://raw.githubusercontent.com/tensorflow/magenta/master/magenta/tools/magenta-install.sh > /tmp/magenta-install.sh
bash /tmp/magenta-install.sh
```

After the script completes, open a new terminal window so the environment
variable changes take effect.

The Magenta libraries are now available for use within Python programs and
Jupyter notebooks, and the Magenta scripts are installed in your path!

Note that you will need to run `source activate magenta` to use Magenta every
time you open a new terminal window.

",3
123,tensorflow-magenta, GPU Installation,"If the automated script fails for any reason, or you'd prefer to install by
hand, do the following steps.

Install the Magenta pip package:

```bash
pip install magenta
```

**NOTE**: In order to install the `rtmidi` package that we depend on, you may need to install headers for some sound libraries. On Linux, this command should install the necessary packages:

```bash
sudo apt-get install build-essential libasound2-dev libjack-dev
```

The Magenta libraries are now available for use within Python programs and
Jupyter notebooks, and the Magenta scripts are installed in your path!

",3
124,tensorflow-magenta, Using Magenta,"If you have a GPU installed and you want Magenta to use it, you will need to
follow the [Manual Install](#manual-install) instructions, but with a few
modifications.

First, make sure your system meets the [requirements to run tensorflow with GPU support](
https://www.tensorflow.org/install/install_linux#nvidia_requirements_to_run_tensorflow_with_gpu_support).

Next, follow the [Manual Install](#manual-install) instructions, but install the
`magenta-gpu` package instead of the `magenta` package:

```bash
pip install magenta-gpu
```

The only difference between the two packages is that `magenta-gpu` depends on
`tensorflow-gpu` instead of `tensorflow`.

Magenta should now have access to your GPU.

",2
125,tensorflow-magenta, Playing a MIDI Instrument,"You can now train our various models and use them to generate music, audio, and images. You can
find instructions for each of the models by exploring the [models directory](magenta/models).

To get started, create your own melodies with TensorFlow using one of the various configurations of our [Melody RNN](magenta/models/melody_rnn) model; a recurrent neural network for predicting melodies.

",2
126,tensorflow-magenta, Development Environment,"After you've trained one of the models above, you can use our [MIDI interface](magenta/interfaces/midi) to play with it interactively.

We also have created several [demos](https://github.com/tensorflow/magenta-demos) that provide a UI for this interface, making it easier to use (e.g., the browser-based [AI Jam](https://github.com/tensorflow/magenta-demos/tree/master/ai-jam-js)).

",2
127,tensorflow-magenta, PIP Release,"If you want to develop on Magenta, you'll need to set up the full Development Environment.

First, clone this repository:

```bash
git clone https://github.com/tensorflow/magenta.git
```

Next, install the dependencies by changing to the base directory and executing the setup command:

```bash
pip install -e .
```

You can now edit the files and run scripts by calling Python as usual. For example, this is how you would run the `melody_rnn_generate` script from the base directory:

```bash
python magenta/models/melody_rnn/melody_rnn_generate --config=...
```

You can also install the (potentially modified) package with:

```bash
pip install .
```

Before creating a pull request, please also test your changes with:

```bash
pip install pytest-pylint
pytest
```

",2
128,facebookresearch-wav2letter, wav2letter++,,1
129,facebookresearch-wav2letter, Building wav2letter++,"[![CircleCI](https://circleci.com/gh/facebookresearch/wav2letter.svg?style=svg)](https://circleci.com/gh/facebookresearch/wav2letter)

wav2letter++ is a fast open source speech processing toolkit from the Speech Team at Facebook AI Research.
It is written entirely in C++ and uses the [ArrayFire](https://github.com/arrayfire/arrayfire) tensor library and the [flashlight](https://github.com/facebookresearch/flashlight) machine learning library for maximum efficiency.
Our approach is detailed in this [arXiv paper](https://arxiv.org/abs/1812.07625).

The goal of this software is to facilitate research in end-to-end models for speech recognition.

The previous version of wav2letter (written in Lua) can be found in the ""wav2letter-lua"" branch under the repository.

",2
130,facebookresearch-wav2letter, Full documentation,"See [Building Instructions](docs/installation.md) for details.

",2
131,facebookresearch-wav2letter, Citation,"- [Data Preparation](docs/data_prep.md)
- [Training](docs/train.md)
- [Testing / Decoding](docs/decoder.md)

To get started with wav2letter++, checkout the [tutorials](tutorials) section.

We also provide complete recipes for WSJ, Timit and Librispeech and they can be found in [recipes](recipes) folder.

",2
132,facebookresearch-wav2letter, Join the wav2letter community,"If you use the code in your paper, then please cite it as:

```
@article{pratap2018w2l,
  author          = {Vineel Pratap, Awni Hannun, Qiantong Xu, Jeff Cai, Jacob Kahn, Gabriel Synnaeve, Vitaliy Liptchinsky, Ronan Collobert},
  title           = {wav2letter++: The Fastest Open-source Speech Recognition System},
  journal         = {CoRR},
  volume          = {abs/1812.07625},
  year            = {2018},
  url             = {https://arxiv.org/abs/1812.07625},
}
```

",2
133,facebookresearch-wav2letter, License,"* Facebook page: https://www.facebook.com/groups/717232008481207/
* Google group: https://groups.google.com/forum/#!forum/wav2letter-users
* Contact: vineelkpratap@fb.com, awni@fb.com, qiantong@fb.com, jcai@fb.com, jacobkahn@fb.com, gab@fb.com, vitaliy888@fb.com, locronan@fb.com

See the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.

",2
134,omfvista, Grab a few elements of interest and plot em up!,"=======

.. image:: https://img.shields.io/pypi/v/omfvista.svg?logo=python&logoColor=white
   :target: https://pypi.org/project/omfvista/
   :alt: PyPI

.. image:: https://img.shields.io/travis/OpenGeoVis/omfvista/master.svg?label=build&logo=travis
   :target: https://travis-ci.org/OpenGeoVis/omfvista
   :alt: Build Status Linux

.. image:: https://ci.appveyor.com/api/projects/status/49tewkw60mykh1nb?svg=true
   :target: https://ci.appveyor.com/project/banesullivan/omfvista
   :alt: Build Status Windows

.. image:: https://codecov.io/gh/OpenGeoVis/omfvista/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/OpenGeoVis/omfvista

.. image:: https://img.shields.io/github/stars/OpenGeoVis/omfvista.svg?style=social&label=Stars
   :target: https://github.com/OpenGeoVis/omfvista
   :alt: GitHub


A PyVista (and VTK) interface for the `Open Mining Format package`_ (``omf``)
providing Python 3D visualization and useable mesh data structures for
processing datasets in the OMF specification.


.. _Open Mining Format package: https://omf.readthedocs.io/en/latest/

Documentation is hosted at https://opengeovis.github.io/omfvista/


Installation
------------

Installation is simply::

    pip install omfvista

All necessary dependencies will be installed alongside ``omfvista``. Please
note that this package heavily leverages the PyVista_ package.

.. _PyVista: https://github.com/pyvista/pyvista


Questions & Support
-------------------

For general questions about the project, its applications, or about software
usage, please create an issue in the `pyvista/pyvista-support`_ repository
where the  PyVista community can collectively address your questions.
You are also welcome to join us on join `@OpenGeoVis`_ on our
`Slack workspace`_ under the ``#omfvista`` channel or send one of the
developers an email. The project support team can be reached at
`info@opengeovis.org`_.

.. _pyvista/pyvista-support: https://github.com/pyvista/pyvista-support
.. _@OpenGeoVis: https://github.com/OpenGeoVis
.. _Slack workspace: http://slack.opengeovis.org
.. _info@opengeovis.org: mailto:info@opengeovis.org

Example Use
-----------

.. image:: https://mybinder.org/badge_logo.svg
   :target: https://mybinder.org/v2/gh/OpenGeoVis/omfvista/master?filepath=Example.ipynb

Be sure to check out the `Example Notebook`_ that demos ``omfvista`` or our
`Example Gallery`_ in the documentation!
Here's an example using the sample data hosted in the `OMF repository`_.

.. _Example Notebook: https://mybinder.org/v2/gh/OpenGeoVis/omfvista/master?filepath=Example.ipynb
.. _Example Gallery: https://opengeovis.github.io/omfvista/examples/index.html
.. _OMF repository: https://github.com/gmggroup/omf/tree/master/assets

.. code-block:: python

    import pyvista as pv
    import omfvista

    project = omfvista.load_project('test_file.omf')
    project

.. image:: https://github.com/OpenGeoVis/omfvista/raw/master/assets/table-repr.png
   :alt: Table Representation


Once the data is loaded as a ``pyvista.MultiBlock`` dataset from ``omfvista``, then
that object can be directly used for interactive 3D visualization from PyVista_:

.. code-block:: python

    project.plot(notebook=False)

Or an interactive scene can be created and manipulated to create a compelling
figure directly in a Jupyter notebook. First, grab the elements from the project:

.. code-block:: python

    ",1
135,omfvista, Grab the active plotting window,"    vol = project['Block Model']
    assay = project['wolfpass_WP_assay']
    topo = project['Topography']
    dacite = project['Dacite']

Then apply a filtering tool from PyVista_ to the volumetric data:

.. code-block:: python

    thresher = pv.Threshold(vol)

.. figure:: https://github.com/OpenGeoVis/omfvista/raw/master/assets/threshold.gif
   :alt: IPython Thresholding Tool

Then you can put it all in one environment!

.. code-block:: python

    ",1
136,omfvista,  from the thresher tool,    ,1
137,omfvista, Add our datasets,"    p = thresher.plotter
    ",1
138,omfvista, Add the bounds axis,"    p.add_mesh(topo, cmap='gist_earth', opacity=0.5)
    p.add_mesh(assay, color='blue', line_width=3)
    p.add_mesh(dacite, color='yellow', opacity=0.6)
    ",1
139,PRM, PyTorch Implementation,"[![](https://img.shields.io/badge/pytorch-1.0-red.svg)](https://github.com/ZhouYanzhao/PRM/tree/pytorch)
[![](https://img.shields.io/badge/mxnet-scheduled-green.svg)](#)

[[Home]](http://yzhou.work) [[Project]](http://yzhou.work/PRM) [[Paper]](https://arxiv.org/pdf/1804.00880) [[Supp]](http://yzhou.work/PRM/Supplementary.pdf) [[Poster]](http://yzhou.work/PRM/Poster.pdf) [[Presentation]](https://www.youtube.com/embed/lNqXyJliVSo?start=4615&end=4850&autoplay=1&controls=0)

![Illustration](illustration.png)

",2
140,PRM, Prerequisites,"The [pytorch branch](https://github.com/ZhouYanzhao/PRM/tree/pytorch) contains:

* the **pytorch** implementation of Peak Response Mapping (Stimulation and Backprop).
* the PASCAL-VOC demo (training, inference, and visualization).

Please follow the instruction below to install it and run the experiment demo.

",3
141,PRM, Installation,"* System (tested on Ubuntu 14.04LTS and Win10)
* NVIDIA GPU + CUDA CuDNN (CPU mode is also supported but significantly slower)
* [Python>=3.5](https://www.python.org)
* [PyTorch>=0.4](https://pytorch.org)
* [Jupyter Notebook](https://jupyter.org/install.html) and [ipywidgets](https://github.com/jupyter-widgets/ipywidgets) (required by the demo):

    ```bash
    #notes: enable the widgetsnbextension before you start the notebook server
    jupyter nbextension enable --py --sys-prefix widgetsnbextension
    ```

",3
142,PRM, Run demo,"1. Install [Nest](https://github.com/ZhouYanzhao/Nest), a flexible tool for building and sharing deep learning modules:
    
    > I created Nest in the process of refactoring PRM's pytorch implementation. It aims at encouraging code reuse and ships with a bunch of useful features. PRM is now implemented as a set of Nest modules; thus you can easily install and use it as demonstrated below.

    ```bash
    $ pip install git+https://github.com/ZhouYanzhao/Nest.git
    ```
    

2. Install PRM via Nest's CLI tool:

    ```bash
    #notes: note that data will be saved under your current path
    $ nest module install github@ZhouYanzhao/PRM:pytorch prm
    #notes: verify the installation
    $ nest module list --filter prm
    #notes: Output:
    #notes:
    #notes: 3 Nest modules found.
    #notes: [0] prm.fc_resnet50 (1.0.0)
    #notes: [1] prm.peak_response_mapping (1.0.0)
    #notes: [2] prm.prm_visualize (1.0.0)
    ```

",3
143,PRM, Citation ,"1. Install Nest's build-in Pytorch modules:

    > To increase reusability, I abstracted some features from the original code, such as network trainer, to build Nest's built-in pytorch module set.
    
    ```bash
    $ nest module install github@ZhouYanzhao/Nest:pytorch pytorch
    ```

2. Download the PASCAL-VOC2012 dataset:

    ```bash
    mkdir ./PRM/demo/datasets
    cd ./PRM/demo/datasets
    #notes: download and extract data
    wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
    tar xvf VOCtrainval_11-May-2012.tar
    ```

3. Run the demo experiment via [demo/main.ipynb](https://github.com/ZhouYanzhao/PRM/tree/pytorch/demo/main.ipynb)

    ![PRM Segmentation](samples.png)

",2
144,GAN_stability, GAN stability,,1
145,GAN_stability, Usage,"This repository contains the experiments in the supplementary material for the paper [Which Training Methods for GANs do actually Converge?](https://avg.is.tuebingen.mpg.de/publications/meschedericml2018).

To cite this work, please use
```
@INPROCEEDINGS{Mescheder2018ICML,
  author = {Lars Mescheder and Sebastian Nowozin and Andreas Geiger},
  title = {Which Training Methods for GANs do actually Converge?},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2018}
}
```
You can find further details on [our project page](https://avg.is.tuebingen.mpg.de/research_projects/convergence-and-stability-of-gan-training).

",1
146,GAN_stability, Notes,"First download your data and put it into the `./data` folder.

To train a new model, first create a config script similar to the ones provided in the `./configs` folder.  You can then train you model using
```
python train.py PATH_TO_CONFIG
```

To compute the inception score for your model and generate samples, use
```
python test.py PATH_TO_CONIFG
```

Finally, you can create nice latent space interpolations using
```
python interpolate.py PATH_TO_CONFIG
```
or
```
python interpolate_class.py PATH_TO_CONFIG
```

",1
147,GAN_stability, Results,"* For the results presented in the paper, we did not use a moving average over the weights. However, using a moving average helps to reduce noise and we therefore recommend its usage. Indeed, we found that using a moving average leads to much better inception scores on Imagenet.
* Batch normalization is currently *not* supported when using an exponential running average, as the running average is only computed over the parameters of the models and not the other buffers of the model.

",1
148,GAN_stability, celebA-HQ,,2
149,GAN_stability, Imagenet,"![celebA-HQ](results/celebA-HQ.jpg)

",2
150,d3, D3: Data-Driven Documents,,1
151,d3, Resources,"<a href=""https://d3js.org""><img src=""https://d3js.org/logo.svg"" align=""left"" hspace=""10"" vspace=""6""></a>

**D3** (or **D3.js**) is a JavaScript library for visualizing data using web standards. D3 helps you bring data to life using SVG, Canvas and HTML. D3 combines powerful visualization and interaction techniques with a data-driven approach to DOM manipulation, giving you the full capabilities of modern browsers and the freedom to design the right visual interface for your data.

",2
152,d3, Installing,"* [API Reference](https://github.com/d3/d3/blob/master/API.md)
* [Release Notes](https://github.com/d3/d3/releases)
* [Gallery](https://github.com/d3/d3/wiki/Gallery)
* [Examples](https://bl.ocks.org/mbostock)
* [Wiki](https://github.com/d3/d3/wiki)

",2
153,microsoft-malmo, Malmö ,,2
154,microsoft-malmo, Getting Started ,"Project Malmö is a platform for Artificial Intelligence experimentation and research built on top of Minecraft. We aim to inspire a new generation of research into challenging new problems presented by this unique environment.

[![Join the chat at https://gitter.im/Microsoft/malmo](https://badges.gitter.im/Microsoft/malmo.svg)](https://gitter.im/Microsoft/malmo?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![Build Status](https://travis-ci.org/Microsoft/malmo.svg?branch=master)](https://travis-ci.org/Microsoft/malmo) [![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/Microsoft/malmo/blob/master/LICENSE.txt)
----
    
",4
155,microsoft-malmo, MalmoEnv ,,6
156,microsoft-malmo, Malmo as a native Python wheel ,"MalmoEnv implements an Open AI ""gym""-like environment in Python without any native code (communicating directly with Java Minecraft). If you only need this functionallity then please see [MalmoEnv](https://github.com/Microsoft/malmo/tree/master/MalmoEnv). This will most likely be the preferred way to develop with Malmo Minecraft going forward.

If you wish to use the ""native"" Malmo implementation, either install the ""Malmo native Python wheel"" (if available for your platform) or a pre-built binary release (more on these options below). Building Malmo yourself from source is always an option!

Advantages:
    
1. No native code - you don't have to build or install platform dependent code.
2. A single network connection is used to run missions. No dynamic ports means it's more virtualization friendly.
3. A simpler multi-agent coordination protocol. 
One Minecraft client instance, one single port is used to start missions.
4. Less impedance miss-match with the gym api.

Disadvantages:

1. The existing Malmo examples are not supported (as API used is different). 
Marlo envs should work with this [port](https://github.com/AndKram/marLo/tree/malmoenv).
2. The API is more limited (e.g. selecting video options) - can edit mission xml directly.

",6
157,microsoft-malmo, Problems: ,"On common Windows, MacOSX and Linux variants it is possible to use ```pip3 install malmo``` to install Malmo as a python with native code package: [Pip install for Malmo](https://github.com/Microsoft/malmo/blob/master/scripts/python-wheel/README.md). Once installed, the malmo Python module can be used to download source and examples and start up Minecraft with the Malmo game mod. 

Alternatively, a pre-built version of Malmo can be installed as follows:

1. [Download the latest *pre-built* version, for Windows, Linux or MacOSX.](https://github.com/Microsoft/malmo/releases)   
      NOTE: This is _not_ the same as downloading a zip of the source from Github. _Doing this **will not work** unless you are planning to build the source code yourself (which is a lengthier process). If you get errors along the lines of ""`ImportError: No module named MalmoPython`"" it will probably be because you have made this mistake._

2. Install the dependencies for your OS: [Windows](doc/install_windows.md), [Linux](doc/install_linux.md), [MacOSX](doc/install_macosx.md).

3. Launch Minecraft with our Mod installed. Instructions below.

4. Launch one of our sample agents, as Python, C#, C++ or Java. Instructions below.

5. Follow the [Tutorial](https://github.com/Microsoft/malmo/blob/master/Malmo/samples/Python_examples/Tutorial.pdf) 

6. Explore the [Documentation](http://microsoft.github.io/malmo/). This is also available in the readme.html in the release zip.

7. Read the [Blog](http://microsoft.github.io/malmo/blog) for more information.

If you want to build from source then see the build instructions for your OS: [Windows](doc/build_windows.md), [Linux](doc/build_linux.md), [MacOSX](doc/build_macosx.md).

----

",4
158,microsoft-malmo, Launching Minecraft with our Mod: ,"We're building up a [Troubleshooting](https://github.com/Microsoft/malmo/wiki/Troubleshooting) page of the wiki for frequently encountered situations. If that doesn't work then please ask a question on our [chat page](https://gitter.im/Microsoft/malmo) or open a [new issue](https://github.com/Microsoft/malmo/issues/new).

----

",4
159,microsoft-malmo, Launch an agent: ,"Minecraft needs to create windows and render to them with OpenGL, so the machine you do this from must have a desktop environment.

Go to the folder where you unzipped the release, then:

`cd Minecraft`  
`launchClient` (On Windows)  
`./launchClient.sh` (On Linux or MacOSX)

or, e.g. `launchClient -port 10001` to launch Minecraft on a specific port.

on Linux or MacOSX: `./launchClient.sh -port 10001`

*NB: If you run this from a terminal, the bottom line will say something like ""Building 95%"" - ignore this - don't wait for 100%! As long as a Minecraft game window has opened and is displaying the main menu, you are good to go.*

By default the Mod chooses port 10000 if available, and will search upwards for a free port if not, up to 11000.
The port chosen is shown in the Mod config page.

To change the port while the Mod is running, use the `portOverride` setting in the Mod config page.

The Mod and the agents use other ports internally, and will find free ones in the range 10000-11000 so if administering
a machine for network use these TCP ports should be open.

----

",4
160,microsoft-malmo, Running a Python agent: ,,8
161,microsoft-malmo, Running a C++ agent: ,"```
cd Python_Examples
python3 run_mission.py
``` 

",8
162,microsoft-malmo, Running a C agent: ,"`cd Cpp_Examples`

To run the pre-built sample:

`run_mission` (on Windows)  
`./run_mission` (on Linux or MacOSX)

To build the sample yourself:

`cmake .`  
`cmake --build .`  
`./run_mission` (on Linux or MacOSX)  
`Debug\run_mission.exe` (on Windows)

",9
163,microsoft-malmo, Running a Java agent: ,"To run the pre-built sample (on Windows):

`cd CSharp_Examples`  
`CSharpExamples_RunMission.exe`

To build the sample yourself, open CSharp_Examples/RunMission.csproj in Visual Studio.

Or from the command-line:

`cd CSharp_Examples`

Then, on Windows:  
```
msbuild RunMission.csproj /p:Platform=x64
bin\x64\Debug\CSharpExamples_RunMission.exe
```

",8
164,microsoft-malmo, Running an Atari agent: (Linux only) ,"`cd Java_Examples`  
`java -cp MalmoJavaJar.jar:JavaExamples_run_mission.jar -Djava.library.path=. JavaExamples_run_mission` (on Linux or MacOSX)  
`java -cp MalmoJavaJar.jar;JavaExamples_run_mission.jar -Djava.library.path=. JavaExamples_run_mission` (on Windows)

",8
165,microsoft-malmo, Citations ,"```
cd Python_Examples
python3 ALE_HAC.py
```

----

",2
166,microsoft-malmo, Code of Conduct ,"Please cite Malmo as:

Johnson M., Hofmann K., Hutton T., Bignell D. (2016) [_The Malmo Platform for Artificial Intelligence Experimentation._](http://www.ijcai.org/Proceedings/16/Papers/643.pdf) [Proc. 25th International Joint Conference on Artificial Intelligence](http://www.ijcai.org/Proceedings/2016), Ed. Kambhampati S., p. 4246. AAAI Press, Palo Alto, California USA. https://github.com/Microsoft/malmo

----

",2
167,da-faster-rcnn,  Domain Adaptive Faster R-CNN for Object Detection in the Wild ,,1
168,da-faster-rcnn, Acknowledgment,"This is the implementation of our CVPR 2018 work 'Domain Adaptive Faster R-CNN for Object Detection in the Wild'. The aim is to improve the cross-domain robustness of object detection, in the screnario where training and test data are drawn from different distributions. The original paper can be found [here](https://arxiv.org/pdf/1803.03243.pdf). 

If you find it helpful for your research, please consider citing:

    @inproceedings{chen2018domain,
      title={Domain Adaptive Faster R-CNN for Object Detection in the Wild},
      author={Chen, Yuhua and Li, Wen and Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
      booktitle = {Computer Vision and Pattern Recognition (CVPR)},
      year={2018}
    }

If you encounter any problems with the code, please contact me at yuhua[dot]chen[at]vision[dot]ee[dot]ethz[dot]ch

",3
169,da-faster-rcnn, Usage,"The implementation is built on the python implementation of Faster RCNN [rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn)

",3
170,da-faster-rcnn, Example,"1. Build Caffe and pycaffe (see: [Caffe installation instructions](http://caffe.berkeleyvision.org/installation.html))

2. Build the Cython modules
    ```Shell
    cd $FRCN_ROOT/lib
    make
    
3. Follow the instrutions of [rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn) to download related data.
    
4. Prepare the dataset, source domain data should start with the filename 'source_', and target domain data with 'target_'.

5. To train the Domain Adaptive Faster R-CNN:
    ```Shell
    cd $FRCN_ROOT
    ./tools/train_net.py --gpu {GPU_ID} --solver models/da_faster_rcnn/solver.prototxt --weights data/imagenet_models/VGG16.v2.caffemodel --imdb voc_2007_trainval --iters  {NUM_ITER}  --cfg  {CONFIGURATION_FILE}
    
",3
171,da-faster-rcnn, Other Implementation,"An example of adapting from **Cityscapes** dataset to **Foggy Cityscapes** dataset is provided:
1. Download the datasets from [here](https://www.cityscapes-dataset.com/downloads/). Specifically, we will use **gtFine_trainvaltest.zip**, **leftImg8bit_trainvaltest.zip** and **leftImg8bit_trainvaltest_foggy.zip**.

2. Prepare the data using the scripts in 'prepare_data/prepare_data.m'.

3. Train the Domain Adaptive Faster R-CNN:
    ```Shell
    cd $FRCN_ROOT
    ./tools/train_net.py --gpu {GPU_ID} --solver models/da_faster_rcnn/solver.prototxt --weights data/imagenet_models/VGG16.v2.caffemodel --imdb voc_2007_trainval --iters  70000  --cfg  models/da_faster_rcnn/faster_rcnn_end2end.yml
    
3. Test the trained model:
    ```Shell
    cd $FRCN_ROOT
    ./tools/test_net.py --gpu {GPU_ID} --def models/da_faster_rcnn/test.prototxt --net output/faster_rcnn_end2end/voc_2007_trainval/vgg16_da_faster_rcnn_iter_70000.caffemodel --imdb voc_2007_test --cfg models/da_faster_rcnn/faster_rcnn_end2end.yml

",3
172,facebookresearch-ResNeXt, ResNeXt: Aggregated Residual Transformations for Deep Neural Networks,,1
173,facebookresearch-ResNeXt, Table of Contents,"By [Saining Xie](http://vcl.ucsd.edu/~sxie), [Ross Girshick](http://www.rossgirshick.info/), [Piotr Dollár](https://pdollar.github.io/), [Zhuowen Tu](http://pages.ucsd.edu/~ztu/), [Kaiming He](http://kaiminghe.com)

UC San Diego, Facebook AI Research

",3
174,facebookresearch-ResNeXt, News,"0. [Introduction](#introduction)
0. [Citation](#citation)
0. [Requirements and Dependencies](#requirements-and-dependencies)
0. [Training](#training)
0. [ImageNet Pretrained Models](#imagenet-pretrained-models)
0. [Third-party re-implementations](#third-party-re-implementations)

",4
175,facebookresearch-ResNeXt, Introduction,"* Congrats to the ILSVRC 2017 classification challenge winner [WMW](http://image-net.org/challenges/LSVRC/2017/results).
ResNeXt is the foundation of their new SENet architecture (a **ResNeXt-152 (64 x 4d)** with the Squeeze-and-Excitation module)!
* Check out Figure 6 in the new [Memory-Efficient Implementation of DenseNets](https://arxiv.org/pdf/1707.06990.pdf) paper for a comparision between ResNeXts and DenseNets. <sub>（*DenseNet cosine is DenseNet trained with cosine learning rate schedule.*）</sub>
<p align=""center"">
<img src=""http://vcl.ucsd.edu/resnext/resnextvsdensenet.png"" width=""480"">
</p>


",3
176,facebookresearch-ResNeXt," Figure: Training curves on ImageNet-1K. (Left): ResNet/ResNeXt-50 with the same complexity (~4.1 billion FLOPs, ~25 million parameters); (Right): ResNet/ResNeXt-101 with the same complexity (~7.8 billion FLOPs, ~44 million parameters).","This repository contains a [Torch](http://torch.ch) implementation for the [ResNeXt](https://arxiv.org/abs/1611.05431) algorithm for image classification. The code is based on [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch).

[ResNeXt](https://arxiv.org/abs/1611.05431) is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.


![teaser](http://vcl.ucsd.edu/resnext/teaser.png)
",5
177,facebookresearch-ResNeXt, Citation,"-----

",3
178,facebookresearch-ResNeXt, Requirements and Dependencies,"If you use ResNeXt in your research, please cite the paper:
```
@article{Xie2016,
  title={Aggregated Residual Transformations for Deep Neural Networks},
  author={Saining Xie and Ross Girshick and Piotr Dollár and Zhuowen Tu and Kaiming He},
  journal={arXiv preprint arXiv:1611.05431},
  year={2016}
}
```

",3
179,facebookresearch-ResNeXt, Training,"See the fb.resnet.torch [installation instructions](https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md) for a step-by-step guide.
- Install [Torch](http://torch.ch/docs/getting-started.html) on a machine with CUDA GPU
- Install [cuDNN v4 or v5](https://developer.nvidia.com/cudnn) and the Torch [cuDNN bindings](https://github.com/soumith/cudnn.torch/tree/R4)
- Download the [ImageNet](http://image-net.org/download-images) dataset and [move validation images](https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset) to labeled subfolders

",3
180,facebookresearch-ResNeXt, 1x Complexity Configurations Reference Table,"Please follow [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch) for the general usage of the code, including [how](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained) to use pretrained ResNeXt models for your own task.

There are two new hyperparameters need to be specified to determine the bottleneck template:

**-baseWidth** and **-cardinality**

",3
181,facebookresearch-ResNeXt, ImageNet Pretrained Models,"| baseWidth | cardinality |
|---------- | ----------- |
| 64        | 1           |
| 40        | 2           |
| 24        | 4           |
| 14        | 8           |
| 4         | 32          |


To train ResNeXt-50 (32x4d) on 8 GPUs for ImageNet:
```bash
th main.lua -dataset imagenet -bottleneckType resnext_C -depth 50 -baseWidth 4 -cardinality 32 -batchSize 256 -nGPU 8 -nThreads 8 -shareGradInput true -data [imagenet-folder]
```

To reproduce CIFAR results (e.g. ResNeXt 16x64d for cifar10) on 8 GPUs:
```bash
th main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 128 -nGPU 8 -nThreads 8 -shareGradInput true
```
To get comparable results using 2/4 GPUs, you should change the batch size and the corresponding learning rate:
```bash
th main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 64 -nGPU 4 -LR 0.05 -nThreads 8 -shareGradInput true
th main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 32 -nGPU 2 -LR 0.025 -nThreads 8 -shareGradInput true
```
Note: CIFAR datasets will be automatically downloaded and processed for the first time. Note that in the arXiv paper CIFAR results are based on pre-activated bottleneck blocks and a batch size of 256. We found that better CIFAR test acurracy can be achieved using original bottleneck blocks and a batch size of 128.

",3
182,facebookresearch-ResNeXt, Single-crop (224x224) validation error rate,"ImageNet pretrained models are licensed under CC BY-NC 4.0.

[![CC BY-NC 4.0](https://i.creativecommons.org/l/by-nc/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc/4.0/)

",4
183,facebookresearch-ResNeXt, Third-party re-implementations,"| Network             | GFLOPS | Top-1 Error |  Download   |
| ------------------- | ------ | ----------- | ------------|
| ResNet-50 (1x64d)   |  ~4.1  |  23.9        | [Original ResNet-50](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained)       |
| ResNeXt-50 (32x4d)  |  ~4.1  |  22.2        | [Download (191MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_50_32x4d.t7)       |
| ResNet-101 (1x64d)  |  ~7.8  |  22.0        | [Original ResNet-101](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained)      |
| ResNeXt-101 (32x4d) |  ~7.8  |  21.2        | [Download (338MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_101_32x4d.t7)      |
| ResNeXt-101 (64x4d) |  ~15.6 |  20.4        | [Download (638MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_101_64x4d.t7)       |

",3
184,vue-devtools, vue-devtools,,1
185,vue-devtools, Installation,"<p align=""center""><img width=""720px"" src=""https://raw.githubusercontent.com/vuejs/vue-devtools/dev/media/screenshot-shadow.png"" alt=""screenshot""></p>

",2
186,vue-devtools, Important Usage Notes,"- [Get the Chrome Extension](https://chrome.google.com/webstore/detail/vuejs-devtools/nhdogjmejiglipccpnnnanhbledajbpd) / ([beta channel](https://chrome.google.com/webstore/detail/vuejs-devtools/ljjemllljcmogpfapbkkighbhhppjdbg))

- [Get the Firefox Addon](https://addons.mozilla.org/en-US/firefox/addon/vue-js-devtools/) / ([beta channel](https://github.com/vuejs/vue-devtools/releases))

- [Get standalone Electron app (works with any environment!)](https://github.com/vuejs/vue-devtools/blob/master/shells/electron/README.md)

",3
187,vue-devtools, Open component in editor,"1. If the page uses a production/minified build of Vue.js, devtools inspection is disabled by default so the Vue pane won't show up.

2. To make it work for pages opened via `file://` protocol, you need to check ""Allow access to file URLs"" for this extension in Chrome's extension management panel.

",3
188,vue-devtools, Manual Installation,"To enable this feature, follow [this guide](./docs/open-in-editor.md).

",3
189,vue-devtools, Development,"This is only necessary when you want to build the extension yourself from source to get not-yet-released features.

**Make sure you are using Node 6+ and NPM 3+**

1. Clone this repo
2. `npm install` (Or `yarn install` if you are using yarn as the package manager)
3. `npm run build`
4. Open Chrome extension page
5. Check ""developer mode""
6. Click ""load unpacked extension"", and choose `shells/chrome`.

",3
190,vue-devtools, Quick Start in chrome,"1. Clone this repo
2. `npm install`
3. `npm run dev`
4. A plain shell with a test app will be available at `localhost:8100`.

",3
191,vue-devtools, Testing as Firefox addon,"```
// Before you create app
Vue.config.devtools = process.env.NODE_ENV === 'development'

// After you create app
window.__VUE_DEVTOOLS_GLOBAL_HOOK__.Vue = app.constructor;

// then had to add in ./store.js as well.
Vue.config.devtools = process.env.NODE_ENV === 'development'

```

",3
192,vue-devtools, Common problems and how to fix," 1. Install `web-ext`

	~~~~
	$ npm install --global web-ext
	~~~~

	Or, for Yarn:

	~~~~
	$ yarn global add web-ext
	~~~~

	Also, make sure `PATH` is set up. Something like this in `~/.bash_profile`:

	~~~~
	$ PATH=$PATH:$(yarn global bin)
	~~~~

 2. Build and run in Firefox

	~~~~
	$ npm run build
	$ npm run run:firefox
	~~~~

	When using Yarn, just replace `npm` with `yarn`.


",3
193,vue-devtools, License,"1. Fixing ""Download the Vue Devtools for a better development experience"" console message when working locally over `file://` protocol:
  1.1 - Google Chrome: Right click on vue-devtools icon and click ""Manage Extensions"" then search for vue-devtools on the extensions list. Check the ""Allow access to file URLs"" box.

2. How to use the devtools in IE/Edge/Safari or any other browser? [Get the standalone Electron app (works with any environment!)](https://github.com/vuejs/vue-devtools/blob/master/shells/electron/README.md)


",3
194,tippecanoe," Linear features (world railroads), visible at all zoom levels","==========

Builds [vector tilesets](https://www.mapbox.com/developers/vector-tiles/) from large (or small) collections of [GeoJSON](http://geojson.org/), [Geobuf](https://github.com/mapbox/geobuf), or [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) features,
[like these](MADE_WITH.md).

![Mapbox Tippecanoe](https://user-images.githubusercontent.com/1951835/36568734-ede27ec0-17df-11e8-8c22-ffaaebb8daf4.JPG)

[![Build Status](https://travis-ci.org/mapbox/tippecanoe.svg)](https://travis-ci.org/mapbox/tippecanoe)
[![Coverage Status](https://codecov.io/gh/mapbox/tippecanoe/branch/master/graph/badge.svg)](https://codecov.io/gh/mapbox/tippecanoe)

Intent
------

The goal of Tippecanoe is to enable making a scale-independent view of your data,
so that at any level from the entire world to a single building, you can see
the density and texture of the data rather than a simplification from dropping
supposedly unimportant features or clustering or aggregating them.

If you give it all of OpenStreetMap and zoom out, it should give you back
something that looks like ""[All Streets](http://benfry.com/allstreets/map5.html)""
rather than something that looks like an Interstate road atlas.

If you give it all the building footprints in Los Angeles and zoom out
far enough that most individual buildings are no longer discernable, you
should still be able to see the extent and variety of development in every neighborhood,
not just the largest downtown buildings.

If you give it a collection of years of tweet locations, you should be able to
see the shape and relative popularity of every point of interest and every
significant travel corridor.

Installation
------------

The easiest way to install tippecanoe on OSX is with [Homebrew](http://brew.sh/):

```sh
$ brew install tippecanoe
```

On Ubuntu it will usually be easiest to build from the source repository:

```sh
$ git clone https://github.com/mapbox/tippecanoe.git
$ cd tippecanoe
$ make -j
$ make install
```

See [Development](#development) below for how to upgrade your
C++ compiler or install prerequisite packages if you get
compiler errors.

Usage
-----

```sh
$ tippecanoe -o file.mbtiles [options] [file.json file.json.gz file.geobuf ...]
```

If no files are specified, it reads GeoJSON from the standard input.
If multiple files are specified, each is placed in its own layer.

The GeoJSON features need not be wrapped in a FeatureCollection.
You can concatenate multiple GeoJSON features or files together,
and it will parse out the features and ignore whatever other objects
it encounters.

Try this first
--------------

If you aren't sure what options to use, try this:

```sh
$ tippecanoe -o out.mbtiles -zg --drop-densest-as-needed in.geojson
```

The `-zg` option will make Tippecanoe choose a maximum zoom level that should be
high enough to reflect the precision of the original data. (If it turns out still
not to be as detailed as you want, use `-z` manually with a higher number.)

If the tiles come out too big, the `--drop-densest-as-needed` option will make
Tippecanoe try dropping what should be the least visible features at each zoom level.
(If it drops too many features, use `-x` to leave out some feature attributes that
you didn't really need.)

Examples
--------

Create a tileset of TIGER roads for Alameda County, to zoom level 13, with a custom layer name and description:

```sh
$ tippecanoe -o alameda.mbtiles -l alameda -n ""Alameda County from TIGER"" -z13 tl_2014_06001_roads.json
```

Create a tileset of all TIGER roads, at only zoom level 12, but with higher detail than normal,
with a custom layer name and description, and leaving out the `LINEARID` and `RTTYP` attributes:

```
$ cat tiger/tl_2014_*_roads.json | tippecanoe -o tiger.mbtiles -l roads -n ""All TIGER roads, one zoom"" -z12 -Z12 -d14 -x LINEARID -x RTTYP
```

Cookbook
--------

",3
195,tippecanoe," Discontinuous polygon features (buildings of Rhode Island), visible at all zoom levels","```
curl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_railroads.zip
unzip ne_10m_railroads.zip
ogr2ogr -f GeoJSON ne_10m_railroads.geojson ne_10m_railroads.shp

tippecanoe -zg -o ne_10m_railroads.mbtiles --drop-densest-as-needed --extend-zooms-if-still-dropping ne_10m_railroads.geojson
```

* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `--drop-densest-as-needed`: If the tiles are too big at low zoom levels, drop the least-visible features to allow tiles to be created with those features that remain
* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features

",3
196,tippecanoe," Continuous polygon features (states and provinces), visible at all zoom levels","```
curl -L -O https://usbuildingdata.blob.core.windows.net/usbuildings-v1-1/RhodeIsland.zip
unzip RhodeIsland.zip

tippecanoe -zg -o RhodeIsland.mbtiles --drop-densest-as-needed --extend-zooms-if-still-dropping RhodeIsland.geojson
```

* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `--drop-densest-as-needed`: If the tiles are too big at low or medium zoom levels, drop the least-visible features to allow tiles to be created with those features that remain
* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features

",3
197,tippecanoe," Large point dataset (GPS bus locations), for visualization at all zoom levels","```
curl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip
unzip -o ne_10m_admin_1_states_provinces.zip
ogr2ogr -f GeoJSON ne_10m_admin_1_states_provinces.geojson ne_10m_admin_1_states_provinces.shp

tippecanoe -zg -o ne_10m_admin_1_states_provinces.mbtiles --coalesce-densest-as-needed --extend-zooms-if-still-dropping ne_10m_admin_1_states_provinces.geojson
```

* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished
* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features

",3
198,tippecanoe," Clustered points (world cities), summing the clustered population, visible at all zoom levels","```
curl -L -O ftp://avl-data.sfmta.com/avl_data/avl_raw/sfmtaAVLRawData01012013.csv
sed 's/PREDICTABLE.*/PREDICTABLE/' sfmtaAVLRawData01012013.csv > sfmta.csv
tippecanoe -zg -o sfmta.mbtiles --drop-densest-as-needed --extend-zooms-if-still-dropping sfmta.csv
```

(The `sed` line is to clean the corrupt CSV header, which contains the wrong number of fields.)

* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `--drop-densest-as-needed`: If the tiles are too big at low or medium zoom levels, drop the least-visible features to allow tiles to be created with those features that remain
* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features

",3
199,tippecanoe, Show countries at low zoom levels but states at higher zoom levels,"```
curl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_populated_places.zip
unzip -o ne_10m_populated_places.zip
ogr2ogr -f GeoJSON ne_10m_populated_places.geojson ne_10m_populated_places.shp

tippecanoe -zg -o ne_10m_populated_places.mbtiles -r1 --cluster-distance=10 --accumulate-attribute=POP_MAX:sum ne_10m_populated_places.geojson
```

* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `-r1`: Do not automatically drop a fraction of points at low zoom levels, since clustering will be used instead
* `--cluster-distance=10`: Cluster together features that are closer than about 10 pixels from each other
* `--accumulate-attribute=POP_MAX:sum`: Sum the `POP_MAX` (population) attribute in features that are clustered together. Other attributes will be arbitrarily taken from the first feature in the cluster.

",3
200,tippecanoe, Represent multiple sources (Illinois and Indiana counties) as separate layers,"```
curl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip
unzip ne_10m_admin_0_countries.zip
ogr2ogr -f GeoJSON ne_10m_admin_0_countries.geojson ne_10m_admin_0_countries.shp

curl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip
unzip -o ne_10m_admin_1_states_provinces.zip
ogr2ogr -f GeoJSON ne_10m_admin_1_states_provinces.geojson ne_10m_admin_1_states_provinces.shp

tippecanoe -z3 -o countries-z3.mbtiles --coalesce-densest-as-needed ne_10m_admin_0_countries.geojson
tippecanoe -zg -Z4 -o states-Z4.mbtiles --coalesce-densest-as-needed --extend-zooms-if-still-dropping ne_10m_admin_1_states_provinces.geojson
tile-join -o states-countries.mbtiles countries-z3.mbtiles states-Z4.mbtiles
```

Countries:

* `-z3`: Only generate zoom levels 0 through 3
* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished

States and Provinces:

* `-Z4`: Only generate zoom levels 4 and beyond
* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished
* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features

",3
201,tippecanoe, Merge multiple sources (Illinois and Indiana counties) into the same layer,"```
curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/COUNTY/2010/tl_2010_17_county10.zip
unzip tl_2010_17_county10.zip
ogr2ogr -f GeoJSON tl_2010_17_county10.geojson tl_2010_17_county10.shp

curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/COUNTY/2010/tl_2010_18_county10.zip
unzip tl_2010_18_county10.zip
ogr2ogr -f GeoJSON tl_2010_18_county10.geojson tl_2010_18_county10.shp

tippecanoe -zg -o counties-separate.mbtiles --coalesce-densest-as-needed --extend-zooms-if-still-dropping tl_2010_17_county10.geojson tl_2010_18_county10.geojson
```

* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature
* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished
* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features

",3
202,tippecanoe, Selectively remove and replace features (Census tracts) to update a tileset,"```
curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/COUNTY/2010/tl_2010_17_county10.zip
unzip tl_2010_17_county10.zip
ogr2ogr -f GeoJSON tl_2010_17_county10.geojson tl_2010_17_county10.shp

curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/COUNTY/2010/tl_2010_18_county10.zip
unzip tl_2010_18_county10.zip
ogr2ogr -f GeoJSON tl_2010_18_county10.geojson tl_2010_18_county10.shp

tippecanoe -zg -o counties-merged.mbtiles -l counties --coalesce-densest-as-needed --extend-zooms-if-still-dropping tl_2010_17_county10.geojson tl_2010_18_county10.geojson
```

As above, but

* `-l counties`: Specify the layer name instead of letting it be derived from the source file names

",3
203,tippecanoe, Output tileset,"```
#notes: Retrieve and tile California 2000 Census tracts
curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2000/tl_2010_06_tract00.zip
unzip tl_2010_06_tract00.zip
ogr2ogr -f GeoJSON tl_2010_06_tract00.shp.json tl_2010_06_tract00.shp
tippecanoe -z11 -o tracts.mbtiles -l tracts tl_2010_06_tract00.shp.json

#notes: Create a copy of the tileset, minus Alameda County (FIPS code 001)
tile-join -j '{""*"":[""none"",[""=="",""COUNTYFP00"",""001""]]}' -f -o tracts-filtered.mbtiles tracts.mbtiles

#notes: Retrieve and tile Alameda County Census tracts for 2010
curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2010/tl_2010_06001_tract10.zip
unzip tl_2010_06001_tract10.zip
ogr2ogr -f GeoJSON tl_2010_06001_tract10.shp.json tl_2010_06001_tract10.shp
tippecanoe -z11 -o tracts-added.mbtiles -l tracts tl_2010_06001_tract10.shp.json

#notes: Merge the filtered tileset and the tileset of new tracts into a final tileset
tile-join -o tracts-final.mbtiles tracts-filtered.mbtiles tracts-added.mbtiles
```

The `-z11` option explicitly specifies the maxzoom, to make sure both the old and new tilesets have the same zoom range.

The `-j` option to `tile-join` specifies a filter, so that only the desired features will be copied to the new tileset.
This filter excludes (using `none`) any features whose FIPS code (`COUNTYFP00`) is the code for Alameda County (`001`).

Options
-------

There are a lot of options. A lot of the time you won't want to use any of them
other than `-o` _output_`.mbtiles` to name the output file, and probably `-f` to
delete the file that already exists with that name.

If you aren't sure what the right maxzoom is for your data, `-zg` will guess one for you
based on the density of features.

Tippecanoe will normally drop a fraction of point features at zooms below the maxzoom,
to keep the low-zoom tiles from getting too big. If you have a smaller data set where
all the points would fit without dropping any of them, use `-r1` to keep them all.
If you do want point dropping, but you still want the tiles to be denser than `-zg`
thinks they should be, use `-B` to set a basezoom lower than the maxzoom.

If some of your tiles are coming out too big in spite of the settings above, you will
often want to use `--drop-densest-as-needed` to drop whatever fraction of the features
is necessary at each zoom level to make that zoom level's tiles work.

If your features have a lot of attributes, use `-y` to keep only the ones you really need.

If your input is formatted as newline-delimited GeoJSON, use `-P` to make input parsing a lot faster.

",3
204,tippecanoe, Tileset description and attribution," * `-o` _file_`.mbtiles` or `--output=`_file_`.mbtiles`: Name the output file.
 * `-e` _directory_ or `--output-to-directory`=_directory_: Write tiles to the specified *directory* instead of to an mbtiles file.
 * `-f` or `--force`: Delete the mbtiles file if it already exists instead of giving an error
 * `-F` or `--allow-existing`: Proceed (without deleting existing data) if the metadata or tiles table already exists
   or if metadata fields can't be set. You probably don't want to use this.

",3
205,tippecanoe, Input files and layer names," * `-n` _name_ or `--name=`_name_: Human-readable name for the tileset (default file.json)
 * `-A` _text_ or `--attribution=`_text_: Attribution (HTML) to be shown with maps that use data from this tileset.
 * `-N` _description_ or `--description=`_description_: Description for the tileset (default file.mbtiles)

",3
206,tippecanoe, Parallel processing of input," * _name_`.json` or _name_`.geojson`: Read the named GeoJSON input file into a layer called _name_.
 * _name_`.json.gz` or _name_`.geojson.gz`: Read the named gzipped GeoJSON input file into a layer called _name_.
 * _name_`.geobuf`: Read the named Geobuf input file into a layer called _name_.
 * _name_`.csv`: Read the named CSV input file into a layer called _name_.
 * `-l` _name_ or `--layer=`_name_: Use the specified layer name instead of deriving a name from the input filename or output tileset. If there are multiple input files
   specified, the files are all merged into the single named layer, even if they try to specify individual names with `-L`.
 * `-L` _name_`:`_file.json_ or `--named-layer=`_name_`:`_file.json_: Specify layer names for individual files. If your shell supports it, you can use a subshell redirect like `-L` _name_`:<(cat dir/*.json)` to specify a layer name for the output of streamed input.
 * `-L{`_layer-json_`}` or `--named-layer={`_layer-json_`}`: Specify an input file and layer options by a JSON object. The JSON object must contain a `""file""` key to specify the filename to read from. (If the `""file""` key is an empty string, it means to read from the standard input stream.) It may also contain a `""layer""` field to specify the name of the layer, and/or a `""description""` field to specify the layer's description in the tileset metadata, and/or a `""format""` field to specify `csv` or `geobuf` file format if it is not obvious from the `name`. Example:

```
tippecanoe -z5 -o world.mbtiles -L'{""file"":""ne_10m_admin_0_countries.json"", ""layer"":""countries"", ""description"":""Natural Earth countries""}'
```

CSV input files currently support only Point geometries, from columns named `latitude`, `longitude`, `lat`, `lon`, `long`, `lng`, `x`, or `y`.

",3
207,tippecanoe, Projection of input," * `-P` or `--read-parallel`: Use multiple threads to read different parts of each GeoJSON input file at once.
   This will only work if the input is line-delimited JSON with each Feature on its
   own line, because it knows nothing of the top-level structure around the Features. Spurious ""EOF"" error
   messages may result otherwise.
   Performance will be better if the input is a named file that can be mapped into memory
   rather than a stream that can only be read sequentially.

If the input file begins with the [RFC 8142](https://tools.ietf.org/html/rfc8142) record separator,
parallel processing of input will be invoked automatically, splitting at record separators rather
than at all newlines.

Parallel processing will also be automatic if the input file is in Geobuf format.

",3
208,tippecanoe, Zoom levels," * `-s` _projection_ or `--projection=`_projection_: Specify the projection of the input data. Currently supported are `EPSG:4326` (WGS84, the default) and `EPSG:3857` (Web Mercator). In general you should use WGS84 for your input files if at all possible.

",3
209,tippecanoe, Tile resolution," * `-z` _zoom_ or `--maximum-zoom=`_zoom_: Maxzoom: the highest zoom level for which tiles are generated (default 14)
 * `-zg` or `--maximum-zoom=g`: Guess what is probably a reasonable maxzoom based on the spacing of features.
 * `-Z` _zoom_ or `--minimum-zoom=`_zoom_: Minzoom: the lowest zoom level for which tiles are generated (default 0)
 * `-ae` or `--extend-zooms-if-still-dropping`: Increase the maxzoom if features are still being dropped at that zoom level.
   The detail and simplification options that ordinarily apply only to the maximum zoom level will apply both to the originally
   specified maximum zoom and to any levels added beyond that.
 * `-R` _zoom_`/`_x_`/`_y_ or `--one-tile=`_zoom_`/`_x_`/`_y_: Set the minzoom and maxzoom to _zoom_ and produce only
   the single specified tile at that zoom level.

If you know the precision to which you want your data to be represented,
or the map scale of a corresponding printed map,
this table shows the approximate precision and scale corresponding to various
`-z` options if you use the default `-d` detail of 12:

zoom level | precision (ft) | precision (m) | map scale
---------- | -------------- | ------------- | ---------
`-z0` | 32000 ft | 10000 m | 1:320,000,000
`-z1` | 16000 ft | 5000 m | 1:160,000,000
`-z2` | 8000 ft | 2500 m | 1:80,000,000
`-z3` | 4000 ft | 1250 m | 1:40,000,000
`-z4` | 2000 ft | 600 m | 1:20,000,000
`-z5` | 1000 ft | 300 m | 1:10,000,000
`-z6` | 500 ft | 150 m | 1:5,000,000
`-z7` | 250 ft | 80 m | 1:2,500,000
`-z8` | 125 ft | 40 m | 1:1,250,000
`-z9` | 64 ft | 20 m | 1:640,000
`-z10` | 32 ft | 10 m | 1:320,000
`-z11` | 16 ft | 5 m | 1:160,000
`-z12` | 8 ft | 2 m | 1:80,000
`-z13` | 4 ft | 1 m | 1:40,000
`-z14` | 2 ft | 0.5 m | 1:20,000
`-z15` | 1 ft | 0.25 m | 1:10,000

",3
210,tippecanoe, Filtering feature attributes," * `-d` _detail_ or `--full-detail=`_detail_: Detail at max zoom level (default 12, for tile resolution of 2^12=4096)
 * `-D` _detail_ or `--low-detail=`_detail_: Detail at lower zoom levels (default 12, for tile resolution of 2^12=4096)
 * `-m` _detail_ or `--minimum-detail=`_detail_: Minimum detail that it will try if tiles are too big at regular detail (default 7)

All internal math is done in terms of a 32-bit tile coordinate system, so 1/(2^32) of the size of Earth,
or about 1cm, is the smallest distinguishable distance. If _maxzoom_ + _detail_ > 32, no additional
resolution is obtained than by using a smaller _maxzoom_ or _detail_.

",3
211,tippecanoe, Modifying feature attributes," * `-x` _name_ or `--exclude=`_name_: Exclude the named attributes from all features. You can specify multiple `-x` options to exclude several attributes. (Don't comma-separate names within a single `-x`.)
 * `-y` _name_ or `--include=`_name_: Include the named attributes in all features, excluding all those not explicitly named. You can specify multiple `-y` options to explicitly include several attributes. (Don't comma-separate names within a single `-y`.)
 * `-X` or `--exclude-all`: Exclude all attributes and encode only geometries

",3
212,tippecanoe, Filtering features by attributes," * `-T`_attribute_`:`_type_ or `--attribute-type=`_attribute_`:`_type_: Coerce the named feature _attribute_ to be of the specified _type_.
   The _type_ may be `string`, `float`, `int`, or `bool`.
   If the type is `bool`, then original attributes of `0` (or, if numeric, `0.0`, etc.), `false`, `null`, or the empty string become `false`, and otherwise become `true`.
   If the type is `float` or `int` and the original attribute was non-numeric, it becomes `0`.
   If the type is `int` and the original attribute was floating-point, it is rounded to the nearest integer.
 * `-Y`_attribute_`:`_description_ or `--attribute-description=`_attribute_`:`_description_: Set the `description` for the specified attribute in the tileset metadata to _description_ instead of the usual `String`, `Number`, or `Boolean`.
 * `-E`_attribute_`:`_operation_ or `--accumulate-attribute=`_attribute_`:`_operation_: Preserve the named _attribute_ from features
   that are dropped, coalesced-as-needed, or clustered. The _operation_ may be
   `sum`, `product`, `mean`, `max`, `min`, `concat`, or `comma`
   to specify how the named _attribute_ is accumulated onto the attribute of the same name in a feature that does survive.
 * `-pe` or `--empty-csv-columns-are-null`: Treat empty CSV columns as nulls rather than as empty strings.
 * `-aI` or `--convert-stringified-ids-to-numbers`: If a feature ID is the string representation of a number, convert it to a plain number to use as the feature ID.
 * `--use-attribute-for-id=`*name*: Use the attribute with the specified *name* as if it were specified as the feature ID. (If this attribute is a stringified number, you must also use `-aI` to convert it to a number.)

",3
213,tippecanoe, Dropping a fixed fraction of features by zoom level," * `-j` *filter* or `--feature-filter`=*filter*: Check features against a per-layer filter (as defined in the [Mapbox GL Style Specification](https://docs.mapbox.com/mapbox-gl-js/style-spec/#other-filter)) and only include those that match. Any features in layers that have no filter specified will be passed through. Filters for the layer `""*""` apply to all layers. The special variable `$zoom` refers to the current zoom level.
 * `-J` *filter-file* or `--feature-filter-file`=*filter-file*: Like `-j`, but read the filter from a file.

Example: to find the Natural Earth countries with low `scalerank` but high `LABELRANK`:

```
tippecanoe -z5 -o filtered.mbtiles -j '{ ""ne_10m_admin_0_countries"": [ ""all"", [ ""<"", ""scalerank"", 3 ], [ "">"", ""LABELRANK"", 5 ] ] }' ne_10m_admin_0_countries.geojson
```

Example: to retain only major TIGER roads at low zoom levels:

```
tippecanoe -o roads.mbtiles -j '{ ""*"": [ ""any"", [ "">="", ""$zoom"", 11 ], [ ""in"", ""MTFCC"", ""S1100"", ""S1200"" ] ] }' tl_2015_06001_roads.json
```

Tippecanoe also accepts expressions of the form `[ ""attribute-filter"", name, expression ]`, to filter individual feature attributes
instead of entire features. For example, you can exclude the road names at low zoom levels by doing

```
tippecanoe -o roads.mbtiles -j '{ ""*"": [ ""attribute-filter"", ""FULLNAME"", [ "">="", ""$zoom"", 9 ] ] }' tl_2015_06001_roads.json
```

An `attribute-filter` expression itself is always considered to evaluate to `true` (in other words, to retain the feature instead
of dropping it). If you want to use multiple `attribute-filter` expressions, or to use other expressions to remove features from
the same layer, enclose them in an `all` expression so they will all be evaluated.

",3
214,tippecanoe, Dropping a fraction of features to keep under tile size limits," * `-r` _rate_ or `--drop-rate=`_rate_: Rate at which dots are dropped at zoom levels below basezoom (default 2.5).
   If you use `-rg`, it will guess a drop rate that will keep at most 50,000 features in the densest tile.
   You can also specify a marker-width with `-rg`*width* to allow fewer features in the densest tile to
   compensate for the larger marker, or `-rf`*number* to allow at most *number* features in the densest tile.
 * `-B` _zoom_ or `--base-zoom=`_zoom_: Base zoom, the level at and above which all points are included in the tiles (default maxzoom).
   If you use `-Bg`, it will guess a zoom level that will keep at most 50,000 features in the densest tile.
   You can also specify a marker-width with `-Bg`*width* to allow fewer features in the densest tile to
   compensate for the larger marker, or `-Bf`*number* to allow at most *number* features in the densest tile.
 * `-al` or `--drop-lines`: Let ""dot"" dropping at lower zooms apply to lines too
 * `-ap` or `--drop-polygons`: Let ""dot"" dropping at lower zooms apply to polygons too
 * `-K` _distance_ or `--cluster-distance=`_distance_: Cluster points (as with `--cluster-densest-as-needed`, but without the experimental discovery process) that are approximately within _distance_ of each other. The units are tile coordinates within a nominally 256-pixel tile, so the maximum value of 255 allows only one feature per tile. Values around 10 are probably appropriate for typical marker sizes. See `--cluster-densest-as-needed` below for behavior.

",3
215,tippecanoe, Dropping tightly overlapping features," * `-as` or `--drop-densest-as-needed`: If a tile is too large, try to reduce it to under 500K by increasing the minimum spacing between features. The discovered spacing applies to the entire zoom level.
 * `-ad` or `--drop-fraction-as-needed`: Dynamically drop some fraction of features from each zoom level to keep large tiles under the 500K size limit. (This is like `-pd` but applies to the entire zoom level, not to each tile.)
 * `-an` or `--drop-smallest-as-needed`: Dynamically drop the smallest features (physically smallest: the shortest lines or the smallest polygons) from each zoom level to keep large tiles under the 500K size limit. This option will not work for point features.
 * `-aN` or `--coalesce-smallest-as-needed`: Dynamically combine the smallest features (physically smallest: the shortest lines or the smallest polygons) from each zoom level into other nearby features to keep large tiles under the 500K size limit. This option will not work for point features, and will probably not help very much with LineStrings. It is mostly intended for polygons, to maintain the full original area covered by polygons while still reducing the feature count somehow. The attributes of the small polygons are *not* preserved into the combined features (except through `--accumulate-attribute`), only their geometry. Furthermore, the polygons to which nested polygons are coalesced may not necessarily be the immediately enclosing features.
 * `-aD` or `--coalesce-densest-as-needed`: Dynamically combine the densest features from each zoom level into other nearby features to keep large tiles under the 500K size limit. (Again, mostly useful for polygons.)
 * `-aS` or `--coalesce-fraction-as-needed`: Dynamically combine a fraction of features from each zoom level into other nearby features to keep large tiles under the 500K size limit. (Again, mostly useful for polygons.)
 * `-pd` or `--force-feature-limit`: Dynamically drop some fraction of features from large tiles to keep them under the 500K size limit. It will probably look ugly at the tile boundaries. (This is like `-ad` but applies to each tile individually, not to the entire zoom level.) You probably don't want to use this.
 * `-aC` or `--cluster-densest-as-needed`: If a tile is too large, try to reduce its size by increasing the minimum spacing between features, and leaving one placeholder feature from each group.  The remaining feature will be given a `""cluster"": true` attribute to indicate that it represents a cluster, a `""point_count""` attribute to indicate the number of features that were clustered into it, and a `""sqrt_point_count""` attribute to indicate the relative width of a feature to represent the cluster. If the features being clustered are points, the representative feature will be located at the average of the original points' locations; otherwise, one of the original features will be left as the representative.

",3
216,tippecanoe, Line and polygon simplification," * `-g` _gamma_ or `--gamma=_gamma`_: Rate at which especially dense dots are dropped (default 0, for no effect). A gamma of 2 reduces the number of dots less than a pixel apart to the square root of their original number.
 * `-aG` or `--increase-gamma-as-needed`: If a tile is too large, try to reduce it to under 500K by increasing the `-g` gamma. The discovered gamma applies to the entire zoom level. You probably want to use `--drop-densest-as-needed` instead.

",3
217,tippecanoe, Attempts to improve shared polygon boundaries," * `-S` _scale_ or `--simplification=`_scale_: Multiply the tolerance for line and polygon simplification by _scale_. The standard tolerance tries to keep
   the line or polygon within one tile unit of its proper location. You can probably go up to about 10 without too much visible difference.
 * `-ps` or `--no-line-simplification`: Don't simplify lines and polygons
 * `-pS` or `--simplify-only-low-zooms`: Don't simplify lines and polygons at maxzoom (but do simplify at lower zooms)
 * `-pn` or `--no-simplification-of-shared-nodes`: Don't simplify away nodes that appear in more than one feature or are used multiple times within the same feature, so that the intersection node will not be lost from intersecting roads. (This will not be effective if you also use `--coalesce` or `--detect-shared-borders`.)
 * `-pt` or `--no-tiny-polygon-reduction`: Don't combine the area of very small polygons into small squares that represent their combined area.

",3
218,tippecanoe, Controlling clipping to tile boundaries," * `-ab` or `--detect-shared-borders`: In the manner of [TopoJSON](https://github.com/mbostock/topojson/wiki/Introduction), detect borders that are shared between multiple polygons and simplify them identically in each polygon. This takes more time and memory than considering each polygon individually.
 * `-aL` or `--grid-low-zooms`: At all zoom levels below _maxzoom_, snap all lines and polygons to a stairstep grid instead of allowing diagonals. You will also want to specify a tile resolution, probably `-D8`. This option provides a way to display continuous parcel, gridded, or binned data at low zooms without overwhelming the tiles with tiny polygons, since features will either get stretched out to the grid unit or lost entirely, depending on how they happened to be aligned in the original data. You probably don't want to use this.

",3
219,tippecanoe, Reordering features within each tile," * `-b` _pixels_ or `--buffer=`_pixels_: Buffer size where features are duplicated from adjacent tiles. Units are ""screen pixels""—1/256th of the tile width or height. (default 5)
 * `-pc` or `--no-clipping`: Don't clip features to the size of the tile. If a feature overlaps the tile's bounds or buffer at all, it is included completely. Be careful: this can produce very large tilesets, especially with large polygons.
 * `-pD` or `--no-duplication`: As with `--no-clipping`, each feature is included intact instead of cut to tile boundaries. In addition, it is included only in a single tile per zoom level rather than potentially in multiple copies. Clients of the tileset must check adjacent tiles (possibly some distance away) to ensure they have all features.

",3
220,tippecanoe, Adding calculated attributes," * `-pi` or `--preserve-input-order`: Preserve the original input order of features as the drawing order instead of ordering geographically. (This is implemented as a restoration of the original order at the end, so that dot-dropping is still geographic, which means it also undoes `-ao`).
 * `-ac` or `--coalesce`: Coalesce consecutive features that have the same attributes. This can be useful if you have lots of small polygons with identical attributes and you would like to merge them together.
 * `-ao` or `--reorder`: Reorder features to put ones with the same attributes in sequence (instead of ones that are approximately spatially adjacent), to try to get them to coalesce. You probably want to use this if you use `--coalesce`.
 * `-ar` or `--reverse`: Try reversing the directions of lines to make them coalesce and compress better. You probably don't want to use this.
 * `-ah` or `--hilbert`: Put features in Hilbert Curve order instead of the usual Z-Order. This improves the odds that spatially adjacent features will be sequentially adjacent, and should improve density calculations and spatial coalescing. It should be the default eventually.

",3
221,tippecanoe, Trying to correct bad source geometry," * `-ag` or `--calculate-feature-density`: Add a new attribute, `tippecanoe_feature_density`, to each feature, to record how densely features are spaced in that area of the tile. You can use this attribute in the style to produce a glowing effect where points are densely packed. It can range from 0 in the sparsest areas to 255 in the densest.
 * `-ai` or `--generate-ids`: Add an `id` (a feature ID, not an attribute named `id`) to each feature that does not already have one. There is currently no guarantee that the `id` added will be stable between runs or that it will not conflict with manually-assigned feature IDs. Future versions of Tippecanoe may change the mechanism for allocating IDs.

",3
222,tippecanoe, Setting or disabling tile size limits," * `-aw` or `--detect-longitude-wraparound`: Detect when consecutive points within a feature jump to the other side of the world, and try to fix the geometry.
 * `-pw` or `--use-source-polygon-winding`: Instead of respecting GeoJSON polygon ring order, use the original polygon winding in the source data to distinguish inner (clockwise) and outer (counterclockwise) polygon rings.
 * `-pW` or `--reverse-source-polygon-winding`: Instead of respecting GeoJSON polygon ring order, use the opposite of the original polygon winding in the source data to distinguish inner (counterclockwise) and outer (clockwise) polygon rings.
 * `--clip-bounding-box=`*minlon*`,`*minlat*`,`*maxlon*`,`*maxlat*: Clip all features to the specified bounding box.

",3
223,tippecanoe, Temporary storage," * `-M` _bytes_ or `--maximum-tile-bytes=`_bytes_: Use the specified number of _bytes_ as the maximum compressed tile size instead of 500K.
 * `-O` _features_ or `--maximum-tile-features=`_features_: Use the specified number of _features_ as the maximum in a tile instead of 200,000.
 * `-pf` or `--no-feature-limit`: Don't limit tiles to 200,000 features
 * `-pk` or `--no-tile-size-limit`: Don't limit tiles to 500K bytes
 * `-pC` or `--no-tile-compression`: Don't compress the PBF vector tile data.
 * `-pg` or `--no-tile-stats`: Don't generate the `tilestats` row in the tileset metadata. Uploads without [tilestats](https://github.com/mapbox/mapbox-geostats) will take longer to process.
 * `--tile-stats-attributes-limit=`*count*: Include `tilestats` information about at most *count* attributes instead of the default 1000.
 * `--tile-stats-sample-values-limit=`*count*: Calculate `tilestats` attribute statistics based on *count* values instead of the default 1000.
 * `--tile-stats-values-limit=`*count*: Report *count* unique attribute values in `tilestats` instead of the default 100.

",3
224,tippecanoe, Progress indicator," * `-t` _directory_ or `--temporary-directory=`_directory_: Put the temporary files in _directory_.
   If you don't specify, it will use `/tmp`.

",3
225,tippecanoe, Filters," * `-q` or `--quiet`: Work quietly instead of reporting progress or warning messages
 * `-Q` or `--no-progress-indicator`: Don't report progress, but still give warnings
 * `-U` _seconds_ or `--progress-interval=`_seconds_: Don't report progress more often than the specified number of _seconds_.
 * `-v` or `--version`: Report Tippecanoe's version number

",3
226,tippecanoe, Examples:," * `-C` _command_ or `--prefilter=`_command_: Specify a shell filter command to be run at the start of assembling each tile
 * `-c` _command_ or `--postfilter=`_command_: Specify a shell filter command to be run at the end of assembling each tile

The pre- and post-filter commands allow you to do optional filtering or transformation on the features of each tile
as it is created. They are shell commands, run with the zoom level, X, and Y as the `$1`, `$2`, and `$3` arguments.
Future versions of Tippecanoe may add additional arguments for more context.

The features are provided to the filter
as a series of newline-delimited GeoJSON objects on the standard input, and `tippecanoe` expects to read another
set of GeoJSON features from the filter's standard output.

The prefilter receives the features at the highest available resolution, before line simplification,
polygon topology repair, gamma calculation, dynamic feature dropping, or other internal processing.
The postfilter receives the features at tile resolution, after simplification, cleaning, and dropping.

The layer name is provided as part of the `tippecanoe` element of the feature and must be passed through
to keep the feature in its correct layer. In the case of the prefilter, the `tippecanoe` element may also
contain `index`, `sequence`, `extent`, and `dropped`, elements, which must be passed through for internal operations like
`--drop-densest-as-needed`, `--drop-smallest-as-needed`, and `--preserve-input-order` to work.

",4
227,tippecanoe, Output tileset," * Make a tileset of the Natural Earth countries to zoom level 5, and also copy the GeoJSON features
   to files in a `tiles/z/x/y.geojson` directory hierarchy.

```
tippecanoe -o countries.mbtiles -z5 -C 'mkdir -p tiles/$1/$2; tee tiles/$1/$2/$3.geojson' ne_10m_admin_0_countries.json
```

 * Make a tileset of the Natural Earth countries to zoom level 5, but including only those tiles that
   intersect the [bounding box of Germany](https://www.flickr.com/places/info/23424829).
   (The `limit-tiles-to-bbox` script is [in the Tippecanoe source directory](filters/limit-tiles-to-bbox).)

```
tippecanoe -o countries.mbtiles -z5 -C './filters/limit-tiles-to-bbox 5.8662 47.2702 15.0421 55.0581 $*' ne_10m_admin_0_countries.json
```

 * Make a tileset of TIGER roads in Tippecanoe County, leaving out all but primary and secondary roads (as [classified by TIGER](https://www.census.gov/geo/reference/mtfcc.html)) below zoom level 11.

```
tippecanoe -o roads.mbtiles -c 'if [ $1 -lt 11 ]; then grep ""\""MTFCC\"": \""S1[12]00\""""; else cat; fi' tl_2016_18157_roads.json
```

Environment
-----------

Tippecanoe ordinarily uses as many parallel threads as the operating system claims that CPUs are available.
You can override this number by setting the `TIPPECANOE_MAX_THREADS` environmental variable.

GeoJSON extension
-----------------

Tippecanoe defines a GeoJSON extension that you can use to specify the minimum and/or maximum zoom level
at which an individual feature will be included in the vector tileset being produced.
If you have a feature like this:

```
{
    ""type"" : ""Feature"",
    ""tippecanoe"" : { ""maxzoom"" : 9, ""minzoom"" : 4 },
    ""properties"" : { ""FULLNAME"" : ""N Vasco Rd"" },
    ""geometry"" : {
        ""type"" : ""LineString"",
        ""coordinates"" : [ [ -121.733350, 37.767671 ], [ -121.733600, 37.767483 ], [ -121.733131, 37.766952 ] ]
    }
}
```

with a `tippecanoe` object specifiying a `maxzoom` of 9 and a `minzoom` of 4, the feature
will only appear in the vector tiles for zoom levels 4 through 9. Note that the `tippecanoe`
object belongs to the Feature, not to its `properties`. If you specify a `minzoom` for a feature,
it will be preserved down to that zoom level even if dot-dropping with `-r` would otherwise have
dropped it.

You can also specify a layer name in the `tippecanoe` object, which will take precedence over
the filename or name specified using `--layer`, like this:

```
{
    ""type"" : ""Feature"",
    ""tippecanoe"" : { ""layer"" : ""streets"" },
    ""properties"" : { ""FULLNAME"" : ""N Vasco Rd"" },
    ""geometry"" : {
        ""type"" : ""LineString"",
        ""coordinates"" : [ [ -121.733350, 37.767671 ], [ -121.733600, 37.767483 ], [ -121.733131, 37.766952 ] ]
    }
}
```

If your source GeoJSON only has `minzoom`, `maxzoom` and/or `layer` within `properties` you can use [ndjson-cli](https://github.com/mbostock/ndjson-cli/blob/master/README.md) to move them into the required `tippecanoe` object by piping the GeoJSON like this:

```sh
ndjson-map 'd.tippecanoe = { minzoom: d.properties.minzoom, maxzoom: d.properties.maxzoom, layer: d.properties.layer }, delete d.properties.minzoom, delete d.properties.maxzoom, delete d.properties.layer, d'
```

Geometric simplifications
-------------------------

At every zoom level, line and polygon features are subjected to Douglas-Peucker
simplification to the resolution of the tile.

For point features, it drops 1/2.5 of the dots for each zoom level above the
point base zoom (which is normally the same as the `-z` max zoom, but can be
a different zoom specified with `-B` if you have precise but sparse data).
I don't know why 2.5 is the appropriate number, but the densities of many different
data sets fall off at about this same rate. You can use -r to specify a different rate.

You can use the gamma option to thin out especially dense clusters of points.
For any area where dots are closer than one pixel together (at whatever zoom level),
a gamma of 3, for example, will reduce these clusters to the cube root of their original density.

For line features, it drops any features that are too small to draw at all.
This still leaves the lower zooms too dark (and too dense for the 500K tile limit,
in some places), so I need to figure out an equitable way to throw features away.

Unless you specify `--no-tiny-polygon-reduction`,
any polygons that are smaller than a minimum area (currently 4 square subpixels) will
have their probability diffused, so that some of them will be drawn as a square of
this minimum size and others will not be drawn at all, preserving the total area that
all of them should have had together.

Features in the same tile that share the same type and attributes are coalesced
together into a single geometry if you use `--coalesce`. You are strongly encouraged to use -x to exclude
any unnecessary attributes to reduce wasted file size.

If a tile is larger than 500K, it will try encoding that tile at progressively
lower resolutions before failing if it still doesn't fit.

Development
-----------

Requires sqlite3 and zlib (should already be installed on MacOS). Rebuilding the manpage
uses md2man (`gem install md2man`).

Linux:

    sudo apt-get install build-essential libsqlite3-dev zlib1g-dev

Then build:

    make

and perhaps

    make install

Tippecanoe now requires features from the 2011 C++ standard. If your compiler is older than
that, you will need to install a newer one. On MacOS, updating to the lastest XCode should
get you a new enough version of `clang++`. On Linux, you should be able to upgrade `g++` with

```
sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test
sudo apt-get update -y
sudo apt-get install -y g++-5
export CXX=g++-5
```

Docker Image
------------

A tippecanoe Docker image can be built from source and executed as a task to
automatically install dependencies and allow tippecanoe to run on any system
supported by Docker.

```docker
$ docker build -t tippecanoe:latest .
$ docker run -it --rm \
  -v /tiledata:/data \
  tippecanoe:latest \
  tippecanoe --output=/data/output.mbtiles /data/example.geojson
```

The commands above will build a Docker image from the source and compile the
latest version. The image supports all tippecanoe flags and options.

Examples
------

Check out [some examples of maps made with tippecanoe](MADE_WITH.md)

Name
----

The name is [a joking reference](http://en.wikipedia.org/wiki/Tippecanoe_and_Tyler_Too) to a ""tiler"" for making map tiles.

tile-join
=========

Tile-join is a tool for copying and merging vector mbtiles files and for
joining new attributes from a CSV file to existing features in them.

It reads the tiles from an
existing .mbtiles file or a directory of tiles, matches them against the
records of the CSV (if one is specified), and writes out a new tileset.

If you specify multiple source mbtiles files or source directories of tiles,
all the sources are read and their combined contents are written to the new
mbtiles output. If they define the same layers or the same tiles, the layers
or tiles are merged.

The options are:

",3
228,tippecanoe, Tileset description and attribution," * `-o` *out.mbtiles* or `--output=`*out.mbtiles*: Write the new tiles to the specified .mbtiles file.
 * `-e` *directory* or `--output-to-directory=`*directory*: Write the new tiles to the specified directory instead of to an mbtiles file.
 * `-f` or `--force`: Remove *out.mbtiles* if it already exists.

",3
229,tippecanoe, Layer filtering and naming," * `-A` *attribution* or `--attribution=`*attribution*: Set the attribution string.
 * `-n` *name* or `--name=`*name*: Set the tileset name.
 * `-N` *description* or `--description=`*description*: Set the tileset description.

",3
230,tippecanoe, Zoom levels," * `-l` *layer* or `--layer=`*layer*: Include the named layer in the output. You can specify multiple `-l` options to keep multiple layers. If you don't specify, they will all be retained.
 * `-L` *layer* or `--exclude-layer=`*layer*: Remove the named layer from the output. You can specify multiple `-L` options to remove multiple layers.
 * `-R`*old*`:`*new* or `--rename-layer=`*old*`:`*new*: Rename the layer named *old* to be named *new* instead. You can specify multiple `-R` options to rename multiple layers. Renaming happens before filtering.

",3
231,tippecanoe, Merging attributes from a CSV file," * `-z` _zoom_ or `--maximum-zoom=`_zoom_: Don't copy tiles from higher zoom levels than the specified zoom
 * `-Z` _zoom_ or `--minimum-zoom=`_zoom_: Don't copy tiles from lower zoom levels than the specified zoom

",3
232,tippecanoe, Filtering features and feature attributes," * `-c` *match*`.csv` or `--csv=`*match*`.csv`: Use *match*`.csv` as the source for new attributes to join to the features. The first line of the file should be the key names; the other lines are values. The first column is the one to match against the existing features; the other columns are the new data to add.

",3
233,tippecanoe, Setting or disabling tile size limits," * `-x` *key* or `--exclude=`*key*: Remove attributes of type *key* from the output. You can use this to remove the field you are matching against if you no longer need it after joining, or to remove any other attributes you don't want.
 * `-X` or `--exclude-all`: Remove all attributes from the output.
 * `-i` or `--if-matched`: Only include features that matched the CSV.
 * `-j` *filter* or `--feature-filter`=*filter*: Check features against a per-layer filter (as defined in the [Mapbox GL Style Specification](https://docs.mapbox.com/mapbox-gl-js/style-spec/#other-filter)) and only include those that match. Any features in layers that have no filter specified will be passed through. Filters for the layer `""*""` apply to all layers.
 * `-J` *filter-file* or `--feature-filter-file`=*filter-file*: Like `-j`, but read the filter from a file.
 * `-pe` or `--empty-csv-columns-are-null`: Treat empty CSV columns as nulls rather than as empty strings.

",3
234,tippecanoe, Options," * `-pk` or `--no-tile-size-limit`: Don't skip tiles larger than 500K.
 * `-pC` or `--no-tile-compression`: Don't compress the PBF vector tile data.
 * `-pg` or `--no-tile-stats`: Don't generate the `tilestats` row in the tileset metadata. Uploads without [tilestats](https://github.com/mapbox/mapbox-geostats) will take longer to process.

Because tile-join just copies the geometries to the new .mbtiles without processing them
(except to rescale the extents if necessary),
it doesn't have any of tippecanoe's recourses if the new tiles are bigger than the 500K tile limit.
If a tile is too big and you haven't specified `-pk`, it is just left out of the new tileset.

Example
-------

Imagine you have a tileset of census blocks:

```sh
curl -O http://www2.census.gov/geo/tiger/TIGER2010/TABBLOCK/2010/tl_2010_06001_tabblock10.zip
unzip tl_2010_06001_tabblock10.zip
ogr2ogr -f GeoJSON tl_2010_06001_tabblock10.json tl_2010_06001_tabblock10.shp
./tippecanoe -o tl_2010_06001_tabblock10.mbtiles tl_2010_06001_tabblock10.json
```

and a CSV of their populations:

```sh
curl -O http://www2.census.gov/census_2010/01-Redistricting_File--PL_94-171/California/ca2010.pl.zip
unzip -p ca2010.pl.zip cageo2010.pl |
awk 'BEGIN {
    print ""GEOID10,population""
}
(substr($0, 9, 3) == ""750"") {
    print ""\"""" substr($0, 28, 2) substr($0, 30, 3) substr($0, 55, 6) substr($0, 62, 4) ""\"","" (0 + substr($0, 328, 9))
}' > population.csv
```

which looks like this:

```
GEOID10,population
""060014277003018"",0
""060014283014046"",0
""060014284001020"",0
...
""060014507501001"",202
""060014507501002"",119
""060014507501003"",193
""060014507501004"",85
...
```

Then you can join those populations to the geometries and discard the no-longer-needed ID field:

```sh
./tile-join -o population.mbtiles -x GEOID10 -c population.csv tl_2010_06001_tabblock10.mbtiles
```

tippecanoe-enumerate
====================

The `tippecanoe-enumerate` utility lists the tiles that an `mbtiles` file defines.
Each line of the output lists the name of the `mbtiles` file and the zoom, x, and y
coordinates of one of the tiles. It does basically the same thing as

    select zoom_level, tile_column, (1 << zoom_level) - 1 - tile_row from tiles;

on the file in sqlite3.

tippecanoe-decode
=================

The `tippecanoe-decode` utility turns vector mbtiles back to GeoJSON. You can use it either
on an entire file:

    tippecanoe-decode file.mbtiles

or on an individual tile:

    tippecanoe-decode file.mbtiles zoom x y
    tippecanoe-decode file.vector.pbf zoom x y

Unless you use `-c`, the output is a set of nested FeatureCollections identifying each
tile and layer separately. Note that the same features generally appear at all zooms,
so the output for the file will have many copies of the same features at different
resolutions.

",3
235,tippecanoe, Options," * `-s` _projection_ or `--projection=`*projection*: Specify the projection of the output data. Currently supported are EPSG:4326 (WGS84, the default) and EPSG:3857 (Web Mercator).
 * `-z` _maxzoom_ or `--maximum-zoom=`*maxzoom*: Specify the highest zoom level to decode from the tileset
 * `-Z` _minzoom_ or `--minimum-zoom=`*minzoom*: Specify the lowest zoom level to decode from the tileset
 * `-l` _layer_ or `--layer=`*layer*: Decode only layers with the specified names. (Multiple `-l` options can be specified.)
 * `-c` or `--tag-layer-and-zoom`: Include each feature's layer and zoom level as part of its `tippecanoe` object rather than as a FeatureCollection wrapper
 * `-S` or `--stats`: Just report statistics about each tile's size and the number of features in it, as a JSON structure.
 * `-f` or `--force`: Decode tiles even if polygon ring order or closure problems are detected

tippecanoe-json-tool
====================

Extracts GeoJSON features or standalone geometries as line-delimited JSON objects from a larger JSON file,
following the same extraction rules that Tippecanoe uses when parsing JSON.

    tippecanoe-json-tool file.json [... file.json]

Optionally also wraps them in a FeatureCollection or GeometryCollection as appropriate.

Optionally extracts an attribute from the GeoJSON `properties` for sorting.

Optionally joins a sorted CSV of new attributes to a sorted GeoJSON file.

The reason for requiring sorting is so that it is possible to work on CSV and GeoJSON files that are larger
than can comfortably fit in memory by streaming through them in parallel, in the same way that the Unix
`join` command does. The Unix `sort` command can be used to sort large files to prepare them for joining.

The sorting interface is weird, and future version of `tippecanoe-json-tool` will replace it with
something better.

",3
236,tippecanoe, Example," * `-w` or `--wrap`: Add the FeatureCollection or GeometryCollection wrapper.
 * `-e` *attribute* or `--extract=`*attribute*: Extract the named attribute as a prefix to each feature.
   The formatting makes excessive use of `\u` quoting so that it follows JSON string rules but will still
   be sorted correctly by tools that just do ASCII comparisons.
 * `-c` *file.csv* or `--csv=`*file.csv*: Join attributes from the named sorted CSV file, using its first column as the join key. Geometries will be passed through even if they do not match the CSV; CSV lines that do not match a geometry will be discarded.
 * `-pe` or `--empty-csv-columns-are-null`: Treat empty CSV columns as nulls rather than as empty strings.

",3
237,lasio, lasio,,1
238,lasio, Documentation,"![Status](https://img.shields.io/badge/status-beta-yellow.svg)
[![Version](http://img.shields.io/pypi/v/lasio.svg)](https://pypi.python.org/pypi/lasio/)
[![License](http://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/kinverarity1/lasio/blob/master/LICENSE)
[![Python versions](https://img.shields.io/pypi/pyversions/lasio.svg)](https://www.python.org/downloads/)
[![Build Status](https://travis-ci.org/kinverarity1/lasio.svg?branch=master)](https://travis-ci.org/kinverarity1/lasio)
[![Build status](https://ci.appveyor.com/api/projects/status/csr7bg8urkbtbq4n?svg=true)](https://ci.appveyor.com/project/kinverarity1/lasio)
[![Docker Build](https://img.shields.io/docker/build/kinverarity/lasio.svg)](https://hub.docker.com/r/kinverarity/lasio/)

This is a Python 2.7 and 3.3+ package to read and write Log ASCII Standard (LAS) files, used for borehole data such as geophysical, geological, or petrophysical logs. It's compatible with versions 1.2 and 2.0 of the LAS file specification, published by the [Canadian Well Logging Society](http://www.cwls.org/las). Support for LAS 3 is being worked on. In principle it is designed to read as many types of LAS files as possible, including ones containing common errors or non-compliant formatting.

Depending on your particular application you may also want to check out [striplog](https://github.com/agile-geoscience/striplog) for stratigraphic/lithological data, or [welly](https://github.com/agile-geoscience/welly) for dealing with data at the well level. lasio is primarily for reading & writing LAS files.

Note this is *not* a package for reading LiDAR data (also called ""LAS files"").

",2
239,lasio, Quick start,"See here for the [complete lasio package documentation](https://lasio.readthedocs.io/en/latest/).

",2
240,lasio, License,"Install the usual way:

```bash
$ pip install lasio
```

Very quick example session:

```python
>>> import lasio
>>> las = lasio.read(""sample_big.las"")
```

Data is accessible both directly as numpy arrays

```python
>>> las.keys()
['DEPT', 'DT', 'RHOB', 'NPHI', 'SFLU', 'SFLA', 'ILM', 'ILD']
>>> las['SFLU']
array([ 123.45,  123.45,  123.45, ...,  123.45,  123.45,  123.45])
>>> las['DEPT']
array([ 1670.   ,  1669.875,  1669.75 , ...,  1669.75 ,  1670.   ,
        1669.875])
```

and as ``CurveItem`` objects with associated metadata:

```python
>>> las.curves
[CurveItem(mnemonic=DEPT, unit=M, value=, descr=1  DEPTH, original_mnemonic=DEPT, data.shape=(29897,)), 
CurveItem(mnemonic=DT, unit=US/M, value=, descr=2  SONIC TRANSIT TIME, original_mnemonic=DT, data.shape=(29897,)), 
CurveItem(mnemonic=RHOB, unit=K/M3, value=, descr=3  BULK DENSITY, original_mnemonic=RHOB, data.shape=(29897,)), 
CurveItem(mnemonic=NPHI, unit=V/V, value=, descr=4   NEUTRON POROSITY, original_mnemonic=NPHI, data.shape=(29897,)), 
CurveItem(mnemonic=SFLU, unit=OHMM, value=, descr=5  RXO RESISTIVITY, original_mnemonic=SFLU, data.shape=(29897,)), 
CurveItem(mnemonic=SFLA, unit=OHMM, value=, descr=6  SHALLOW RESISTIVITY, original_mnemonic=SFLA, data.shape=(29897,)), 
CurveItem(mnemonic=ILM, unit=OHMM, value=, descr=7  MEDIUM RESISTIVITY, original_mnemonic=ILM, data.shape=(29897,)), 
CurveItem(mnemonic=ILD, unit=OHMM, value=, descr=8  DEEP RESISTIVITY, original_mnemonic=ILD, data.shape=(29897,))]
```

Header information is parsed into simple HeaderItem objects, and stored in a dictionary for each section of the header:

```python
>>> las.version
[HeaderItem(mnemonic=VERS, unit=, value=1.2, descr=CWLS LOG ASCII STANDARD -VERSION 1.2, original_mnemonic=VERS), 
HeaderItem(mnemonic=WRAP, unit=, value=NO, descr=ONE LINE PER DEPTH STEP, original_mnemonic=WRAP)]
>>> las.well
[HeaderItem(mnemonic=STRT, unit=M, value=1670.0, descr=, original_mnemonic=STRT), 
HeaderItem(mnemonic=STOP, unit=M, value=1660.0, descr=, original_mnemonic=STOP), 
HeaderItem(mnemonic=STEP, unit=M, value=-0.125, descr=, original_mnemonic=STEP), 
HeaderItem(mnemonic=NULL, unit=, value=-999.25, descr=, original_mnemonic=NULL), 
HeaderItem(mnemonic=COMP, unit=, value=ANY OIL COMPANY LTD., descr=COMPANY, original_mnemonic=COMP), 
HeaderItem(mnemonic=WELL, unit=, value=ANY ET AL OIL WELL #notes:12, descr=WELL, original_mnemonic=WELL), 
HeaderItem(mnemonic=FLD, unit=, value=EDAM, descr=FIELD, original_mnemonic=FLD), 
HeaderItem(mnemonic=LOC, unit=, value=A9-16-49, descr=LOCATION, original_mnemonic=LOC), 
HeaderItem(mnemonic=PROV, unit=, value=SASKATCHEWAN, descr=PROVINCE, original_mnemonic=PROV), 
HeaderItem(mnemonic=SRVC, unit=, value=ANY LOGGING COMPANY LTD., descr=SERVICE COMPANY, original_mnemonic=SRVC), 
HeaderItem(mnemonic=DATE, unit=, value=25-DEC-1988, descr=LOG DATE, original_mnemonic=DATE), 
HeaderItem(mnemonic=UWI, unit=, value=100091604920, descr=UNIQUE WELL ID, original_mnemonic=UWI)]
>>> las.params
[HeaderItem(mnemonic=BHT, unit=DEGC, value=35.5, descr=BOTTOM HOLE TEMPERATURE, original_mnemonic=BHT), 
HeaderItem(mnemonic=BS, unit=MM, value=200.0, descr=BIT SIZE, original_mnemonic=BS), 
HeaderItem(mnemonic=FD, unit=K/M3, value=1000.0, descr=FLUID DENSITY, original_mnemonic=FD), 
HeaderItem(mnemonic=MATR, unit=, value=0.0, descr=NEUTRON MATRIX(0=LIME,1=SAND,2=DOLO), original_mnemonic=MATR), 
HeaderItem(mnemonic=MDEN, unit=, value=2710.0, descr=LOGGING MATRIX DENSITY, original_mnemonic=MDEN), 
HeaderItem(mnemonic=RMF, unit=OHMM, value=0.216, descr=MUD FILTRATE RESISTIVITY, original_mnemonic=RMF), 
HeaderItem(mnemonic=DFD, unit=K/M3, value=1525.0, descr=DRILL FLUID DENSITY, original_mnemonic=DFD)]
```

The data is stored as a 2D numpy array:

```python
>>> las.data
array([[ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       ...,
       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ]])
```

You can also retrieve and load data as a ``pandas`` DataFrame, build LAS files from scratch, 
write them back to disc, and export to Excel, amongst other things.

See the [documentation](https://lasio.readthedocs.io/en/latest/) for more details.

",2
241,DID-MDN, DID-MDN,,1
242,DID-MDN, Density-aware Single Image De-raining using a Multi-stream Dense Network,,2
243,DID-MDN, Prerequisites:,"[He Zhang](https://sites.google.com/site/hezhangsprinter), [Vishal M. Patel](http://www.rci.rutgers.edu/~vmp93/)

[[Paper Link](https://arxiv.org/abs/1802.07412)] (CVPR'18)

We present a novel density-aware multi-stream densely connected convolutional neural
network-based algorithm, called DID-MDN, for joint rain density estimation and de-raining. The proposed method
enables the network itself to automatically determine the rain-density information and then efficiently remove the
corresponding rain-streaks guided by the estimated rain-density label. To better characterize rain-streaks with dif-
ferent scales and shapes, a multi-stream densely connected de-raining network is proposed which efficiently leverages
features from different scales. Furthermore, a new dataset containing images with rain-density labels is created and
used to train the proposed density-aware network. 

	@inproceedings{derain_zhang_2018,		
	  title={Density-aware Single Image De-raining using a Multi-stream Dense Network},
	  author={Zhang, He and Patel, Vishal M},
	  booktitle={CVPR},
	  year={2018}
	} 

<p align=""center"">
<img src=""sample_results/121_input.jpg"" width=""300px"" height=""200px""/>         <img src=""sample_results/121_our.jpg"" width=""300px"" height=""200px""/>
<img src=""sample_results/38_input.jpg"" width=""300px"" height=""200px""/>         <img src=""sample_results/38_our.jpg"" width=""300px"" height=""200px""/>
</p>



",2
244,DID-MDN, Installation:,"1. Linux
2. Python 2 or 3
3. CPU or NVIDIA GPU + CUDA CuDNN (CUDA 8.0)
 
",2
245,DID-MDN, Demo using pre-trained model,"1. Install PyTorch and dependencies from http://pytorch.org (Ubuntu+Python2.7)
   (conda install pytorch torchvision -c pytorch)

2. Install Torch vision from the source.
   (git clone https://github.com/pytorch/vision
   cd vision
   python setup.py install)

3. Install python package: 
   numpy, scipy, PIL, pdb
   
",2
246,DID-MDN, Training (Density-aware Deraining network using GT label),"	python test.py --dataroot ./facades/github --valDataroot ./facades/github --netG ./pre_trained/netG_epoch_9.pth   
Pre-trained model can be downloaded at (put it in the folder 'pre_trained'): https://drive.google.com/drive/folders/1VRUkemynOwWH70bX9FXL4KMWa4s_PSg2?usp=sharing

Pre-trained density-aware model can be downloaded at (Put it in the folder 'classification'): https://drive.google.com/drive/folders/1-G86JTvv7o1iTyfB2YZAQTEHDtSlEUKk?usp=sharing

Pre-trained residule-aware model can be downloaded at (Put it in the folder 'residual_heavy'): https://drive.google.com/drive/folders/1bomrCJ66QVnh-WduLuGQhBC-aSWJxPmI?usp=sharing

",2
247,DID-MDN, Density-estimation Training (rain-density classifier),"	python derain_train_2018.py  --dataroot ./facades/DID-MDN-training/Rain_Medium/train2018new  --valDataroot ./facades/github --exp ./check --netG ./pre_trained/netG_epoch_9.pth.
	Make sure you download the training sample and put in the right folder

",2
248,DID-MDN, Testing,"	python train_rain_class.py  --dataroot ./facades/DID-MDN-training/Rain_Medium/train2018new  --exp ./check_class	

",2
249,DID-MDN, Reproduce,"	python demo.py --dataroot ./your_dataroot --valDataroot ./your_dataroot --netG ./pre_trained/netG_epoch_9.pth   

",2
250,DID-MDN, Dataset,"To reproduce the quantitative results shown in the paper, please save both generated and target using python demo.py  into the .png format and then test using offline tool such as the PNSR and SSIM measurement in Python or Matlab.   In addition, please use netG.train() for testing since the batch for training is 1. 
 
",2
251,DID-MDN, Acknowledgments,"Training (heavy, medium, light) and testing (TestA and Test B) data can be downloaded at the following link:
https://drive.google.com/file/d/1cMXWICiblTsRl1zjN8FizF5hXOpVOJz4/view?usp=sharing

",2
252,bootstrap, Table of contents,"  <a href=""https://getbootstrap.com/"">
    <img src=""https://getbootstrap.com/docs/4.3/assets/brand/bootstrap-solid.svg"" alt=""Bootstrap logo"" width=""72"" height=""72"">
  </a>
</p>

<h3 align=""center"">Bootstrap</h3>

<p align=""center"">
  Sleek, intuitive, and powerful front-end framework for faster and easier web development.
  <br>
  <a href=""https://getbootstrap.com/docs/4.3/""><strong>Explore Bootstrap docs »</strong></a>
  <br>
  <br>
  <a href=""https://github.com/twbs/bootstrap/issues/new?template=bug.md"">Report bug</a>
  ·
  <a href=""https://github.com/twbs/bootstrap/issues/new?template=feature.md&labels=feature"">Request feature</a>
  ·
  <a href=""https://themes.getbootstrap.com/"">Themes</a>
  ·
  <a href=""https://blog.getbootstrap.com/"">Blog</a>
</p>


",2
253,bootstrap, Quick start,"- [Quick start](#quick-start)
- [Status](#status)
- [What's included](#whats-included)
- [Bugs and feature requests](#bugs-and-feature-requests)
- [Documentation](#documentation)
- [Contributing](#contributing)
- [Community](#community)
- [Versioning](#versioning)
- [Creators](#creators)
- [Thanks](#thanks)
- [Copyright and license](#copyright-and-license)


",2
254,bootstrap, Status,"Several quick start options are available:

- [Download the latest release.](https://github.com/twbs/bootstrap/archive/v4.3.1.zip)
- Clone the repo: `git clone https://github.com/twbs/bootstrap.git`
- Install with [npm](https://www.npmjs.com/): `npm install bootstrap`
- Install with [yarn](https://yarnpkg.com/): `yarn add bootstrap@4.3.1`
- Install with [Composer](https://getcomposer.org/): `composer require twbs/bootstrap:4.3.1`
- Install with [NuGet](https://www.nuget.org/): CSS: `Install-Package bootstrap` Sass: `Install-Package bootstrap.sass`

Read the [Getting started page](https://getbootstrap.com/docs/4.3/getting-started/introduction/) for information on the framework contents, templates and examples, and more.


",2
255,bootstrap, What's included,"[![Slack](https://bootstrap-slack.herokuapp.com/badge.svg)](https://bootstrap-slack.herokuapp.com/)
[![Build Status](https://img.shields.io/travis/twbs/bootstrap/master.svg)](https://travis-ci.org/twbs/bootstrap)
[![npm version](https://img.shields.io/npm/v/bootstrap.svg)](https://www.npmjs.com/package/bootstrap)
[![Gem version](https://img.shields.io/gem/v/bootstrap.svg)](https://rubygems.org/gems/bootstrap)
[![Meteor Atmosphere](https://img.shields.io/badge/meteor-twbs%3Abootstrap-blue.svg)](https://atmospherejs.com/twbs/bootstrap)
[![Packagist Prerelease](https://img.shields.io/packagist/vpre/twbs/bootstrap.svg)](https://packagist.org/packages/twbs/bootstrap)
[![NuGet](https://img.shields.io/nuget/vpre/bootstrap.svg)](https://www.nuget.org/packages/bootstrap/absoluteLatest)
[![peerDependencies Status](https://img.shields.io/david/peer/twbs/bootstrap.svg)](https://david-dm.org/twbs/bootstrap?type=peer)
[![devDependency Status](https://img.shields.io/david/dev/twbs/bootstrap.svg)](https://david-dm.org/twbs/bootstrap?type=dev)
[![Coverage Status](https://img.shields.io/coveralls/github/twbs/bootstrap/master.svg)](https://coveralls.io/github/twbs/bootstrap?branch=master)
[![CSS gzip size](https://img.badgesize.io/twbs/bootstrap/master/dist/css/bootstrap.min.css?compression=gzip&label=CSS+gzip+size)](https://github.com/twbs/bootstrap/tree/master/dist/css/bootstrap.min.css)
[![JS gzip size](https://img.badgesize.io/twbs/bootstrap/master/dist/js/bootstrap.min.js?compression=gzip&label=JS+gzip+size)](https://github.com/twbs/bootstrap/tree/master/dist/js/bootstrap.min.js)
[![BrowserStack Status](https://www.browserstack.com/automate/badge.svg?badge_key=SkxZcStBeExEdVJqQ2hWYnlWckpkNmNEY213SFp6WHFETWk2bGFuY3pCbz0tLXhqbHJsVlZhQnRBdEpod3NLSDMzaHc9PQ==--3d0b75245708616eb93113221beece33e680b229)](https://www.browserstack.com/automate/public-build/SkxZcStBeExEdVJqQ2hWYnlWckpkNmNEY213SFp6WHFETWk2bGFuY3pCbz0tLXhqbHJsVlZhQnRBdEpod3NLSDMzaHc9PQ==--3d0b75245708616eb93113221beece33e680b229)
[![Backers on Open Collective](https://img.shields.io/opencollective/backers/bootstrap.svg)](#backers)
[![Sponsors on Open Collective](https://img.shields.io/opencollective/sponsors/bootstrap.svg)](#sponsors)


",2
256,bootstrap, Bugs and feature requests,"Within the download you'll find the following directories and files, logically grouping common assets and providing both compiled and minified variations. You'll see something like this:

```text
bootstrap/
└── dist/
    ├── css/
    │   ├── bootstrap-grid.css
    │   ├── bootstrap-grid.css.map
    │   ├── bootstrap-grid.min.css
    │   ├── bootstrap-grid.min.css.map
    │   ├── bootstrap-reboot.css
    │   ├── bootstrap-reboot.css.map
    │   ├── bootstrap-reboot.min.css
    │   ├── bootstrap-reboot.min.css.map
    │   ├── bootstrap.css
    │   ├── bootstrap.css.map
    │   ├── bootstrap.min.css
    │   └── bootstrap.min.css.map
    └── js/
        ├── bootstrap.bundle.js
        ├── bootstrap.bundle.js.map
        ├── bootstrap.bundle.min.js
        ├── bootstrap.bundle.min.js.map
        ├── bootstrap.esm.js
        ├── bootstrap.esm.js.map
        ├── bootstrap.esm.min.js
        ├── bootstrap.esm.min.js.map
        ├── bootstrap.js
        ├── bootstrap.js.map
        ├── bootstrap.min.js
        └── bootstrap.min.js.map
```

We provide compiled CSS and JS (`bootstrap.*`), as well as compiled and minified CSS and JS (`bootstrap.min.*`). [source maps](https://developers.google.com/web/tools/chrome-devtools/javascript/source-maps) (`bootstrap.*.map`) are available for use with certain browsers' developer tools. Bundled JS files (`bootstrap.bundle.js` and minified `bootstrap.bundle.min.js`) include [Popper](https://popper.js.org/).


",2
257,bootstrap, Documentation,"Have a bug or a feature request? Please first read the [issue guidelines](https://github.com/twbs/bootstrap/blob/master/.github/CONTRIBUTING.md#using-the-issue-tracker) and search for existing and closed issues. If your problem or idea is not addressed yet, [please open a new issue](https://github.com/twbs/bootstrap/issues/new).


",2
258,bootstrap, Running documentation locally,"Bootstrap's documentation, included in this repo in the root directory, is built with [Hugo](https://gohugo.io/) and publicly hosted on GitHub Pages at <https://getbootstrap.com/>. The docs may also be run locally.

Documentation search is powered by [Algolia's DocSearch](https://community.algolia.com/docsearch/). Working on our search? Be sure to set `debug: true` in `site/static/docs/4.3/assets/js/src/search.js` file.

",3
259,bootstrap, Documentation for previous releases,"1. Run `npm install` to install the Node.js dependencies, including Hugo (the site builder).
2. Run `npm run test` (or a specific npm script) to rebuild distributed CSS and JavaScript files, as well as our docs assets.
3. From the root `/bootstrap` directory, run `npm run docs-serve` in the command line.
4. Open `http://localhost:9001/` in your browser, and voilà.

Learn more about using Hugo by reading its [documentation](https://gohugo.io/documentation/).

",3
260,bootstrap, Contributing,"You can find all our previous releases docs on <https://getbootstrap.com/docs/versions/>.

[Previous releases](https://github.com/twbs/bootstrap/releases) and their documentation are also available for download.


",2
261,bootstrap, Community,"Please read through our [contributing guidelines](https://github.com/twbs/bootstrap/blob/master/.github/CONTRIBUTING.md). Included are directions for opening issues, coding standards, and notes on development.

Moreover, if your pull request contains JavaScript patches or features, you must include [relevant unit tests](https://github.com/twbs/bootstrap/tree/master/js/tests). All HTML and CSS should conform to the [Code Guide](https://github.com/mdo/code-guide), maintained by [Mark Otto](https://github.com/mdo).

Editor preferences are available in the [editor config](https://github.com/twbs/bootstrap/blob/master/.editorconfig) for easy use in common text editors. Read more and download plugins at <https://editorconfig.org/>.


",2
262,bootstrap, Versioning,"Get updates on Bootstrap's development and chat with the project maintainers and community members.

- Follow [@getbootstrap on Twitter](https://twitter.com/getbootstrap).
- Read and subscribe to [The Official Bootstrap Blog](https://blog.getbootstrap.com/).
- Join [the official Slack room](https://bootstrap-slack.herokuapp.com/).
- Chat with fellow Bootstrappers in IRC. On the `irc.freenode.net` server, in the `##bootstrap` channel.
- Implementation help may be found at Stack Overflow (tagged [`bootstrap-4`](https://stackoverflow.com/questions/tagged/bootstrap-4)).
- Developers should use the keyword `bootstrap` on packages which modify or add to the functionality of Bootstrap when distributing through [npm](https://www.npmjs.com/browse/keyword/bootstrap) or similar delivery mechanisms for maximum discoverability.


",2
263,bootstrap, Creators,"For transparency into our release cycle and in striving to maintain backward compatibility, Bootstrap is maintained under [the Semantic Versioning guidelines](https://semver.org/). Sometimes we screw up, but we adhere to those rules whenever possible.

See [the Releases section of our GitHub project](https://github.com/twbs/bootstrap/releases) for changelogs for each release version of Bootstrap. Release announcement posts on [the official Bootstrap blog](https://blog.getbootstrap.com/) contain summaries of the most noteworthy changes made in each release.


",2
264,bootstrap, Thanks,"**Mark Otto**

- <https://twitter.com/mdo>
- <https://github.com/mdo>

**Jacob Thornton**

- <https://twitter.com/fat>
- <https://github.com/fat>


",2
265,bootstrap, Backers,"<a href=""https://www.browserstack.com/"">
  <img src=""https://live.browserstack.com/images/opensource/browserstack-logo.svg"" alt=""BrowserStack Logo"" width=""192"" height=""42"">
</a>

Thanks to [BrowserStack](https://www.browserstack.com/) for providing the infrastructure that allows us to test in real browsers!


",2
266,bootstrap, Sponsors,"Thank you to all our backers! 🙏 [[Become a backer](https://opencollective.com/bootstrap#backer)]

[![Bakers](https://opencollective.com/bootstrap/backers.svg?width=890)](https://opencollective.com/bootstrap#backers)


",2
267,bootstrap, Copyright and license,"Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [[Become a sponsor](https://opencollective.com/bootstrap#sponsor)]

[![](https://opencollective.com/bootstrap/sponsor/0/avatar.svg)](https://opencollective.com/bootstrap/sponsor/0/website)
[![](https://opencollective.com/bootstrap/sponsor/1/avatar.svg)](https://opencollective.com/bootstrap/sponsor/1/website)
[![](https://opencollective.com/bootstrap/sponsor/2/avatar.svg)](https://opencollective.com/bootstrap/sponsor/2/website)
[![](https://opencollective.com/bootstrap/sponsor/3/avatar.svg)](https://opencollective.com/bootstrap/sponsor/3/website)
[![](https://opencollective.com/bootstrap/sponsor/4/avatar.svg)](https://opencollective.com/bootstrap/sponsor/4/website)
[![](https://opencollective.com/bootstrap/sponsor/5/avatar.svg)](https://opencollective.com/bootstrap/sponsor/5/website)
[![](https://opencollective.com/bootstrap/sponsor/6/avatar.svg)](https://opencollective.com/bootstrap/sponsor/6/website)
[![](https://opencollective.com/bootstrap/sponsor/7/avatar.svg)](https://opencollective.com/bootstrap/sponsor/7/website)
[![](https://opencollective.com/bootstrap/sponsor/8/avatar.svg)](https://opencollective.com/bootstrap/sponsor/8/website)
[![](https://opencollective.com/bootstrap/sponsor/9/avatar.svg)](https://opencollective.com/bootstrap/sponsor/9/website)


",2
268,sequelize-sequelize, Sequelize,,1
269,sequelize-sequelize, v5 Release,"[![npm version](https://badgen.net/npm/v/sequelize)](https://www.npmjs.com/package/sequelize)
[![Travis Build Status](https://badgen.net/travis/sequelize/sequelize?icon=travis)](https://travis-ci.org/sequelize/sequelize)
[![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/9l1ypgwsp5ij46m3/branch/master?svg=true)](https://ci.appveyor.com/project/sushantdhiman/sequelize/branch/master)
[![npm downloads](https://badgen.net/npm/dm/sequelize)](https://www.npmjs.com/package/sequelize)
[![codecov](https://badgen.net/codecov/c/github/sequelize/sequelize?icon=codecov)](https://codecov.io/gh/sequelize/sequelize)
[![Last commit](https://badgen.net/github/last-commit/sequelize/sequelize)](https://github.com/sequelize/sequelize)
[![Merged PRs](https://badgen.net/github/merged-prs/sequelize/sequelize)](https://github.com/sequelize/sequelize)
[![GitHub stars](https://badgen.net/github/stars/sequelize/sequelize)](https://github.com/sequelize/sequelize)
[![Bountysource](https://www.bountysource.com/badge/team?team_id=955&style=bounties_received)](https://www.bountysource.com/teams/sequelize/issues?utm_source=Sequelize&utm_medium=shield&utm_campaign=bounties_received)
[![Slack Status](http://sequelize-slack.herokuapp.com/badge.svg)](http://sequelize-slack.herokuapp.com/)
[![node](https://badgen.net/npm/node/sequelize)](https://www.npmjs.com/package/sequelize)
[![License](https://badgen.net/github/license/sequelize/sequelize)](https://github.com/sequelize/sequelize/blob/master/LICENSE)
[![semantic-release](https://img.shields.io/badge/%20%20%F0%9F%93%A6%F0%9F%9A%80-semantic--release-e10079.svg)](https://github.com/semantic-release/semantic-release)

Sequelize is a promise-based Node.js ORM for Postgres, MySQL, MariaDB, SQLite and Microsoft SQL Server. It features solid transaction support, relations, eager and lazy loading, read replication and more.

Sequelize follows [SEMVER](http://semver.org). Supports Node v6 and above to use ES6 features.

New to Sequelize? Take a look at the [Tutorials and Guides](https://sequelize.org/master). You might also be interested in the [API Reference](https://sequelize.org/master/identifiers).

",2
270,sequelize-sequelize, Table of Contents,"You can find the upgrade guide and changelog [here](https://sequelize.org/master/manual/upgrade-to-v5.html).

",2
271,sequelize-sequelize, Installation,"- [Installation](#installation)
- [Documentation](#documentation)
- [Responsible disclosure](#responsible-disclosure)
- [Resources](#resources)

",2
272,sequelize-sequelize, Documentation,"```bash
$ npm install --save sequelize #notes: This will install v5

#notes: And one of the following:
$ npm install --save pg pg-hstore #notes: Postgres
$ npm install --save mysql2
$ npm install --save mariadb
$ npm install --save sqlite3
$ npm install --save tedious #notes: Microsoft SQL Server
```

",2
273,sequelize-sequelize, Responsible disclosure,"- [v5 Documentation](https://sequelize.org/master)
- [v4 Documentation](https://sequelize.org/v4)
- [v3 Documentation](https://sequelize.org/v3)
- [Contributing](https://github.com/sequelize/sequelize/blob/master/CONTRIBUTING.md)

",2
274,sequelize-sequelize, Resources,"If you have any security issue to report, contact project maintainers privately. You can find contact information in [CONTACT.md](https://github.com/sequelize/sequelize/blob/master/CONTACT.md).

",2
275,sequelize-sequelize, Tools,"- [Changelog](https://github.com/sequelize/sequelize/releases)
- [Slack](http://sequelize-slack.herokuapp.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/sequelize.js)

",3
276,sequelize-sequelize, Learning,"- [Sequelize & TypeScript](https://sequelize.org/master/manual/typescript.html)
- [Enhanced TypeScript with decorators](https://github.com/RobinBuschmann/sequelize-typescript)
- [Sequelize & GraphQL](https://github.com/mickhansen/graphql-sequelize)
- [Add-ons & Plugins](https://sequelize.org/master/manual/resources.html)
- [Sequelize CLI](https://github.com/sequelize/cli)

",3
277,sequelize-sequelize, Translations,"- [Getting Started](https://sequelize.org/master/manual/getting-started)
- [Express Example](https://github.com/sequelize/express-example)

",3
278,DCPDN, DCPDN,,1
279,DCPDN, Densely Connected Pyramid Dehazing Network (CVPR'2018),,2
280,DCPDN, Prerequisites:,"[He Zhang](https://sites.google.com/site/hezhangsprinter), [Vishal M. Patel](http://www.rci.rutgers.edu/~vmp93/)

[[Paper Link](https://arxiv.org/abs/1803.08396)] (CVPR'18)

We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Network (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end-to-end learning is achieved by directly embedding the atmospheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense network that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi-level pyramid pooling module for estimating the transmission map. This network is optimized using a newly introduced edge-preserving loss function. To further incorporate the mutual structural information between the estimated transmission map and the dehazed result, we propose a joint-discriminator based on generative adversarial network framework to decide whether the
corresponding dehazed image and the estimated transmission map are real or fake. An ablation study is conducted to demonstrate the effectiveness of each module evaluated at both estimated transmission map and dehazed result. Extensive experiments demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods.

	@inproceedings{dehaze_zhang_2018,		
	  title={Densely Connected Pyramid Dehazing Network},
	  author={Zhang, He and Patel, Vishal M},
	  booktitle={CVPR},
	  year={2018}
	} 

<p align=""center"">
<img src=""demo_image/over_input1.png"" width=""250px"" height=""200px""/>         <img src=""demo_image/over_our.png"" width=""250px"" height=""200px""/>



",2
281,DCPDN, Installation:,"1. Linux
2. Python 2 or 3
3. CPU or NVIDIA GPU + CUDA CuDNN (CUDA 8.0)
 
",2
282,DCPDN, Demo using pre-trained model,"1. Install PyTorch and dependencies from http://pytorch.org (Ubuntu+Python2.7)
   (conda install pytorch torchvision -c pytorch)
Install pytorch 0.3.1 https://pytorch.org/previous-versions/
2. Install Torch vision from the source.
   
   	git clone https://github.com/pytorch/vision
	
   	cd vision
	
	python setup.py install

3. Install python package: 
   numpy, scipy, PIL, pdb
   
",2
283,DCPDN, Training (Fine-tuning),"	python demo.py --dataroot ./facades/nat_new4 --valDataroot ./facades/nat_new4 --netG ./demo_model/netG_epoch_8.pth   
Pre-trained dehazing model can be downloaded at (put it in the folder 'demo_model'): https://drive.google.com/drive/folders/1BmNP5ZUWEFeGGEL1NsZSRbYPyjBQ7-nn?usp=sharing

Testing images (nature)  can be downloaded at (put it in the folder 'facades'):
https://drive.google.com/drive/folders/1q5bRQGgS8SFEGqMwrLlku4Ad-0Tn3va7?usp=sharing

Testing images (syn (Test A in the paper))  can be downloaded at (put it in the folder 'facades'):
https://drive.google.com/drive/folders/1hbwYCzoI3R3o2Gj_kfT6GHG7RmYEOA-P?usp=sharing


",2
284,DCPDN, Testing,"	python train.py --dataroot ./facades/train512 --valDataroot ./facades/test512 --exp ./checkpoints_new --netG ./demo_model/netG_epoch_8.pth
More training details (especially how to repreduce the results using stage-wise training strategy) can be found in the paper. 

",2
285,DCPDN, Reproduce,"	python demo.py --dataroot ./your_dataroot --valDataroot ./your_dataroot --netG ./pre_trained/netG_epoch_9.pth   

",2
286,DCPDN, Dataset,"To reproduce the quantitative results shown in the paper, please save both generated and target using python demo.py  into the .png format and then test using offline tool such as the PNSR and SSIM measurement in Python or Matlab.   In addition, please use netG.train() for testing since the batch for training is 1. 


",2
287,DCPDN, How to creat your own testing samples,"Training images (syn)  can be downloaded at (put it in the folder 'facades'):
https://drive.google.com/drive/folders/1Qv7SIZBVAtb9G1d6iVKu_8rVSsXJdv26?usp=sharing

All the syn samples (both training and testing) are strored in Hdf5 file.
You can also generate your sample using 'create_train.py'
(Please download the NYU-depth @ http://horatio.cs.nyu.edu/mit/silberman/nyu_depth_v2/nyu_depth_v2_labeled.mat)

Following are the sample python codes how to read the Hdf5 file:
    
    file_name=self.root+'/'+str(index)+'.h5'
    f=h5py.File(file_name,'r')

    haze_image=f['haze'][:]
    gt_trans_map=f['trans'][:]
    gt_ato_map=f['ato'][:]
    GT=f['gt'][:]

Testing images (nature)  can be downloaded at (put it in the folder 'facades'):
https://drive.google.com/drive/folders/1q5bRQGgS8SFEGqMwrLlku4Ad-0Tn3va7?usp=sharing

Testing images (syn (Test A in the paper))  can be downloaded at (put it in the folder 'facades'):
https://drive.google.com/drive/folders/1hbwYCzoI3R3o2Gj_kfT6GHG7RmYEOA-P?usp=sharing

",2
288,DCPDN, Extension,"Since the proposed methods using hdf5 file to load the training samples, the generate_testsample.py help you to creat the testing or training sample yourself. 


",2
289,DCPDN, Acknowledgments,"The proposed transmission net has demonstrated it effectiveness in multiple appplcaitions such as segmentation. crowd counting, face reconstruction from sparse sample and image synthesis. 

It has also been asked by other researchers and used for participating the [NTIRE-2018](http://www.vision.ee.ethz.ch/en/ntire18/) dehazing challenge and the proposed netowrk has demonstrated effectivenss from the performance in leaderboard. 

",2
290,react, [React](https://reactjs.org/) &middot; [![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/facebook/react/blob/master/LICENSE) [![npm version](https://img.shields.io/npm/v/react.svg?style=flat)](https://www.npmjs.com/package/react) [![CircleCI Status](https://circleci.com/gh/facebook/react.svg?style=shield&circle-token=:circle-token)](https://circleci.com/gh/facebook/react) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://reactjs.org/docs/how-to-contribute.htmlyour-first-pull-request),,2
291,react, Installation,"React is a JavaScript library for building user interfaces.

* **Declarative:** React makes it painless to create interactive UIs. Design simple views for each state in your application, and React will efficiently update and render just the right components when your data changes. Declarative views make your code more predictable, simpler to understand, and easier to debug.
* **Component-Based:** Build encapsulated components that manage their own state, then compose them to make complex UIs. Since component logic is written in JavaScript instead of templates, you can easily pass rich data through your app and keep state out of the DOM.
* **Learn Once, Write Anywhere:** We don't make assumptions about the rest of your technology stack, so you can develop new features in React without rewriting existing code. React can also render on the server using Node and power mobile apps using [React Native](https://facebook.github.io/react-native/).

[Learn how to use React in your own project](https://reactjs.org/docs/getting-started.html).

",2
292,react, Documentation,"React has been designed for gradual adoption from the start, and **you can use as little or as much React as you need**:

* Use [Online Playgrounds](https://reactjs.org/docs/getting-started.html#online-playgrounds) to get a taste of React.
* [Add React to a Website](https://reactjs.org/docs/add-react-to-a-website.html) as a `<script>` tag in one minute.
* [Create a New React App](https://reactjs.org/docs/create-a-new-react-app.html) if you're looking for a powerful JavaScript toolchain.

You can use React as a `<script>` tag from a [CDN](https://reactjs.org/docs/cdn-links.html), or as a `react` package on [npm](https://www.npmjs.com/).

",2
293,react, Examples,"You can find the React documentation [on the website](https://reactjs.org/docs).  

Check out the [Getting Started](https://reactjs.org/docs/getting-started.html) page for a quick overview.

The documentation is divided into several sections:

* [Tutorial](https://reactjs.org/tutorial/tutorial.html)
* [Main Concepts](https://reactjs.org/docs/hello-world.html)
* [Advanced Guides](https://reactjs.org/docs/jsx-in-depth.html)
* [API Reference](https://reactjs.org/docs/react-api.html)
* [Where to Get Support](https://reactjs.org/community/support.html)
* [Contributing Guide](https://reactjs.org/docs/how-to-contribute.html)

You can improve it by sending pull requests to [this repository](https://github.com/reactjs/reactjs.org).

",2
294,react, Contributing,"We have several examples [on the website](https://reactjs.org/). Here is the first one to get you started:

```jsx
function HelloMessage({ name }) {
  return <div>Hello {name}</div>;
}

ReactDOM.render(
  <HelloMessage name=""Taylor"" />,
  document.getElementById('container')
);
```

This example will render ""Hello Taylor"" into a container on the page.

You'll notice that we used an HTML-like syntax; [we call it JSX](https://reactjs.org/docs/introducing-jsx.html). JSX is not required to use React, but it makes code more readable, and writing it feels like writing HTML. If you're using React as a `<script>` tag, read [this section](https://reactjs.org/docs/add-react-to-a-website.html#optional-try-react-with-jsx) on integrating JSX; otherwise, the [recommended JavaScript toolchains](https://reactjs.org/docs/create-a-new-react-app.html) handle it automatically.

",2
295,react, [Code of Conduct](https://code.fb.com/codeofconduct),"The main purpose of this repository is to continue to evolve React core, making it faster and easier to use. Development of React happens in the open on GitHub, and we are grateful to the community for contributing bugfixes and improvements. Read below to learn how you can take part in improving React.

",3
296,react, [Contributing Guide](https://reactjs.org/contributing/how-to-contribute.html),"Facebook has adopted a Code of Conduct that we expect project participants to adhere to. Please read [the full text](https://code.fb.com/codeofconduct) so that you can understand what actions will and will not be tolerated.

",3
297,react, Good First Issues,"Read our [contributing guide](https://reactjs.org/contributing/how-to-contribute.html) to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes to React.

",3
298,react, License,"To help you get your feet wet and get you familiar with our contribution process, we have a list of [good first issues](https://github.com/facebook/react/labels/good%20first%20issue) that contain bugs which have a relatively limited scope. This is a great place to get started.

",3
299,pose-residual-network-pytorch, Pose Residual Network,,1
300,pose-residual-network-pytorch, Getting Started,"This repository contains a PyTorch implementation of the Pose Residual Network (PRN) presented in our ECCV 2018 paper: 

Muhammed Kocabas, Salih Karagoz, Emre Akbas. MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network. In ECCV, 2018. [arxiv](https://arxiv.org/abs/1807.04067)

PRN is described in Section 3.2 of the  paper.

",2
301,pose-residual-network-pytorch, Prerequisites,"We have tested our method on [Coco Dataset](http://cocodataset.org)

",3
302,pose-residual-network-pytorch, Installing,"```
python
pytorch
numpy
tqdm
pycocotools
progress
scikit-image
```

",3
303,pose-residual-network-pytorch, Training,"1. Clone this repository 
`git clone https://github.com/salihkaragoz/pose-residual-network-pytorch.git`

2. Install [Pytorch](https://pytorch.org/)

3. `pip install -r src/requirements.txt`

4. To download COCO dataset train2017 and val2017 annotations run: `bash data/coco.sh`. (data size: ~240Mb)

",2
304,pose-residual-network-pytorch, Testing,"`python train.py`

For more options look at opt.py

",2
305,pose-residual-network-pytorch, Results,"1. Download pre-train [model](https://drive.google.com/file/d/1OhdMllLGnpRAk6Wexw8LzXF_EHiolVj1/view?usp=sharing)

2. `python test.py --test_cp=PathToPreTrainModel/PRN.pth.tar`

",2
306,pose-residual-network-pytorch, License,"Results on COCO val2017 Ground Truth data.

```
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.892
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.978
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.921
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.883
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.912
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.917
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.982
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.937
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.902
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.944

```

",2
307,pose-residual-network-pytorch, Citation,,2
308,gempy," <p align=""left""><img src=""docs/logos/gempy1.png"" width=""300""></p>",,1
309,gempy, What is it,"> Open-source, implicit 3D structural geological modeling in Python for uncertainty analysis.


[![PyPI](https://img.shields.io/badge/python-3-blue.svg)](https://www.python.org/downloads/)
[![PyPI](https://img.shields.io/badge/pypi-1.0-blue.svg)](https://pypi.org/project/gempy/)
[![license: LGPL v3](https://img.shields.io/badge/license-LGPL%20v3-blue.svg)]()
[![Documentation Status](https://readthedocs.org/projects/gempy/badge/?version=latest)](http://gempy.readthedocs.io/?badge=latest)
[![Travis Build](https://travis-ci.org/cgre-aachen/gempy.svg?branch=master)]()
[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/cgre-aachen/gempy/master)
[![DOI](https://zenodo.org/badge/96211155.svg)](https://zenodo.org/badge/latestdoi/96211155)
[![DOCKER](https://img.shields.io/docker/cloud/automated/leguark/gempy.svg)](https://cloud.docker.com/repository/docker/leguark/gempy)

<p align=""center""><img src=""docs/source/images/model_examples.png"" width=""800""></p>

",2
310,gempy, Table of Contents,"*GemPy* is a Python-based, open-source library for **implicitly generating 3D structural geological models**. It is capable of
constructing complex 3D geological models of folded structures, fault networks and unconformities. It was designed from the 
ground up to support easy embedding in probabilistic frameworks for the uncertainty analysis of subsurface structures.

Check out the documentation either in [gempy.org](https://www.gempy.org/) (better option), or [read the docs](http://gempy.readthedocs.io/).

",2
311,gempy, News,"* [Features](#feat)
    * [Sandbox](#sandbox)
    * [Remote Geomod](#remotegeo)
* [Getting Started](#getstart)
    * [Dependencies](#depend)
    * [Installation](#installation)
* [Documentation](#doc)
* [References](#ref)

<a name=""feat""></a>
",2
312,gempy, GemPy v2.0 beta release,,3
313,gempy, What is new,"It has been a long journey since the release of GemPy v1.0. What started as a small library to carry out research
on uncertainty analysis for structural geology has grown to be used in multiple projects around the world. Carried
by the community enthusiasm, we commenced a way-longer-than-planned rewritten of the code in order to
not only be able to fulfill the needs of many of you but also to set the foundations of a package driven by the
community. For this end, all the logic has been splat into multiple modules, classes and containers limiting
duplicities and exposing a large mutation api at different levels of abstraction. Hope the work has been worth it.

So long,

Miguel

",4
314,gempy, Features,"- Full redesign of the back-end: much more modular, explicit and avoiding object duplicities to insane levels
- Topography
- Onlap, Erosion relations
- Choose your favourite type of fault: infinite faults, finite faults, faults offsetting faults, faults ending on series
- Masked marching cubes: this fix the ugly surfaces following the voxels faces
- All series are fully stored after interpolation
- Save your model
- Compile once, modify as much as you want
- Full integration with qgrid
- Real time computations via vtk or python-qgrid
- Adaptive regular grids for geophysics
- Refactored some legacy names:
    + formations renamed to surfaces
    + interfaces renamed to surfaces_points
- Minor changes:
    + New colormap and easy way to change the surfaces colors (even integration with widgets!)
    + The order of the formations will be given by the interpolation itself if the input was wrong
    + The split between reference and rest surface_points happens in theano. This makes much easier the modification
    of reference points


",2
315,gempy, Sandbox,"The core algorithm of *GemPy* is based on a universal cokriging interpolation method devised by
Lajaunie et al. (1997) and extended by Calcagno et al. (2008). Its implicit nature allows the user to automatically
generate complex 3D structural geological models through the interpolation of input data:

- *Surface contact points*: 3D coordinates of points marking the boundaries between different features (e.g. layer interfaces, fault planes, unconformities).
- *Orientation measurements*: Orientation of the poles perpendicular to the dipping of surfaces at any point in the 3D space.

*GemPy* also allows for the definition of topological elements such as combining multiple stratigraphic sequences and 
complex fault networks to be considered in the modeling process.

<p align=""center""><img src=""docs/source/images/modeling_principle.png"" width=""600""></p>

*GemPy* itself offers direct visualization of 2D model sections via matplotlib
and in full, interactive 3D using the Visualization Toolkit (VTK). The VTK support also allow to the real time maniulation
of the 3-D model, allowing for the exact modification of data. Models can also easily be exportes in VTK file format
for further visualization and processing in other software such as ParaView.

<p align=""center""><img src=""docs/source/images/vtkFault.png"" width=""600""></p>


*GemPy* was designed from the beginning to support stochastic geological modeling for uncertainty analysis (e.g. Monte Carlo simulations, Bayesian inference). This was achieved by writing *GemPy*'s core architecture
using the numerical computation library [Theano](http://deeplearning.net/software/theano/) to couple it with the probabilistic programming framework [PyMC3](https://pymc-devs.github.io/pymc3/notebooks/getting_started.html).
This enables the use of advanced sampling methods (e.g. Hamiltonian Monte Carlo) and is of particular relevance when considering
uncertainties in the model input data and making use of additional secondary information in a Bayesian inference framework.

We can, for example, include uncertainties with respect to the z-position of layer boundaries
in the model space. Simple Monte Carlo simulation via PyMC will then result in different model realizations:

<p align=""center""><img src=""docs/source/images/gempy_zunc.png"" height=""300""> <img src=""docs/source/images/model_wobble.gif"" height=""300""></p>

Theano allows the automated computation of gradients opening the door to the use of advanced gradient-based sampling methods
coupling *GeMpy* and [PyMC3](https://pymc-devs.github.io/pymc3/notebooks/getting_started.html) for advanced stochastic modeling.
Also, the use of Theano allows making use of GPUs through cuda (see the Theano documentation for more information.

Making use of vtk interactivity and Qgrid (https://github.com/quantopian/qgrid) *GemPy* provides a functional interface to interact with input data and models.

<p align=""center""><a href=""https://youtu.be/aA4MaHpLWVE?t=67""><img src=""https://img.youtube.com/vi/aA4MaHpLWVE/0.jpg"" width=""600""></a></p>




For a more detailed elaboration of the theory behind *GemPy*, take a look at the upcoming scientific publication
*""GemPy 1.0: open-source stochastic geological modeling and inversion""* by de la Varga et al. (2018).

Besides the main functionality GemPy is powering currently some further projects:

<a name=""sandbox""></a>
",3
316,gempy, Remote Geomod: From GoogleEarth to 3-D Geology,"New developments in the field of augmented reality, i.e. the superimposition of real and digital objects, offer interesting and diverse possibilities that have hardly been exploited to date.
The aim of the project is therefore the development and realization of an augmented reality sandbox for interaction with geoscientific data and models.
In this project, methods are to be developed to project geoscientific data (such as the outcrop of a geological layer surface or geophysical measurement data) onto real surfaces.

The AR Sandbox is based on a container filled with sand, the surface of which can be shaped as required. The topography of the sand surface is continuously scanned by a 3D sensor and a camera.
In the computer the scanned surface is now blended with a digital geological 3D model (or other data) in real time and an image is calculated, which is projected onto the sand surface by means
of a beamer. This results in an interactive model with which the user can interact in an intuitive way and which visualizes and comprehend complex three-dimensional facts in an accessible way.

In addition to applications in teaching and research, this development offers great potential as an interactive exhibit with high outreach for the geosciences thanks to its intuitive operation.
The finished sandbox can be used in numerous lectures and public events , but is mainly used as an interface to GemPy software and for rapid prototyping of implicit geological models.

<p align=""center""><a href=""https://youtu.be/oE3Atw-YvSA""><img src=""https://img.youtube.com/vi/oE3Atw-YvSA/0.jpg"" width=""600""></p>

<a name=""remotegeo""></a>
",3
317,gempy, Getting Started,"We support this effort here with a full 3-D geomodeling exercise
on the basis of the excellent possibilities offered by open global data sets, implemented in
GoogleEarth, and dedicated geoscientific open-source software and motivate the use of 3-D
geomodeling to address specific geological questions. Initial steps include the selection of
relevant geological surfaces in GoogleEarth and the analysis of determined orientation values
for a selected region This information is subsequently used
to construct a full 3-D geological model with a state-of-the-art interpolation algorithm. Fi-
nally, the generated model is intersected with a digital elevation model to obtain a geological
map, which can then be reimported into GoogleEarth.

<p align=""center""><img src=""docs/source/images/ge.png"" width=""900""></p>

<a name=""getstart""></a>
",2
318,gempy, Dependencies,"<a name=""depend""></a>
",3
319,gempy, Conflictive packages.,"*GemPy* requires Python 3 and makes use of numerous open-source libraries:

* pandas>=0.21.0
* cython
* Theano
* matplotlib
* numpy
* pytest
* nbsphinx
* seaborn
* networkx
* ipywidgets

Optional:

* git+git://github.com/Leguark/scikit-image@master
* steno3d
* vtk
* gdal
* qgrid
* pymc
* pymc3

* `vtk>=7` for interactive 3-D visualization 
* `pymc` or `pymc3`
* `steno3d` 

Overall we recommend the use of a dedicated Python distribution, such as 
[Anaconda](https://www.continuum.io/what-is-anaconda), for hassle-free package installation. 
We are currently working on providing GemPy also via Anaconda Cloud, for easier installation of
its dependencies.

",4
320,gempy, Installation,"Installing Theano (specially in windows) and vtk sometimes is problematic. Here we give a few advices that
usually works for us:
* Theano: install the following packages before installing theano: `conda install mingw libpython m2w64-toolchain`. Then install Theano via `conda install theano`. 
If the installation fails at some point try to re-install anaconda for a single user (no administrator priveleges) and with the Path Environment set.
To use Theano with `numpy version 1.16.0` or following, it has to be updated to `Theano 1.0.4` using `pip install theano --upgrade`.
Note that this is not yet available in the conda package manager.

* scikit_image (Spring 2019): To use scikit_image with `numpy version 1.16.0` or following, it has to be updated to `scikit_image 1.14.2` using `pip install scikit_image --upgrade`.
Note that this is not yet available in the conda package manager.

* vtk: Right now (Fall 2018), does not have compatibility with python 3.7. The simplest solution to install it is to
use `conda install python=3.6` to downgrade the python version and then using `pip install vtk`.

<a name=""installation""></a>
",3
321,gempy, PyPi ,"We provide the latest release version of *GemPy* via the **Conda** and **PyPi** package services. We highly
recommend using either PyPi as it will take care of automatically installing all dependencies.

",4
322,gempy, New in GemPy 2.0: Docker image,"`$ pip install gempy`


",4
323,gempy, Pull Docker image from DockerHub,"Finally e also provide precompiled Docker images hosted on Docker Hub with all necessary dependencies to get 
GemPy up and running (**except vtk**).

ocker is an operating-system-level-visualization software,
meaning that we can package a tiny operating system with pre-installed
software into a Docker image. This Docker image can then be shared
with and run by others, enabling them to use intricate dependencies
with just a few commands. For this to work the user needs to have a
working [Docker](https://www.docker.com/) installation.

",5
324,gempy, Manual,"The easiest way to get remote-geomod running is by running the pre-compiled Docker image (containing everything you
need) directly from the cloud service Docker Hub to get a locally running Docker container. Make sure to set your 
Docker daemon to Linux containers in Docker's context menu.

    $ docker run -it -p 8899:8899 leguark/gempy
    
This will automatically pull the Docker image from Docker Hub and run it, opening a command line shell inside of the
running Docker container. There you have access to the file system inside of the container. Note that this pre-compiled
Docker image already contains the GemPy repository. 

Once you are in the docker console if you want to open the tutorials you will need to run:

    $ jupyter notebook --ip 0.0.0.0 --port 8899 --no-browser --allow-root
     
Notice that we are running the notebook on the port  8899 to try to avoid conflicts with jupyter servers running in
your system. If everything worked fine, the address to the jupyter notebook will be display on the console. It
has to look something like this (Just be aware of the  brackets):

    To access the notebook, open this file in a browser:
            file:///root/.local/share/jupyter/runtime/nbserver-286-open.html
    Or copy and paste one of these URLs:
        http://(ce2cdcc55bb0 or 127.0.0.1):8899/?token=97d52c1dc321c42083d8c1b4d


",4
325,gempy, Windows installation guide (Jun 2019),"Otherwise you can clone the current repository by downloading is manually or by using Git by calling

`$ git clone https://github.com/cgre-aachen/gempy.git`

and then manually install it using the provided Python install file by calling

`$ python gempy/setup.py install`

in the cloned or downloaded repository folder. Make sure you have installed all necessary dependencies listed above before using *GemPy*.

",5
326,gempy, Documentation,"1) Install CUDA if you do not have it already.

2) Install Anaconda3 2019.03 with Python 3.7 (this is the last release).

3) Install Theano and associated packages from the Anaconda prompt as administrator, and finally install GemPy 2.0:

- conda update --all
- conda install libpython
- conda install m2w64-toolchain
- conda install git
- conda install pygpu
- pip install theano==1.0.4
- pip install gempy==2.0b0.dev2

Note that:

a) some other packages required by Theano are already included in Anaconda: numpy, scipy, mkl-service, nose, and sphinx.

b) pydot-ng (suggested on Theano web site) yields a lot of errors. I dropped this. It is needed to handle large picture for gif/images and probably it is not needed by GemPy.

c) Trying to install all the packages in one go but it does not work, as well as doing the same in Anaconda Navigator, or installing an older Anaconda release with Python 3.5 (Anaconda3 4.2.0) as indicated in some tutorial on Theano.


<a name=""doc""></a>
",2
327,gempy, References,"Extensive documentation for *GemPy* is hosted at [gempy.readthedocs.io](http://gempy.readthedocs.io/),
explaining its capabilities, [the theory behind it](http://gempy.readthedocs.io/Kriging.html) and 
providing detailed [tutorials](http://gempy.readthedocs.io/tutorial.html) on how to use it.

<a name=""ref""></a>
",2
328,iter-reason, Iterative Visual Reasoning Beyond Convolutions,,1
329,iter-reason, Disclaimer,"By Xinlei Chen, Li-Jia Li, Li Fei-Fei and Abhinav Gupta. 

",3
330,iter-reason, Prerequisites,"  - This is the authors' implementation of the system described in the paper, not an official Google product.
  - Right now:
    - The available reasoning module is based on convolutions and spatial memory.
    - For simplicity, the released code uses the tensorflow default `crop_and_resize` operation, rather than the customized one reported in the paper (I find the default one is actually better by ~1%).

",3
331,iter-reason, Setup and Running,"1. Tensorflow, tested with version 1.6 with Ubuntu 16.04, installed with:
  ```Shell
  pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.6.0-cp27-none-linux_x86_64.whl
  ```

2. Other packages needed can be installed with `pip`:
  ```Shell
  pip install Cython easydict matplotlib opencv-python Pillow pyyaml scipy
  ```

3. For running COCO, the API can be installed globally:
  ```Shell
  #notes: any path is okay
  mkdir ~/install && cd ~/install
  git clone https://github.com/cocodataset/cocoapi.git cocoapi
  cd cocoapi/PythonAPI
  python setup.py install --user
  ```

",3
332,iter-reason, References,"1. Clone the repository.
  ```Shell
  git clone https://github.com/endernewton/iter-reason.git
  cd iter-reason
  ```

2. Set up data, here we use [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/) as an example.
  ```Shell
  mkdir -p data/ADE
  cd data/ADE
  wget -v http://groups.csail.mit.edu/vision/datasets/ADE20K/ADE20K_2016_07_26.zip
  tar -xzvf ADE20K_2016_07_26.zip
  mv ADE20K_2016_07_26/* ./
  rmdir ADE20K_2016_07_26
  #notes: then get the train/val/test split
  wget -v http://xinleic.xyz/data/ADE_split.tar.gz
  tar -xzvf ADE_split.tar.gz
  rm -vf ADE_split.tar.gz
  cd ../..
  ```

3. Set up pre-trained ImageNet models. This is similarly done in [tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn). Here by default we use ResNet-50 as the backbone:
  ```Shell
   mkdir -p data/imagenet_weights
   cd data/imagenet_weights
   wget -v http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz
   tar -xzvf resnet_v1_50_2016_08_28.tar.gz
   mv resnet_v1_50.ckpt res50.ckpt
   cd ../..
   ```

4. Compile the library (for computing bounding box overlaps).
  ```Shell
  cd lib
  make
  cd ..
  ```

5. Now you are ready to run! For example, to train and test the baseline:
  ```Shell
  ./experiments/scripts/train.sh [GPU_ID] [DATASET] [NET] [STEPS] [ITER] 
  #notes: GPU_ID is the GPU you want to test on
  #notes: DATASET in {ade, coco, vg} is the dataset to train/test on, defined in the script
  #notes: NET in {res50, res101} is the backbone networks to choose from
  #notes: STEPS (x10K) is the number of iterations before it reduces learning rate, can support multiple steps separated by character 'a'
  #notes: ITER (x10K) is the total number of iterations to run
  #notes: Examples:
  #notes: train on ADE20K for 320K iterations, reducing learning rate at 280K.
  ./experiments/scripts/train.sh 0 ade 28 32
  #notes: train on COCO for 720K iterations, reducing at 320K and 560K.
  ./experiments/scripts/train.sh 1 coco 32a56 72
  ```

6. To train and test the reasoning modules (based on ResNet-50):
  ```Shell
  ./experiments/scripts/train_memory.sh [GPU_ID] [DATASET] [MEM] [STEPS] [ITER] 
  #notes: MEM in {local} is the type of reasoning modules to use 
  #notes: Examples:
  #notes: train on ADE20K on the local spatial memory.
  ./experiments/scripts/train_memory.sh 0 ade local 28 32
  ```

7. Once the training is done, you can test the models separately with `test.sh` and `test_memory.sh`, we also provided a separate set of scripts to test on larger image inputs.

8. You can use tensorboard to visualize and track the progress, for example:
  ```Shell
  tensorboard --logdir=tensorboard/res50/ade_train_5/ --port=7002 &
  ```

",3
333,DeepGuidedFilter, Fast End-to-End Trainable Guided Filter,,1
334,DeepGuidedFilter, Overview,"[[Project]](http://wuhuikai.me/DeepGuidedFilterProject)    [[Paper]](http://wuhuikai.me/DeepGuidedFilterProject/deep_guided_filter.pdf)    [[arXiv]](https://arxiv.org/abs/1803.05619)    [[Demo]](http://wuhuikai.me/DeepGuidedFilterProject#demo)    [[Home]](http://wuhuikai.me)
  
Official implementation of **Fast End-to-End Trainable Guided Filter**.     
**Faster**, **Better** and **Lighter**  for image processing and dense prediction. 

",2
335,DeepGuidedFilter, Try it on an image!,"![](images/results.jpg)

**DeepGuidedFilter** is the author's implementation of the deep learning building block for joint upsampling described in:  

**Fast End-to-End Trainable Guided Filter**     
Huikai Wu, Shuai Zheng, Junge Zhang, Kaiqi Huang    
CVPR 2018

Given a reference image pair in high-resolution and low-resolution, our algorithm generates high-resolution target from the low-resolution input. Through joint training with CNNs, our algorithm achieves the state-of-the-art performance while runs **10-100** times faster. 

Contact: Hui-Kai Wu (huikaiwu@icloud.com)

",2
336,DeepGuidedFilter, Prepare Environment,,3
337,DeepGuidedFilter, Ready to **GO** !,"1. Download source code from GitHub.
    ```sh
    git clone https://github.com/wuhuikai/DeepGuidedFilter
    
    cd DeepGuidedFilter && git checkout release
    ```
2. Install dependencies (PyTorch version).
    ```sh
    conda install opencv
    conda install pytorch=0.2.0 cuda80 -c soumith
    
    pip install -r requirements.txt 
    ```
3. (**Optional**) Install dependencies for MonoDepth (Tensorflow version).
    ```sh
    cd ComputerVision/MonoDepth
    
    pip install -r requirements.txt
    ```
",3
338,DeepGuidedFilter, Image Processing,,4
339,DeepGuidedFilter, Semantic Segmentation with Deeplab-Resnet,"```sh
cd ImageProcessing/DeepGuidedFilteringNetwork

python predict.py  --task auto_ps \
                   --img_path ../../images/auto_ps.jpg \
                   --save_folder . \
                   --model deep_guided_filter_advanced \
                   --low_size 64 \
                   --gpu 0
```
See [Here](ImageProcessing/DeepGuidedFilteringNetwork/#predict) or `python predict.py -h` for more details.
",4
340,DeepGuidedFilter, Saliency Detection with DSS,"1. Enter the directory.
    ```sh
    cd ComputerVision/Deeplab-Resnet
    ```
2. Download the pretrained model [[Google Drive](https://drive.google.com/open?id=1YXZoZIZNR1ACewiUBp4UDvo_P65cCooK)|[BaiduYunPan](https://pan.baidu.com/s/1dEnpcGfchlZA_fVGdve0ig)].
3. Run it now !
    ```sh
    python predict_dgf.py --img_path ../../images/segmentation.jpg --snapshots [MODEL_PATH]
    ```
Note:
1. Result is in `../../images`.
2. Run `python predict_dgf.py -h` for more details.
",4
341,DeepGuidedFilter, Monocular Depth Estimation (TensorFlow version),"1. Enter the directory.
    ```sh
    cd ComputerVision/Saliency_DSS
    ```
2. Download the pretrained model [[Google Drive](https://drive.google.com/open?id=1ZxbAAJw9BxCKj2e2QsBmCnjWLFlCGLf1)|[BaiduYunPan](https://pan.baidu.com/s/1pgOMh3V50lRa6slbIW_SKQ)].
3. Try it now !
    ```sh
    python predict.py --im_path ../../images/saliency.jpg \
                      --netG [MODEL_PATH] \
                      --thres 161 \
                      --dgf --nn_dgf \
                      --post_sigmoid --cuda
    ```
Note:
1. Result is in `../../images`.
2. See [Here](ComputerVision/Saliency_DSS/#try_on_an_image) or `python predict.py -h` for more details.
",4
342,DeepGuidedFilter, Guided Filtering Layer,"1. Enter the directory.
    ```sh
    cd ComputerVision/MonoDepth
    ```
2. **Download** and **Unzip** Pretrained Model [[Google Drive](https://drive.google.com/file/d/1dKDYRtZPahoFJZ5ZJNilgHEvT6gG4SC6/view?usp=sharing)|[BaiduYunPan](https://pan.baidu.com/s/1-GkMaRAVym8UEmQ6ia5cHw)]
2. Run on an Image !
    ```sh
    python monodepth_simple.py --image_path ../../images/depth.jpg --checkpoint_path [MODEL_PATH] --guided_filter
    ```
Note:
1. Result is in `../../images`.
2. See [Here](ComputerVision/MonoDepth/#try_it_on_an_image) or `python monodepth_simple.py -h` for more details.

",2
343,DeepGuidedFilter, Install Released Version,,3
344,DeepGuidedFilter, Usage,"* PyTorch Version
    ```sh
    pip install guided-filter-pytorch
    ```
* Tensorflow Version
    ```sh
    pip install guided-filter-tf
    ```
",3
345,DeepGuidedFilter, Training from scratch,"* PyTorch Version
    ```python
    from guided_filter_pytorch.guided_filter import FastGuidedFilter
    
    hr_y = FastGuidedFilter(r, eps)(lr_x, lr_y, hr_x)
    ```
    ```python
    from guided_filter_pytorch.guided_filter import GuidedFilter
    
    hr_y = GuidedFilter(r, eps)(hr_x, init_hr_y)
    ``` 
* Tensorflow Version
    ```python
    from guided_filter_tf.guided_filter import fast_guided_filter
    
    hr_y = fast_guided_filter(lr_x, lr_y, hr_x, r, eps, nhwc)
    ```
    ```python
    from guided_filter_tf.guided_filter import guided_filter
    
    hr_y = guided_filter(hr_x, init_hr_y, r, eps, nhwc)
    ```
",2
346,DeepGuidedFilter, Prepare Training Environment,,3
347,DeepGuidedFilter, Start to Train,"```sh
git checkout master

conda install opencv
conda install pytorch=0.2.0 cuda80 -c soumith
    
pip install -r requirements.txt

#notes: (Optional) For MonoDepth (TF Version).
pip install -r ComputerVision/MonoDepth/requirements.txt 
```
",3
348,DeepGuidedFilter, Citation,"* [Image Processing](ImageProcessing/DeepGuidedFilteringNetwork)
* [Semantic Segmentation with Deeplab-Resnet](ComputerVision/Deeplab-Resnet)
* [Saliency Detection with DSS](ComputerVision/Saliency_DSS)
* [Monocular Depth Estimation (TensorFlow version)](ComputerVision/MonoDepth)

",2
349,gitfolio, Gitfolio  [![Tweet](https://img.shields.io/twitter/url/https/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=personal%20website%20and%20a%20blog%20for%20every%20github%20user%20&url=https://github.com/imfunniee/gitfolio) ![GitHub release](https://img.shields.io/github/release/imfunniee/gitfolio.svg?style=popout-square) ![npm](https://img.shields.io/npm/dm/gitfolio.svg?style=popout-square) ![GitHub top language](https://img.shields.io/github/languages/top/imfunniee/gitfolio.svg?style=popout-square) ![GitHub last commit](https://img.shields.io/github/last-commit/imfunniee/gitfolio.svg?style=popout-square) ![GitHub](https://img.shields.io/github/license/imfunniee/gitfolio.svg?style=popout-square),,1
350,gitfolio, personal website + blog  for every github user,"  
",3
351,gitfolio, Getting Started,"Gitfolio will help you get started with a portfolio website where you could showcase your work + a blog that will help you spread your ideas into real world.

Check out this [live demo](https://imfunniee.github.io/gitfolio/) to see gitfolio in action.


",1
352,gitfolio, Let's Install,,3
353,gitfolio, Let's Build,"Install gitfolio

```sh
npm i gitfolio -g
```

",3
354,gitfolio, Let's Customize,"```sh
gitfolio build <username>
```
`<username>` is your username on github. This will build your website using your GitHub username and put it in the `/dist` folder.

To run your website use `run` command

```sh
gitfolio run
```

Open your browser at http://localhost:3000

🎉 Congrats, you just made yourself a personal website!


",3
355,gitfolio, Forks,,4
356,gitfolio, Sorting Repos,"To include forks on your personal website just provide `-f` or `--fork` argument while building

```sh
$ gitfolio build <username> -f
```

",4
357,gitfolio, Ordering Repos,"To sort repos provide `--sort [sortBy]` argument while building. Where `[sortBy]` can be `star`, `created`, `updated`, `pushed`,`full_name`. Default: `created`

```sh
$ gitfolio build <username> --sort star
```

",4
358,gitfolio, Customize Themes,"To order the sorted repos provide `--order [orderBy]` argument while building. Where `[orderBy]` can be `asc` or `desc`. Default: `asc`

```sh
$ gitfolio build <username> --sort star --order desc
```

",4
359,gitfolio, Customize background image,"Themes are specified using the `--theme [theme-name]` flag when running the `build` command. The available themes are

* `light`
* `dark`
> TODO: Add more themes

For example, the following command will build the website with the dark theme
```sh
$ gitfolio build <username> --theme dark
```

",4
360,gitfolio, Let's Publish,"To customize the background image just provide `--background [url]` argument while building

```sh
$ gitfolio build <username> --background https://images.unsplash.com/photo-1557277770-baf0ca74f908?w=1634
```

You could also add in your custom CSS inside `index.css` to give it a more personal feel.


",3
361,gitfolio, Updating,"Head over to GitHub and create a new repository named `username.github.io`, where username is your username. Push the files inside`/dist` folder to repo you just created.

Go To `username.github.io` your site should be up!!


",3
362,gitfolio, Add a Blog,"To update your info, simply run

```sh
$ gitfolio update
```
This will update your info and your repository info.

To Update background or theme you need to run `build` command again.


",3
363,gitfolio, License,"To add your first blog run this command.

```sh
$ gitfolio blog my-first-blog
```
> (use ""-"" instead of spaces)

This will create a `my-first-blog` folder inside `blog`. Inside `my-first-blog` you will find an `index.html` file which contains all the necessary elements for writing a blog. Customize the content of the file to write your first blog.

This also adds content to `blog.json` file. This file helps in showcasing your blogs on your personal website as [cards](https://imfunniee.github.io/gitfolio/#blog_section). You could customize the JSON object that corresponds your current blog.

Blog Demo? [here](https://imfunniee.github.io/gitfolio/blog/my-first-post/)

Default JSON Format
```
{
  ""url_title"": ""my-first-blog"", // the title you provide while creating a new blog, this appears in url
  ""title"": ""Lorem ipsum dolor sit amet"", // main title of blog
  ""sub_title"": ""Lorem ipsum dolor sit amet, consectetur adipiscing elit."", // sub-title of blog
  ""top_image"": ""https://images.unsplash.com/photo-1553748024-d1b27fb3f960?w=1450"", // main image of blog
  ""visible"": true // don't worry about this
}
```

More Arguments for Blog

```
--subtitle [subtitle] : gives blog a subtitle (Default : 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.')
--pagetitle [pagetitle] : gives blog page a title
--folder [folder] : give folder a title
```

> (use ""-"" instead of spaces)


",2
364,gitbucket-gitbucket, 4.31.2 - 7 Apr 2019,"=========

GitBucket is a Git web platform powered by Scala offering:

- Easy installation
- Intuitive UI
- High extensibility by plugins
- API compatibility with GitHub

You can try an [online demo](https://gitbucket.herokuapp.com/) *(ID: root / Pass: root)* of GitBucket, and also get the latest information at [GitBucket News](https://gitbucket.github.io/gitbucket-news/).

Features
--------
The current version of GitBucket provides many features such as:

- Public / Private Git repositories (with http/https and ssh access)
- GitLFS support
- Repository viewer including an online file editor
- Issues, Pull Requests and Wiki for repositories
- Activity timeline and email notifications
- Account and group management with LDAP integration
- a Plug-in system

If you want to try the development version of GitBucket, see the [Developer's Guide](https://github.com/gitbucket/gitbucket/blob/master/doc/readme.md).

Installation
--------
GitBucket requires **Java8**. You have to install it, if it is not already installed.

1. Download the latest **gitbucket.war** from [the releases page](https://github.com/gitbucket/gitbucket/releases) and run it by `java -jar gitbucket.war`.
2. Go to `http://[hostname]:8080/` and log in with ID: **root** / Pass: **root**.

You can specify following options:

- `--port=[NUMBER]`
- `--prefix=[CONTEXTPATH]`
- `--host=[HOSTNAME]`
- `--gitbucket.home=[DATA_DIR]`
- `--temp_dir=[TEMP_DIR]`
- `--max_file_size=[MAX_FILE_SIZE]`

`TEMP_DIR` is used as the [temporary directory for the jetty application context](https://www.eclipse.org/jetty/documentation/9.3.x/ref-temporary-directories.html). This is the directory into which the `gitbucket.war` file is unpacked, the source files are compiled, etc. If given this parameter **must** match the path of an existing directory or the application will quit reporting an error; if not given the path used will be a `tmp` directory inside the gitbucket home.

`MAX_FILE_SIZE` is the max file size for upload files.

You can also deploy `gitbucket.war` to a servlet container which supports Servlet 3.0 (like Jetty, Tomcat, JBoss, etc)

For more information about installation on Mac or Windows Server (with IIS), or configuration of Apache or Nginx and also integration with other tools or services such as Jenkins or Slack, see [Wiki](https://github.com/gitbucket/gitbucket/wiki).

To upgrade GitBucket, replace `gitbucket.war` with the new version, after stopping GitBucket. All GitBucket data is stored in `HOME/.gitbucket` by default. So if you want to back up GitBucket's data, copy this directory to the backup location.

Plugins
--------
GitBucket has a plug-in system that allows extra functionality. Officially the following plug-ins are provided:

- [gitbucket-gist-plugin](https://github.com/gitbucket/gitbucket-gist-plugin)
- [gitbucket-emoji-plugin](https://github.com/gitbucket/gitbucket-emoji-plugin)
- [gitbucket-pages-plugin](https://github.com/gitbucket/gitbucket-pages-plugin)
- [gitbucket-notifications-plugin](https://github.com/gitbucket/gitbucket-notifications-plugin)

You can find more plugins made by the community at [GitBucket community plugins](https://gitbucket-plugins.github.io/).

Support
--------

- If you have any questions about GitBucket, see [Wiki](https://github.com/gitbucket/gitbucket/wiki) and check issues whether there is a same question or request in the past.
- If you can't find same question and report, send it to [gitter room](https://gitter.im/gitbucket/gitbucket) before raising an issue.
- The highest priority of GitBucket is the ease of installation and API compatibility with GitHub, so your feature request might be rejected if they go against those principles.

What's New in 4.31.x
-------------
",3
365,gitbucket-gitbucket, 4.31.1 - 17 Mar 2019,"- Bug and security fix

",3
366,gitbucket-gitbucket, 4.31.0 - 17 Mar 2019,"- Bug fix

",3
367,neural_renderer, Neural 3D Mesh Renderer (CVPR 2018),,1
368,neural_renderer, For PyTorch users,"This is code for the paper [Neural 3D Mesh Renderer](http://hiroharu-kato.com/projects_en/neural_renderer.html) by Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada.

![](http://hiroharu-kato.com/assets/img/neural_renderer/thumbnail_en.png)

For more details, please visit [project page](http://hiroharu-kato.com/projects_en/neural_renderer.html).

This repository only contains the core component and simple examples. Related repositories are:

* Neural Renderer (this repository)
    * [Single-image 3D mesh reconstruction](https://github.com/hiroharu-kato/mesh_reconstruction)
    * [2D-to-3D style transfer](https://github.com/hiroharu-kato/style_transfer_3d)
    * [3D DeepDream](https://github.com/hiroharu-kato/deep_dream_3d)

",2
369,neural_renderer, Installation,"This code is written in Chainer. For PyTorch users, there are two options.

* [Angjoo Kanazawa & Shubham Tulsiani provides PyTorch wrapper of our renderer](https://github.com/akanazawa/cmr) used in their work ""Learning Category-Specific Mesh Reconstruction from Image Collections"" (ECCV 2018).
* [Nikos Kolotouros provides PyTorch re-implementation of our renderer](https://github.com/daniilidis-group/neural_renderer), which does not require installation of Chainer / CuPy.

I'm grateful to these researchers for writing and releasing their codes.

",2
370,neural_renderer, Running examples,"```
sudo python setup.py install
```

",2
371,neural_renderer, Example 1: Drawing an object from multiple viewpoints,"```
python ./examples/example1.py
python ./examples/example2.py
python ./examples/example3.py
python ./examples/example4.py
```


",2
372,neural_renderer, Example 2: Optimizing vertices,"![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example1.gif)

",2
373,neural_renderer, Example 3: Optimizing textures,"Transforming the silhouette of a teapot into a rectangle. The loss function is the difference between the rendered image and the reference image.

Reference image, optimization, and the result.

![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example2_ref.png) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example2_optimization.gif) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example2_result.gif)

",2
374,neural_renderer, Example 4: Finding camera parameters,"Matching the color of a teapot with a reference image.

Reference image, result.

![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example3_ref.png) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example3_result.gif)

",2
375,neural_renderer, FAQ,"The derivative of images with respect to camera pose can be computed through this renderer. In this example the position of the camera is optimized by gradient descent.

From left to right: reference image, initial state, and optimization process.

![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example4_ref.png) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example4_init.png) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example4_result.gif)

",2
376,neural_renderer, CPU implementation?,,3
377,neural_renderer, Python3 support?,"Currently, this code has no CPU implementation. Since CPU implementation would be probably too slow for practical usage, we do not plan to support CPU.

",3
378,neural_renderer, Citation,"Code in this repository is only for Python 2.x. [PyTorch port by Nikos Kolotourosr](https://github.com/daniilidis-group/neural_renderer), supports Python 3.x.

If you want to install neural renderer using Python 3, please add ./neural_renderer to $PYTHON_PATH temporarily as mentioned in [issue #6](https://github.com/hiroharu-kato/neural_renderer/issues/6). However, since we did not tested our code using Python 3, it might not work well.

",2
379,facebookresearch-DensePose, DensePose: ,,1
380,facebookresearch-DensePose, Installation,"**Dense Human Pose Estimation In The Wild**

_Rıza Alp Güler, Natalia Neverova, Iasonas Kokkinos_

[[`densepose.org`](https://densepose.org)] [[`arXiv`](https://arxiv.org/abs/1802.00434)] [[`BibTeX`](#CitingDensePose)]

Dense human pose estimation aims at mapping all human pixels of an RGB image to the 3D surface of the human body. 
DensePose-RCNN is implemented in the [Detectron](https://github.com/facebookresearch/Detectron) framework and is powered by [Caffe2](https://github.com/caffe2/caffe2).

<div align=""center"">
  <img src=""https://drive.google.com/uc?export=view&id=1qfSOkpueo1kVZbXOuQJJhyagKjMgepsz"" width=""700px"" />
</div>


In this repository, we provide the code to train and evaluate DensePose-RCNN. We also provide notebooks to visualize the collected DensePose-COCO dataset and show the correspondences to the SMPL model.

",2
381,facebookresearch-DensePose, Inference-Training-Testing,"Please find installation instructions for Caffe2 and DensePose in [`INSTALL.md`](INSTALL.md), a document based on the [Detectron](https://github.com/facebookresearch/Detectron) installation instructions.

",2
382,facebookresearch-DensePose, Notebooks,"After installation, please see [`GETTING_STARTED.md`](GETTING_STARTED.md)  for examples of inference and training and testing.

",2
383,facebookresearch-DensePose, Visualization of DensePose-COCO annotations:,,3
384,facebookresearch-DensePose, DensePose-COCO in 3D:,"See [`notebooks/DensePose-COCO-Visualize.ipynb`](notebooks/DensePose-COCO-Visualize.ipynb) to visualize the DensePose-COCO annotations on the images:

<div align=""center"">
  <img src=""https://drive.google.com/uc?export=view&id=1uYRJkIA24KkJU2i4sMwrKa61P0xtZzHk"" width=""800px"" />
</div>

---

",3
385,facebookresearch-DensePose, Visualize DensePose-RCNN Results:,"See [`notebooks/DensePose-COCO-on-SMPL.ipynb`](notebooks/DensePose-COCO-on-SMPL.ipynb) to localize the DensePose-COCO annotations on the 3D template ([`SMPL`](http://smpl.is.tue.mpg.de)) model:

<div align=""center"">
  <img src=""https://drive.google.com/uc?export=view&id=1m32oyMuE7AZd3EOf9k8zHpr75C8bHlYj"" width=""500px"" />
</div>

---
",3
386,facebookresearch-DensePose, DensePose-RCNN Texture Transfer:,"See [`notebooks/DensePose-RCNN-Visualize-Results.ipynb`](notebooks/DensePose-RCNN-Visualize-Results.ipynb) to visualize the inferred DensePose-RCNN Results.

<div align=""center"">
  <img src=""https://drive.google.com/uc?export=view&id=1k4HtoXpbDV9MhuyhaVcxDrXnyP_NX896"" width=""900px"" />
</div>

---
",3
387,facebookresearch-DensePose, License,"See [`notebooks/DensePose-RCNN-Texture-Transfer.ipynb`](notebooks/DensePose-RCNN-Texture-Transfer.ipynb) to localize the DensePose-COCO annotations on the 3D template ([`SMPL`](http://smpl.is.tue.mpg.de)) model:

<div align=""center"">
  <img src=""https://drive.google.com/uc?export=view&id=1r-w1oDkDHYnc1vYMbpXcYBVD1-V3B4Le"" width=""900px"" />
</div>

",2
388,facebookresearch-DensePose," <a name=""CitingDensePose""></a>Citing DensePose","This source code is licensed under the license found in the [`LICENSE`](LICENSE) file in the root directory of this source tree.

",2
389,node-qa-masker, node-qa-masker,,1
390,node-qa-masker, Installation,"This is a NodeJS port of [pymasker](https://github.com/haoliangyu/pymasker). It provides a convenient way to produce masks from the Quality Assessment band of Landsat 8 OLI images, as well as MODIS land products.

",2
391,node-qa-masker, Use Example,"``` bash
npm install qa-masker
```

",2
392,node-qa-masker, Landsat 8,,3
393,node-qa-masker, MODIS Land Products,"The `LandsatMasker` class provides the functionality to load and generate masks from the Quality Assessment band of Landsat 8 OLI image.

``` javascript
var qm = require('qa-masker');
var Masker = qm.LandsatMasker;
var Confidence = qm.LandsatConfidence;

// read the band file to initialize
var masker = new Masker('LC80170302016198LGN00_BQA.TIF');

// generate mask in ndarray format
var mask = masker.getWaterMask(Confidence.high);

// save the mask as GeoTIFF
masker.saveAsTif(mask, 'test.tif');
```

Five methods are provided for masking:

* `getCloudMask(confidence)`

* `getCirrusMask(confidence)`

* `getWaterMask(confidence)`

* `getVegMask(confidence)` (for vegetation)

* `getSnowMask(confidence)`

* `getFillMask()` (for filled pixels)

The `LandsatConfidence` class provide the definition of the confidence that certain condition exists at the pixel:

* `LandsatConfidence.high` (66% - 100% confidence)

* `LandsatConfidence.medium` (33% - 66% confidence)

* `LandsatConfidence.low` (0% - 33% confidence)

* `LandsatConfidence.undefined`

For more detail about the definition, please visit [the USGS Landsat website](http://landsat.usgs.gov/qualityband.php);

These five methods would return a [ndarray](https://github.com/scijs/ndarray) mask.

If a mask that matches multiple conditions is desired, the function `getMultiMask()` could help:

``` javascript
var mask = masker.getMultiMask([
  { type: 'could', confidence: LandsatConfidence.high },
  { type: 'cirrus', confidence: LandsatConfidence.medium }
]);
```

",3
394,node-qa-masker, Looking for command line tool?,"By using the lower level `Masker` class, the masking of MODIS land product QA band is supported. Because [node-gdal](https://github.com/scijs/ndarray) doesn't support HDF format, you need to convert the QA band to a GeoTIFF first using like [QGIS](http://www.qgis.org/en/site/),

A handy class `ModisMasker` is provided for particularly masking the quality of land products:

``` javascript
var qm = require('qa-masker');
var Masker = qm.ModisMasker;
var Quality = qm.ModisQuality;

// read the band file to initialize
var masker = new Masker('MODIS_QC_Band.tif');

// generate mask in ndarray format
var mask = masker.getQaMask(Quality.high);

// save the mask as GeoTIFF
masker.saveAsTif(mask, 'mask.tif');
```

The `ModisQuality` provides the definition of pixel quality:

  * `ModisQuality.high`: corrected product produced at ideal quality for all bands

  * `ModisQuality.medium`: corrected product produced at less than ideal quality for some or all bands

  * `ModisQuality.low`: corrected product not produced due to some reasons for some or all bands

  * `ModisQuality.low_cloud`: corrected product not produced due to cloud effects for all bands

Masking other than the product quality is not directly provided because of the variety of bit structure for different products.

A low-level method is available to extract mask with the understand of bit structure:

``` javascript
var masker = new Masker('modis_qa_band.tif');
var mask = masker.getMask(0, 2, 2);
```

`getMask(bitPos, bitLen, value)` function use to bit mask to extract quality mask:

* `bitPos`: the start position of quality assessment bits

* `bitLen`: the length of all used quality assessment bits

* `value`: the desired bit value (in integer)

For the detail explanation, please read [MODIS Land Product QA Tutorial](https://lpdaac.usgs.gov/sites/default/files/public/modis/docs/MODIS_LP_QA_Tutorial-1b.pdf).

",3
395,node-qa-masker, You are a GIS guy and want something GIS?,"If the command line tool is wanted, please use [pymasker](https://github.com/haoliangyu/pymasker).

",3
396,mapshaper, mapshaper,,1
397,mapshaper, Introduction,,3
398,mapshaper, Command line tool,"Mapshaper is software for editing Shapefile, GeoJSON, [TopoJSON](https://github.com/mbostock/topojson/wiki), CSV and several other data formats, written in JavaScript.

The `mapshaper` command line program supports essential map making tasks like simplifying shapes, editing attribute data, clipping, erasing, dissolving, filtering and more.

The web UI supports interactive simplification, attribute data editing, and running cli commands in a built-in console. Visit the public website at [www.mapshaper.org](http://www.mapshaper.org) or use the web UI locally via the `mapshaper-gui` script.

See the [project wiki](https://github.com/mbloch/mapshaper/wiki) for more documentation on how to use mapshaper.

To suggest improvements, add an [issue](https://github.com/mbloch/mapshaper/issues).

To learn about recent updates, read the [changelog](https://github.com/mbloch/mapshaper/releases).

",3
399,mapshaper, Interactive web interface,"The `mapshaper` command line program has been used successfully under Mac OS X, Linux and Windows.

The project wiki has an [introduction](https://github.com/mbloch/mapshaper/wiki/Introduction-to-the-Command-Line-Tool) to using the command line tool that includes many simple examples.

For a detailed reference, see the [Command Reference](https://github.com/mbloch/mapshaper/wiki/Command-Reference).



",3
400,mapshaper, Large file support,"The web UI works in recent desktop versions of Chrome, Firefox, Safari and Internet Explorer. Safari before v10.1 and IE before v10 are not supported.

The mapshaper distribution includes the script `mapshaper-gui`, which runs mapshaper's web interface locally. You can also visit [mapshaper.org](http://www.mapshaper.org) to use mapshaper online.

All processing is done in the browser, so your data stays private, even when using the public website.

",3
401,mapshaper, run the command line program,"**Web interface**

Firefox is able to load Shapefiles and GeoJSON files larger than 1GB. Chrome has improved in recent versions, but is still prone to out-of-memory errors when importing files larger than several hundred megabytes.

**Command line interface**

When working with very large files, mapshaper may become unresponsive or crash with the message ""JavaScript heap out of memory.""

One option is to run `mapshaper-xl` (added in v0.4.63), which allocates more memory than the standard `mapshaper` program.

Another solution is to run Node directly with the `--max-old-space-size` option. The following example (Mac or Linux) allocates 8GB of memory:
```bash
$ node  --max-old-space-size=8192 `which mapshaper` <mapshaper commands>
```

#notes:#notes:#notes: Installation

Mapshaper requires [Node.js](http://nodejs.org).

With Node installed, you can install the latest release version of mapshaper using npm. Install with the ""-g"" flag to make the executable scripts available systemwide.

```bash
npm install -g mapshaper
```

To install and run the latest development code from github:

```bash
git clone git@github.com:mbloch/mapshaper.git
cd mapshaper
npm install
bin/mapshaper     ",1
402,mapshaper, use the web UI locally,bin/mapshaper-gui ,1
403,mapshaper, Building and testing,"```

",3
404,mapshaper, License,"Run the `build` script to build both the cli and web UI modules.

Run `npm test` in the project directory to run mapshaper's tests.

",3
405,mapshaper, Acknowledgements,"This software is licensed under [MPL 2.0](http://www.mozilla.org/MPL/2.0/).

According to Mozilla's [FAQ](http://www.mozilla.org/MPL/2.0/FAQ.html), ""The MPL's ‘file-level’ copyleft is designed to encourage contributors to share modifications they make to your code, while still allowing them to combine your code with code under other licenses (open or proprietary) with minimal restrictions.""



",3
406,segyio, segyio ,,2
407,segyio, Index ,"[![Travis](https://img.shields.io/travis/equinor/segyio/master.svg?label=travis)](https://travis-ci.org/equinor/segyio)
[![Appveyor](https://ci.appveyor.com/api/projects/status/2i5cr8ui2t9qbxk9?svg=true)](https://ci.appveyor.com/project/statoil-travis/segyio)
[![PyPI Updates](https://pyup.io/repos/github/equinor/segyio/shield.svg)](https://pyup.io/repos/github/equinor/segyio/)
[![Python 3](https://pyup.io/repos/github/equinor/segyio/python-3-shield.svg)](https://pyup.io/repos/github/equinor/segyio/)

[readthedocs](https://segyio.readthedocs.io/)

",4
408,segyio, Introduction ,"* [Introduction](#introduction)
* [Feature summary](#feature-summary)
* [Getting started](#getting-started)
    * [Quick start](#quick-start)
    * [Get segyio](#get-segyio)
    * [Build segyio](#build-segyio)
* [Tutorial](#tutorial)
    * [Basics](#basics)
    * [Modes](#modes)
    * [Mode examples](#mode-examples)
* [Goals](#project-goals)
* [Contributing](#contributing)
* [Examples](#examples)
* [Common issues](#common-issues)
* [History](#history)

",4
409,segyio, Feature summary ,"Segyio is a small LGPL licensed C library for easy interaction with SEG-Y and
Seismic Unix formatted seismic data, with language bindings for Python and
Matlab. Segyio is an attempt to create an easy-to-use, embeddable,
community-oriented library for seismic applications. Features are added as they
are needed; suggestions and contributions of all kinds are very welcome.

To catch up on the latest development and features, see the
[changelog](changelog.md). To write future proof code, consult the planned
[breaking changes](breaking-changes.md).

",4
410,segyio, Getting started ,"  * A low-level C interface with few assumptions; easy to bind to other
    languages
  * Read and write binary and textual headers
  * Read and write traces and trace headers
  * Simple, powerful, and native-feeling Python interface with numpy
    integration
  * Read and write seismic unix files
  * xarray integration with netcdf_segy
  * Some simple applications with unix philosophy

",4
411,segyio, Quick start ,"When segyio is built and installed, you're ready to start programming! Check
out the [tutorial](#tutorial), [examples](#examples), [example
programs](python/examples), and [example
notebooks](https://github.com/equinor/segyio-notebooks). For a technical
reference with examples and small recipes, [read the
docs](https://segyio.readthedocs.io/). API docs are also available with pydoc -
start your favourite Python interpreter and type `help(segyio)`, which should
integrate well with IDLE, pycharm and other Python tools.

",6
412,segyio, Get segyio ,"```python
import segyio
import numpy as np
with segyio.open('file.sgy') as f:
    for trace in f.trace:
        filtered = trace[np.where(trace < 1e-2)]
```

See the [examples](#examples) for more.

",6
413,segyio, Build segyio ,"A copy of segyio is available both as pre-built binaries and source code:

* In Debian [unstable](https://packages.debian.org/source/sid/segyio)
    * `apt install python3-segyio`
* Wheels for Python from [PyPI](https://pypi.python.org/pypi/segyio/)
    * `pip install segyio`
* Source code from [github](https://github.com/equinor/segyio)
    * `git clone https://github.com/statoil/segyio`
* Source code in [tarballs](https://github.com/equinor/segyio/releases)

",6
414,segyio, Developers ,"To build segyio you need:
 * A C99 compatible C compiler (tested mostly on gcc and clang)
 * A C++ compiler for the Python extension, and C++11 for the tests
 * [CMake](https://cmake.org/) version 2.8.12 or greater
 * [Python](https://www.python.org/) 2.7 or 3.x.
 * [numpy](http://www.numpy.org/) version 1.10 or greater
 * [setuptools](https://pypi.python.org/pypi/setuptools) version 28 or greater
 * [setuptools-scm](https://pypi.python.org/pypi/setuptools_scm)
 * [pytest](https://pypi.org/project/pytest)

 To build the documentation, you also need
 [sphinx](https://pypi.org/project/Sphinx)

To build and install segyio, perform the following actions in your console:

```bash
git clone https://github.com/equinor/segyio
mkdir segyio/build
cd segyio/build
cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON
make
make install
```

`make install` must be done as root for a system install; if you want to
install in your home directory, add `-DCMAKE_INSTALL_PREFIX=~/` or some other
appropriate directory, or `make DESTDIR=~/ install`. Please ensure your
environment picks up on non-standard install locations (PYTHONPATH,
LD_LIBRARY_PATH and PATH).

If you have multiple Python installations, or want to use some alternative
interpreter, you can help cmake find the right one by passing
`-DPYTHON_EXECUTABLE=/opt/python/binary` along with install prefix and build
type.

To build the matlab bindings, invoke CMake with the option `-DBUILD_MEX=ON`. In
some environments the Matlab binaries are in a non-standard location, in which
case you need to help CMake find the matlab binaries by passing
`-DMATLAB_ROOT=/path/to/matlab`.

",8
415,segyio, Tutorial ,"It's recommended to build in debug mode to get more warnings and to embed debug
symbols in the objects. Substituting `Debug` for `Release` in the
`CMAKE_BUILD_TYPE` is plenty.

Tests are located in the language/tests directories, and it's highly
recommended that new features added are demonstrated for correctness and
contract by adding a test. All tests can be run by invoking `ctest`. Feel free
to use the tests already written as a guide.

After building segyio you can run the tests with `ctest`, executed from the
build directory.

Please note that to run the Python examples you need to let your environment
know where to find the Python library. It can be installed as a user, or on
adding the segyio/build/python library to your pythonpath.

",4
416,segyio, Basics ,"All code in this tutorial assumes segyio is imported, and that numpy is
available as np.

```python
import segyio
import numpy as np
```

This tutorial assumes you're familiar with Python and numpy. For a refresh,
check out the [python tutorial](https://docs.python.org/3/tutorial/) and [numpy
quickstart](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html)

",6
417,segyio, Modes ,"Opening a file for reading is done with the `segyio.open` function, and
idiomatically used with context managers. Using the `with` statement, files are
properly closed even in the case of exceptions. By default, files are opened
read-only.

```python
with segyio.open(filename) as f:
    ...
```

Open accepts several options (for more a more comprehensive reference, check
the open function's docstring with `help(segyio.open)`. The most important
option is the second (optional) positional argument. To open a file for
writing, do `segyio.open(filename, 'r+')`, from the C `fopen` function.

Files can be opened in *unstructured* mode, either by passing `segyio.open` the
optional arguments `strict=False`, in which case not establishing structure
(inline numbers, crossline numbers etc.) is not an error, and
`ignore_geometry=True`, in which case segyio won't even try to set these
internal attributes.

The segy file object has several public attributes describing this structure:
* `f.ilines`
    Inferred inline numbers
* `f.xlines`
    Inferred crossline numbers
* `f.offsets`
    Inferred offsets numbers
* `f.samples`
    Inferred sample offsets (frequency and recording time delay)
* `f.unstructured`
    True if unstructured, False if structured
* `f.ext_headers`
    The number of extended textual headers

If the file is opened *unstructured*, all the line properties will will be
`None`.

",6
418,segyio, Mode examples ,"In segyio, data is retrieved and written through so-called *modes*. Modes are
abstract arrays, or addressing schemes, and change what names and indices mean.
All modes are properties on the file handle object, support the `len` function,
and reads and writes are done through `f.mode[]`. Writes are done with
assignment. Modes support array slicing inspired by numpy. The following modes
are available:

* `trace`

    The trace mode offers raw addressing of traces as they are laid out in the
    file. This, along with `header`, is the only mode available for
    unstructured files. Traces are enumerated `0..len(f.trace)`.

    Reading a trace yields a numpy `ndarray`, and reading multiple traces
    yields a generator of `ndarray`s. Generator semantics are used and the same
    object is reused, so if you want to cache or address trace data later, you
    must explicitly copy.

    ```python
    >>> f.trace[10]
    >>> f.trace[-2]
    >>> f.trace[15:45]
    >>> f.trace[:45:3]
    ```

* `header`

    With addressing behaviour similar to `trace`, accessing items yield header
    objects instead of numpy `ndarray`s. Headers are dict-like objects, where
    keys are integers, seismic unix-style keys (in segyio.su module) and segyio
    enums (segyio.TraceField).

    Header values can be updated by assigning a dict-like to it, and keys not
    present on the right-hand-side of the assignment are *unmodified*.

    ```python
    >>> f.header[5] = { segyio.su.tracl: 10 }
    >>> f.header[5].items()
    >>> f.header[5][25, 37] #notes: read multiple values at once
    ```

* `iline`, `xline`

    These modes will raise an error if the file is unstructured. They consider
    arguments to `[]` as the *keys* of the respective lines. Line numbers are
    always increasing, but can have arbitrary, uneven spacing. The valid names
    can be found in the `ilines` and `xlines` properties.

    As with traces, getting one line yields an `ndarray`, and a slice of lines
    yields a generator of `ndarray`s. When using slices with a step, some
    intermediate items might be skipped if it is not matched by the step, i.e.
    doing `f.line[1:10:3]` on a file with lines `[1,2,3,4,5]` is equivalent of
    looking up `1, 4, 7`, and finding `[1,4]`.

    When working with a 4D pre-stack file, the first offset is implicitly read.
    To access a different or a range of offsets, use comma separated indices or
    ranges, as such: `f.iline[120, 4]`.

* `fast`, `slow`

    These are aliases for `iline` and `xline`, determined by how the traces are
    laid out. For inline sorted files, `fast` would yield `iline`.

* `depth_slice`

    The depth slice is a horizontal, file-wide cut at a depth. The yielded
    values are `ndarray`s and generators-of-arrays.

* `gather`

    The `gather` is the intersection of an inline and crossline, a vertical
    column of the survey, and unless a single offset is specified returns an
    offset x samples `ndarray`. In the presence of ranges, it returns a
    generator of such `ndarray`s.

* `text`

    The `text` mode is an array of the textual headers, where `text[0]` is the
    standard-mandated textual header, and `1..n` are the optional extended
    headers.

    The text headers are returned as 3200-byte string-like blobs (bytes in
    Python 3, str in Python 2), as it is in the file. The `segyio.tools.wrap`
    function can create a line-oriented version of this string.

* `bin`

    The values of the file-wide binary header with a dict-like interface.
    Behaves like the `header` mode, but without the indexing.

",6
419,segyio, Project goals ,"```python
>>> for line in f.iline[:2430]:
...     print(np.average(line))

>>> for line in f.xline[2:10]:
...     print(line)

>>> for line in f.fast[::2]:
...     print(np.min(line))

>>> for factor, offset in enumerate(f.iline[10, :]):
...     offset *= factor
        print(offset)

>>> f.gather[200, 241, :].shape

>>> text = f.text[0]
>>> type(text)
<type 'bytes'> #notes: 'str' in Python 2

>>> f.trace[10] = np.zeros(len(f.samples))
```

More examples and recipes can be found in the docstrings `help(segyio)` and the
[examples](#examples) section.

",4
420,segyio, SEG-Y Revisions ,"Segyio does not necessarily attempt to be the end-all of SEG-Y interactions;
rather, we aim to lower the barrier to interacting with SEG-Y files for
embedding, new applications or free-standing programs.

Additionally, the aim is not to support the full standard or all exotic (but
standard compliant) formatted files out there. Some assumptions are made, such
as:

 * All traces in a file are assumed to be of the same size

Currently, segyio supports:
 * Post-stack 3D volumes, sorted with respect to two header words (generally
   INLINE and CROSSLINE)
 * Pre-stack 4D volumes, sorted with respect to three header words (generally
   INLINE, CROSSLINE, and OFFSET)
 * Unstructured data, i.e. a collection of traces
 * Most numerical formats (including IEEE 4- and 8-byte float, IBM float, 2-
   and 4-byte integers)

The writing functionality in segyio is largely meant to *modify* or adapt
files. A file created from scratch is not necessarily a to-spec SEG-Y file, as
we only necessarily write the header fields segyio needs to make sense of the
geometry. It is still highly recommended that SEG-Y files are maintained and
written according to specification, but segyio **does not** enforce this.


",6
421,segyio, Contributing ,"Segyio can handle a lot of files that are SEG-Y-like, i.e. segyio handles files
that don't strictly conform to the SEG-Y standard. Segyio also does not
discriminate between the revisions, but instead tries to use information
available in the file. For an *actual* standard's reference, please see the
publications by SEG:

- [SEG-Y 0 (1975)](https://seg.org/Portals/0/SEG/News%20and%20Resources/Technical%20Standards/seg_y_rev0.pdf)
- [SEG-Y 1 (2002)](https://seg.org/Portals/0/SEG/News%20and%20Resources/Technical%20Standards/seg_y_rev1.pdf)
- [SEG-Y 2 (2017)](https://seg.org/Portals/0/SEG/News%20and%20Resources/Technical%20Standards/seg_y_rev2_0-mar2017.pdf)

",4
422,segyio, xarray integration ,"We welcome all kinds of contributions, including code, bug reports, issues,
feature requests, and documentation. The preferred way of submitting a
contribution is to either make an
[issue](https://github.com/equinor/segyio/issues) on github or by forking the
project on github and making a pull request.

",4
423,segyio, Reproducing the test data ,"[Alan Richardson](https://github.com/ar4) has written a great little tool for
using [xarray](http://xarray.pydata.org/en/stable/) with segy files, which he
demos in this
[notebook](https://github.com/ar4/netcdf_segy/blob/master/notebooks/netcdf_segy.ipynb)

",4
424,segyio, Examples ,"Small SEG-Y formatted files are included in the repository for test purposes.
The data is non-sensical and made to be predictable, and it is reproducible by
using segyio. The tests file are located in the test-data directory. To
reproduce the data file, build segyio and run the test program `make-file.py`,
`make-ps-file.py`, and `make-rotated-copies.py` as such:

```python
python examples/make-file.py small.sgy 50 1 6 20 25
python examples/make-ps-file.py small-ps.sgy 10 1 5 1 4 1 3
python examples/make-rotated-copies.py small.sgy
```

The small-lsb.sgy file was created by running the flip-endianness program. This
program is included in the segyio source tree, but not a part of the package,
and not intended for distribution and installation, only for reproducing test
files.

The seismic unix file small.su and small-lsb.su were created by the following
commands:

```bash
segyread tape=small.sgy ns=50 remap=tracr,cdp byte=189l,193l conv=1 format=1 \
         > small-lsb.su
suswapbytes < small.su > small-lsb.su
```

If you have have small data files with a free license, feel free to submit it
to the project!

",4
425,segyio, Python ,,6
426,segyio, MATLAB ,"Import useful libraries:

```python
import segyio
import numpy as np
from shutil import copyfile
```

Open segy file and inspect it:

```python
filename = 'name_of_your_file.sgy'
with segyio.open(filename) as segyfile:

    #notes: Memory map file for faster reading (especially if file is big...)
    segyfile.mmap()

    #notes: Print binary header info
    print(segyfile.bin)
    print(segyfile.bin[segyio.BinField.Traces])

    #notes: Read headerword inline for trace 10
    print(segyfile.header[10][segyio.TraceField.INLINE_3D])

    #notes: Print inline and crossline axis
    print(segyfile.xlines)
    print(segyfile.ilines)
```

Read post-stack data cube contained in segy file:

```python
#notes: Read data along first xline
data = segyfile.xline[segyfile.xlines[1]]

#notes: Read data along last iline
data = segyfile.iline[segyfile.ilines[-1]]

#notes: Read data along 100th time slice
data = segyfile.depth_slice[100]

#notes: Read data cube
data = segyio.tools.cube(filename)
```

Read pre-stack data cube contained in segy file:

```python
filename = 'name_of_your_prestack_file.sgy'
with segyio.open(filename) as segyfile:

    #notes: Print offsets
    print(segyfile.offset)

    #notes: Read data along first iline and offset 100:  data [nxl x nt]
    data = segyfile.iline[0, 100]

    #notes: Read data along first iline and all offsets gath:  data [noff x nxl x nt]
    data = np.asarray([np.copy(x) for x in segyfile.iline[0:1, :]])

    #notes: Read data along first 5 ilines and all offsets gath:  data [noff nil x nxl x nt]
    data = np.asarray([np.copy(x) for x in segyfile.iline[0:5, :]])

    #notes: Read data along first xline and all offsets gath:  data [noff x nil x nt]
    data = np.asarray([np.copy(x) for x in segyfile.xline[0:1, :]])
```

Read and understand fairly 'unstructured' data (e.g., data sorted in common shot gathers):

```python
filename = 'name_of_your_prestack_file.sgy'
with segyio.open(filename, ignore_geometry=True) as segyfile:
    segyfile.mmap()

    #notes: Extract header word for all traces
    sourceX = segyfile.attributes(segyio.TraceField.SourceX)[:]

    #notes: Scatter plot sources and receivers color-coded on their number
    plt.figure()
    sourceY = segyfile.attributes(segyio.TraceField.SourceY)[:]
    nsum = segyfile.attributes(segyio.TraceField.NSummedTraces)[:]
    plt.scatter(sourceX, sourceY, c=nsum, edgecolor='none')

    groupX = segyfile.attributes(segyio.TraceField.GroupX)[:]
    groupY = segyfile.attributes(segyio.TraceField.GroupY)[:]
    nstack = segyfile.attributes(segyio.TraceField.NStackedTraces)[:]
    plt.scatter(groupX, groupY, c=nstack, edgecolor='none')
```

Write segy file using same header of another file but multiply data by *2

```python
input_file = 'name_of_your_input_file.sgy'
output_file = 'name_of_your_output_file.sgy'

copyfile(input_file, output_file)

with segyio.open(output_file, ""r+"") as src:

    #notes: multiply data by 2
    for i in src.ilines:
        src.iline[i] = 2 * src.iline[i]
```

[Make segy file from sctrach](python/examples/make-file.py)

",6
427,segyio, Common issues ,"```
filename='name_of_your_file.sgy'

% Inspect segy
Segy_struct=SegySpec(filename,189,193,1);

% Read headerword inline for each trace
Segy.get_header(filename,'Inline3D')

%Read data along first xline
data= Segy.readCrossLine(Segy_struct,Segy_struct.crossline_indexes(1));

%Read cube
data=Segy.get_cube(Segy_struct);

%Write segy, use same header but multiply data by *2
input_file='input_file.sgy';
output_file='output_file.sgy';
copyfile(input_file,output_file)
data = Segy.get_traces(input_file);
data1 = 2*data;
Segy.put_traces(output_file, data1);
```

",4
428,segyio, ImportError: libsegyio.so.1: cannot open shared object file,,3
429,segyio, Possible solutions,"This error shows up when the loader cannot find the core segyio library. If
you've explicitly set the install prefix (with `-DCMAKE_INSTALL_PREFIX`) you
must configure your loader to also look in this prefix, either with a
`ld.conf.d` file or the `LD_LIBRARY_PATH` variable.

If you haven't set `CMAKE_INSTALL_PREFIX`, cmake will by default install to
`/usr/local`, which your loader usually knows about. On Debian based systems,
the library often gets installed to `/usr/local/lib`, which the loader may not
know about. See [issue #239](https://github.com/equinor/segyio/issues/239).

",4
430,segyio, RuntimeError: unable to find sorting,"* Configure the loader (`sudo ldconfig` often does the trick)
* Install with a different, known prefix, e.g. `-DCMAKE_INSTALL_LIBDIR=lib64`

",3
431,segyio, Possible solutions,"This exception is raised when segyio tries to open the in strict mode, under
the assumption that the file is a regular, sorted 3D volume. If the file is
just a collection of traces in arbitrary order, this would fail.

",4
432,segyio, History ,"Segyio supports files that are just a collection of traces too, but has to be
told that it's ok to do so. Pass `strict = False` or `ignore_geometry = True`
to `segyio.open` to allow or force unstructured mode respectively. Please note
that `f.iline` and similar features are now disabled and will raise errors.

",4
433,vid2vid, vid2vid,"<br><br><br><br>

",1
434,vid2vid, [Project](https://tcwang0509.github.io/vid2vid/) | [YouTube(short)](https://youtu.be/5zlcXTCpQqM) | [YouTube(full)](https://youtu.be/GrP_aOSXt5U) | [arXiv](https://arxiv.org/abs/1808.06601) | [Paper(full)](https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf),,3
435,vid2vid, Video-to-Video Translation,"Pytorch implementation for high-resolution (e.g., 2048x1024) photorealistic video-to-video translation. It can be used for turning semantic label maps into photo-realistic videos, synthesizing people talking from edge maps, or generating human motions from poses. The core of video-to-video translation is image-to-image translation. Some of our work in that space can be found in [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE). <br><br>
[Video-to-Video Synthesis](https://tcwang0509.github.io/vid2vid/)  
 [Ting-Chun Wang](https://tcwang0509.github.io/)<sup>1</sup>, [Ming-Yu Liu](http://mingyuliu.net/)<sup>1</sup>, [Jun-Yan Zhu](http://people.csail.mit.edu/junyanz/)<sup>2</sup>, [Guilin Liu](https://liuguilin1225.github.io/)<sup>1</sup>, Andrew Tao<sup>1</sup>, [Jan Kautz](http://jankautz.com/)<sup>1</sup>, [Bryan Catanzaro](http://catanzaro.name/)<sup>1</sup>  
 <sup>1</sup>NVIDIA Corporation, <sup>2</sup>MIT CSAIL  
 In Neural Information Processing Systems (**NeurIPS**) 2018  

",2
436,vid2vid, Prerequisites,"- Label-to-Streetview Results
<p align='center'>  
  <img src='imgs/city_change_styles.gif' width='440'/>  
  <img src='imgs/city_change_labels.gif' width='440'/>
</p>

- Edge-to-Face Results
<p align='center'>
  <img src='imgs/face.gif' width='440'/>
  <img src='imgs/face_multiple.gif' width='440'/>
</p>

- Pose-to-Body Results
<p align='center'>
  <img src='imgs/pose.gif' width='550'/>
</p>

- Frame Prediction Results
<p align='center'>
  <img src='imgs/framePredict.gif' width='550'/>
</p>

",2
437,vid2vid, Getting Started,"- Linux or macOS
- Python 3
- NVIDIA GPU + CUDA cuDNN
- PyTorch 0.4


",2
438,vid2vid, Installation,,3
439,vid2vid, Testing ,"- Install python libraries [dominate](https://github.com/Knio/dominate) and requests.
```bash
pip install dominate requests
```
- If you plan to train with face datasets, please install dlib.
```bash
pip install dlib
```
- If you plan to train with pose datasets, please install [DensePose](https://github.com/facebookresearch/DensePose) and/or [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose).
- Clone this repo:
```bash
git clone https://github.com/NVIDIA/vid2vid
cd vid2vid
```
- Docker Image
If you have difficulty building the repo, a docker image can be found in the `docker` folder.

",3
440,vid2vid, Dataset,"- Please first download example dataset by running `python scripts/download_datasets.py`.
- Next, download and compile a snapshot of [FlowNet2](https://github.com/NVIDIA/flownet2-pytorch) by running `python scripts/download_flownet2.py`.
- Cityscapes    
  - Please download the pre-trained Cityscapes model by:
    ```bash
    python scripts/street/download_models.py
    ```
  - To test the model (`bash ./scripts/street/test_2048.sh`):
    ```bash
    #notes:!./scripts/street/test_2048.sh
    python test.py --name label2city_2048 --label_nc 35 --loadSize 2048 --n_scales_spatial 3 --use_instance --fg --use_single_G
    ```
    The test results will be saved in: `./results/label2city_2048/test_latest/`.

  - We also provide a smaller model trained with single GPU, which produces slightly worse performance at 1024 x 512 resolution.
    - Please download the model by
    ```bash
    python scripts/street/download_models_g1.py
    ```
    - To test the model (`bash ./scripts/street/test_g1_1024.sh`):
    ```bash
    #notes:!./scripts/street/test_g1_1024.sh
    python test.py --name label2city_1024_g1 --label_nc 35 --loadSize 1024 --n_scales_spatial 3 --use_instance --fg --n_downsample_G 2 --use_single_G
    ```
  - You can find more example scripts in the `scripts/street/` directory.

- Faces
  - Please download the pre-trained model by:
    ```bash
    python scripts/face/download_models.py
    ```
  - To test the model (`bash ./scripts/face/test_512.sh`):
    ```bash
    #notes:!./scripts/face/test_512.sh
    python test.py --name edge2face_512 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 512 --use_single_G
    ```
    The test results will be saved in: `./results/edge2face_512/test_latest/`.

",3
441,vid2vid, Training with Cityscapes dataset,"- Cityscapes
  - We use the Cityscapes dataset as an example. To train a model on the full dataset, please download it from the [official website](https://www.cityscapes-dataset.com/) (registration required).
  - We apply a pre-trained segmentation algorithm to get the corresponding semantic maps (train_A) and instance maps (train_inst).
  - Please add the obtained images to the `datasets` folder in the same way the example images are provided.
- Face
  - We use the [FaceForensics](http://niessnerlab.org/projects/roessler2018faceforensics.html) dataset. We then use landmark detection to estimate the face keypoints, and interpolate them to get face edges.
- Pose
  - We use random dancing videos found on YouTube. We then apply DensePose / OpenPose to estimate the poses for each frame.

",3
442,vid2vid, Training with face datasets,"- First, download the FlowNet2 checkpoint file by running `python scripts/download_models_flownet2.py`.
- Training with 8 GPUs:
  - We adopt a coarse-to-fine approach, sequentially increasing the resolution from 512 x 256, 1024 x 512, to 2048 x 1024.
  - Train a model at 512 x 256 resolution (`bash ./scripts/street/train_512.sh`)
  ```bash
  #notes:!./scripts/street/train_512.sh
  python train.py --name label2city_512 --label_nc 35 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 6 --use_instance --fg
  ```
  - Train a model at 1024 x 512 resolution (must train 512 x 256 first) (`bash ./scripts/street/train_1024.sh`):
  ```bash
  #notes:!./scripts/street/train_1024.sh
  python train.py --name label2city_1024 --label_nc 35 --loadSize 1024 --n_scales_spatial 2 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 4 --use_instance --fg --niter_step 2 --niter_fix_global 10 --load_pretrain checkpoints/label2city_512
  ```
If you have TensorFlow installed, you can see TensorBoard logs in `./checkpoints/label2city_1024/logs` by adding `--tf_log` to the training scripts.

- Training with a single GPU:
  - We trained our models using multiple GPUs. For convenience, we provide some sample training scripts (train_g1_XXX.sh) for single GPU users, up to 1024 x 512 resolution. Again a coarse-to-fine approach is adopted (256 x 128, 512 x 256, 1024 x 512). Performance is not guaranteed using these scripts.
  - For example, to train a 256 x 128 video with a single GPU (`bash ./scripts/street/train_g1_256.sh`)
  ```bash
  #notes:!./scripts/street/train_g1_256.sh
  python train.py --name label2city_256_g1 --label_nc 35 --loadSize 256 --use_instance --fg --n_downsample_G 2 --num_D 1 --max_frames_per_gpu 6 --n_frames_total 6
  ```

- Training at full (2k x 1k) resolution
  - To train the images at full resolution (2048 x 1024) requires 8 GPUs with at least 24G memory (`bash ./scripts/street/train_2048.sh`). If only GPUs with 12G/16G memory are available, please use the script `./scripts/street/train_2048_crop.sh`, which will crop the images during training. Performance is not guaranteed with this script.

",3
443,vid2vid, Training with pose datasets,"- If you haven't, please first download example dataset by running `python scripts/download_datasets.py`.
- Run the following command to compute face landmarks for training dataset: 
  ```bash
  python data/face_landmark_detection.py train
  ```
- Run the example script (`bash ./scripts/face/train_512.sh`)
  ```bash
  python train.py --name edge2face_512 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 512 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12  
  ```
- For single GPU users, example scripts are in train_g1_XXX.sh. These scripts are not fully tested and please use at your own discretion. If you still hit out of memory errors, try reducing `max_frames_per_gpu`.
- More examples scripts can be found in `scripts/face/`.
- Please refer to [More Training/Test Details](https://github.com/NVIDIA/vid2vid#more-trainingtest-details) for more explanations about training flags.


",3
444,vid2vid, Training with your own dataset,"- If you haven't, please first download example dataset by running `python scripts/download_datasets.py`.
- Example DensePose and OpenPose results are included. If you plan to use your own dataset, please generate these results and put them in the same way the example dataset is provided.
- Run the example script (`bash ./scripts/pose/train_256p.sh`)
  ```bash
  python train.py --name pose2body_256p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --num_D 2 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 384 --fineSize 256 --gpu_ids 0,1,2,3,4,5,6,7 --batchSize 8 --max_frames_per_gpu 3 --no_first_img --n_frames_total 12 --max_t_step 4
  ```
- Again, for single GPU users, example scripts are in train_g1_XXX.sh. These scripts are not fully tested and please use at your own discretion. If you still hit out of memory errors, try reducing `max_frames_per_gpu`.
- More examples scripts can be found in `scripts/pose/`.
- Please refer to [More Training/Test Details](https://github.com/NVIDIA/vid2vid#more-trainingtest-details) for more explanations about training flags.

",3
445,vid2vid, More Training/Test Details,"- If your input is a label map, please generate label maps which are one-channel whose pixel values correspond to the object labels (i.e. 0,1,...,N-1, where N is the number of labels). This is because we need to generate one-hot vectors from the label maps. Please use `--label_nc N` during both training and testing.
- If your input is not a label map, please specify `--input_nc N` where N is the number of input channels (The default is 3 for RGB images).
- The default setting for preprocessing is `scaleWidth`, which will scale the width of all training images to `opt.loadSize` (1024) while keeping the aspect ratio. If you want a different setting, please change it by using the `--resize_or_crop` option. For example, `scaleWidth_and_crop` first resizes the image to have width `opt.loadSize` and then does random cropping of size `(opt.fineSize, opt.fineSize)`. `crop` skips the resizing step and only performs random cropping. `scaledCrop` crops the image while retraining the original aspect ratio. `randomScaleHeight` will randomly scale the image height to be between `opt.loadSize` and `opt.fineSize`. If you don't want any preprocessing, please specify `none`, which will do nothing other than making sure the image is divisible by 32.

",2
446,vid2vid, Citation,"- We generate frames in the video sequentially, where the generation of the current frame depends on previous frames. To generate the first frame for the model, there are 3 different ways:  
  - 1. Using another generator which was trained on generating single images (e.g., pix2pixHD) by specifying `--use_single_G`. This is the option we use in the test scripts.
  - 2. Using the first frame in the real sequence by specifying `--use_real_img`. 
  - 3. Forcing the model to also synthesize the first frame by specifying `--no_first_img`. This must be trained separately before inference.
- The way we train the model is as follows: suppose we have 8 GPUs, 4 for generators and 4 for discriminators, and we want to train 28 frames. Also, assume each GPU can generate only one frame. The first GPU generates the first frame, and pass it to the next GPU, and so on. After the 4 frames are generated, they are passed to the 4 discriminator GPUs to compute the losses. Then the last generated frame becomes input to the next batch, and the next 4 frames in the training sequence are loaded into GPUs. This is repeated 7 times (4 x 7 = 28), to train all the 28 frames.
- Some important flags:
  - `n_gpus_gen`: the number of GPUs to use for generators (while the others are used for discriminators). We separate generators and discriminators into different GPUs since when dealing with high resolutions, even one frame cannot fit in a GPU. If the number is set to `-1`, there is no separation and all GPUs are used for both generators and discriminators (only works for low-res images).
  - `n_frames_G`: the number of input frames to feed into the generator network; i.e., `n_frames_G - 1` is the number of frames we look into the past. the default is 3 (conditioned on previous two frames).
  - `n_frames_D`: the number of frames to feed into the temporal discriminator. The default is 3.
  - `n_scales_spatial`: the number of scales in the spatial domain. We train from the coarsest scale and all the way to the finest scale. The default is 3.
  - `n_scales_temporal`: the number of scales for the temporal discriminator. The finest scale takes in the sequence in the original frame rate. The coarser scales subsample the frames by a factor of `n_frames_D` before feeding the frames into the discriminator. For example, if `n_frames_D = 3` and `n_scales_temporal = 3`, the discriminator effectively sees 27 frames. The default is 3.
  - `max_frames_per_gpu`: the number of frames in one GPU during training. If you run into out of memory error, please first try to reduce this number. If your GPU memory can fit more frames, try to make this number bigger to make training faster. The default is 1.
  - `max_frames_backpropagate`: the number of frames that loss backpropagates to previous frames. For example, if this number is 4, the loss on frame n will backpropagate to frame n-3. Increasing this number will slightly improve the performance, but also cause training to be less stable. The default is 1.
  - `n_frames_total`: the total number of frames in a sequence we want to train with. We gradually increase this number during training.
  - `niter_step`: for how many epochs do we double `n_frames_total`. The default is 5.  
  - `niter_fix_global`: if this number if not 0, only train the finest spatial scale for this number of epochs before starting to fine-tune all scales.
  - `batchSize`: the number of sequences to train at a time. We normally set batchSize to 1 since often, one sequence is enough to occupy all GPUs. If you want to do batchSize > 1, currently only `batchSize == n_gpus_gen` is supported.
  - `no_first_img`: if not specified, the model will assume the first frame is given and synthesize the successive frames. If specified, the model will also try to synthesize the first frame instead.
  - `fg`: if specified, use the foreground-background separation model as stated in the paper. The foreground labels must be specified by `--fg_labels`.
  - `no_flow`: if specified, do not use flow warping and directly synthesize frames. We found this usually still works reasonably well when the background is static, while saving memory and training time.
  - `sparse_D`: if specified, only apply temporal discriminator on sparse frames in the sequence. This helps save memory while having little effect on performance.
- For other flags, please see `options/train_options.py` and `options/base_options.py` for all the training flags; see `options/test_options.py` and `options/base_options.py` for all the test flags.

- Additional flags for edge2face examples:
  - `no_canny_edge`: do not use canny edges for background as input.
  - `no_dist_map`: by default, we use distrance transform on the face edge map as input. This flag will make it directly use edge maps.

- Additional flags for pose2body examples:
  - `densepose_only`: use only densepose results as input. Please also remember to change `input_nc` to be 3.
  - `openpose_only`: use only openpose results as input. Please also remember to change `input_nc` to be 3.
  - `add_face_disc`: add an additional discriminator that only works on the face region.
  - `remove_face_labels`: remove densepose results for face, and add noise to openpose face results, so the network can get more robust to different face shapes. This is important if you plan to do inference on half-body videos (if not, usually this flag is unnecessary).
  - `random_drop_prob`: the probability to randomly drop each pose segment during training, so the network can get more robust to missing poses at inference time. Default is 0.05.
  - `basic_point_only`: if specified, only use basic joint keypoints for OpenPose output, without using any hand or face keypoints.

",2
447,vid2vid, Acknowledgments,"If you find this useful for your research, please cite the following paper.

```
@inproceedings{wang2018vid2vid,
   author    = {Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu and Guilin Liu
                and Andrew Tao and Jan Kautz and Bryan Catanzaro},
   title     = {Video-to-Video Synthesis},
   booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},   
   year      = {2018},
}
```

",2
448,PVGeo, Demonstrations of *PVGeo*,"Share this project: [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Check%20out%20this%20project%20for%20data%20and%20model%20visualization%20in%20ParaView&url=https://github.com/OpenGeoVis/PVGeo&hashtags=ParaView,PVGeo,visualization,geoscience)


The *PVGeo* Python package contains VTK powered tools for data visualization in
geophysics which are wrapped for direct use within the application
[ParaView by Kitware](https://www.paraview.org) or in a Python environment with
[**PyVista**](https://github.com/pyvista/pyvista). These tools are tailored to
data visualization in the geosciences with a heavy focus on structured data sets
like 2D or 3D time-varying grids.


**Learn More:**
[![Vimeo](https://img.shields.io/badge/demos-grey.svg?logo=vimeo)](https://vimeo.com/user82050125)
[![Slack Badge](https://img.shields.io/badge/Slack-PVGeo-4B0082.svg?logo=slack)](http://slack.pvgeo.org)

**Status:** [![PyPI](https://img.shields.io/pypi/v/PVGeo.svg?logo=python&logoColor=white)](https://pypi.org/project/PVGeo/)
[![Build Status](https://img.shields.io/travis/OpenGeoVis/PVGeo/master.svg?label=build&logo=travis)](https://travis-ci.org/OpenGeoVis/PVGeo)
[![AppVeyor](https://ci.appveyor.com/api/projects/status/it085qovtnb0mcgr/branch/master?svg=true)](https://ci.appveyor.com/project/banesullivan/pvgeo/branch/master)


**Metrics:**
[![GitHub contributors](https://img.shields.io/github/contributors/OpenGeoVis/PVGeo.svg?logo=github&logoColor=white)](https://GitHub.com/OpenGeoVis/PVGeo/graphs/contributors/)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/4b9e8d0ef37a4f70a2d02c0d53ed096f)](https://www.codacy.com/app/banesullivan/PVGeo?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=OpenGeoVis/PVGeo&amp;utm_campaign=Badge_Grade)
[![codecov](https://codecov.io/gh/OpenGeoVis/PVGeo/branch/master/graph/badge.svg)](https://codecov.io/gh/OpenGeoVis/PVGeo/branch/master)


",2
449,PVGeo, Connections,"For a quick overview of how  *PVGeo* can be used in a Python environment or
directly within ParaView, checkout the code snippets and videos on the
[**About Examples Page**](https://pvgeo.org/about-examples.html)



",2
450,PVGeo, Getting Started,"This package provides many VTK-like algorithms designed for geoscientific data
formats and types to perform data integration and analysis.
To ensure our users have powerful and easy to use tools that can visualize the
results of PVGeo algorithms, we are actively involved in the development of
[**PyVista**](https://github.com/pyvista/pyvista): a toolset for easy access to
VTK data objects and 3D visualization in Python.
To learn more about pairing PVGeo with PyVista, please check out the
[**example Jupyter notebooks**](https://github.com/OpenGeoVis/PVGeo-Examples).


",2
451,PVGeo, Report Issues and Contribute,"To begin using the *PVGeo* Python package, create/activate your Python virtual
environment (we highly recommend using anaconda) and install *PVGeo* through pip:

```bash
pip install PVGeo
```

Now *PVGeo* is ready for use in your standard Python environment (2.7 or >=3.6)
with all dependencies installed! Go ahead and test your install:

```bash
python -c ""import PVGeo; print(PVGeo.__version__)""
```

Note that Windows users must use Python >=3.6 when outside of ParaView.
Further insight can be found in the [**Getting Started Guide**](http://pvgeo.org/overview/getting-started.html).


",2
452,PVGeo, About the Authors [![Open Source](https://img.shields.io/badge/open--source-yes-brightgreen.svg)](https://opensource.com/resources/what-open-source),"Please feel free to post features you would like to see from this package on the
[**issues page**](https://github.com/OpenGeoVis/PVGeo/issues) as a feature
request.
If you stumble across any bugs or crashes while using code distributed here,
report them in the issues section so we can promptly address it.
For other questions, join the [***PVGeo* community on Slack**](http://slack.pvgeo.org).

Interested in contributing to PVGeo? Please see the [contributing guide](https://pvgeo.org/dev-guide/contributing.html)

",2
453,PVGeo, Linking PVGeo to ParaView,"The *PVGeo* code library was created and is managed by [**Bane Sullivan**](http://banesullivan.com),
graduate student in the Hydrological Science and Engineering interdisciplinary
program at the Colorado School of Mines under Whitney Trainor-Guitton.
If you would like to contact us, inquire with [**info@pvgeo.org**](mailto:info@pvgeo.org).

It is important to note the project is open source and that many features in
this repository were made possible by contributors volunteering their time.
Head over to the [**Contributors Page**](https://github.com/OpenGeoVis/PVGeo/graphs/contributors)
to learn more about the developers of *PVGeo*.



",2
454,CU-Net, ,,1
455,CU-Net, **[Quantized Densely Connected U-Nets for Efficient Landmark Localization](https://arxiv.org/abs/1808.02194)**,,2
456,CU-Net, **[CU-Net: Coupled U-Nets](https://arxiv.org/abs/1808.06521)**,,2
457,CU-Net, Overview,,2
458,CU-Net, Prerequisites,"The follwoing figure gives an illustration of naive dense U-Net, stacked U-Nets and coupled U-Nets (CU-Net). The naive dense U-Net and stacked U-Nets have shortcut connections only inside each U-Net. In contrast, the coupled U-Nets also have connections for semantic blocks across U-Nets. The CU-Net is a hybrid of naive dense U-Net and stacked U-Net, integrating the merits of both dense connectivity, intermediate supervisions and multi-stage top-down and bottom-up refinement. The resulted CU-Net could save ~70% parameters of the previous stacked U-Nets but with comparable accuracy.
<p align=""center""><img src=""figures/framework-comparison.jpg"" alt="""" width=""600""></p>

If we couple each U-Net pair in multiple U-Nets, the coupling connections would have quadratic growth with respect to the U-Net number. To make the model more parameter efficient, we propose the order-K coupling to trim off the long-distance coupling connections.
<p align=""center""><img src=""figures/order-k.jpg"" alt="""" width=""400""></p>
For simplicity, each dot represents one U-Net. The red and blue lines are the shortcut connections of inside semantic blocks and outside inputs. Order-0 connectivity (Top) strings U-Nets together only by their inputs and outputs, i.e. stacked U-Nets. Order-1 connectivity (Middle) has shortcut connections for adjacent U-Nets. Similarly, order-2 connectivity (Bottom) has shortcut connections for 3 nearby U-Nets.

",3
459,CU-Net, Training,"This package has the following requirements:

* `Python 2.7`
* `Pytorch v0.4.0` or `Pytorch v0.1.12`

Note that the script name with string `prev-version` requires `Pytorch v0.1.12`.

",3
460,CU-Net, Validation,"```
python cu-net.py --gpu_id 0 --exp_id cu-net-2 --layer_num 2 --order 1 --loss_num 2 --is_train true --bs 24
```

",3
461,CU-Net, Model Options,"```
python cu-net.py --gpu_id 0 --exp_id cu-net-2 --layer_num 2 --order 1 --loss_num 2 --resume_prefix your_pretrained_model.pth.tar --is_train false --bs 24
```

",3
462,CU-Net, Project Page,"```
layer_num     #notes: number of coupled U-Nets
order         #notes: the order of coupling
loss_num      #notes: number of losses. Losses are uniformly distributed along the CU-Net. Each U-Net at most has one loss. (loss_num <= layer_num)
```

",3
463,CU-Net, Citation,"For more details, please refer to our **[project page](https://sites.google.com/site/xipengcshomepage/research/eccv18)**.

",2
464,cltk-cltk, The Classical Language Toolkit,,1
465,cltk-cltk, About,"[![PyPi downloads](http://img.shields.io/pypi/v/cltk.svg?style=flat)](https://pypi.python.org/pypi/cltk/) [![Documentation Status](https://readthedocs.org/projects/cltk/badge/?version=latest)](http://docs.cltk.org/en/latest/?badge=latest) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.593336.svg)](https://doi.org/10.5281/zenodo.593336)

[![Build Status](https://travis-ci.org/cltk/cltk.svg?branch=master)](https://travis-ci.org/cltk/cltk) [![codecov.io](http://codecov.io/github/cltk/cltk/coverage.svg?branch=master)](http://codecov.io/github/cltk/cltk?branch=master)

[![Join the chat at https://gitter.im/cltk/cltk](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/cltk/cltk?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)


",2
466,cltk-cltk, Documentation,"The Classical Language Toolkit (CLTK) offers natural language processing (NLP) support for the languages of Ancient, Classical, and Medieval Eurasia. Greek, Latin, Akkadian, and the Germanic languages are currently most complete. The goals of the CLTK are to:
*   compile analysis-friendly corpora;
*   collect and generate linguistic data;
*   act as a free and open platform for generating scientific research.


",2
467,cltk-cltk, Installation,"The docs are at [docs.cltk.org](http://docs.cltk.org).


",3
468,cltk-cltk, Tutorials,"CLTK supports Python versions 3.6 and 3.7. The software only runs on POSIX–compliant operating systems (Linux, Mac OS X, FreeBSD, etc.).

``` bash
$ pip install cltk
```

See docs for [complete installation instructions](http://docs.cltk.org/en/latest/installation.html).

The [CLTK organization curates corpora](https://github.com/cltk) which can be downloaded directly or, better, [imported by the toolkit](http://docs.cltk.org/en/latest/importing_corpora.html).


",3
469,cltk-cltk, Contributing,"For interactive tutorials, in the form of Jupyter Notebooks, see <https://github.com/cltk/tutorials>.


",2
470,cltk-cltk, Citation,"See the [Quickstart for contributors](https://github.com/cltk/cltk/wiki/Quickstart-for-contributors) for an overview of the process. If you're looking to start with a small contribution, see the [Issue tracker for ""easy"" jobs](https://github.com/cltk/cltk/issues?q=is%3Aopen+is%3Aissue+label%3Aeasy) needing to be done. Bigger projects may be found at [Project ideas](https://github.com/cltk/cltk/wiki/Project-ideas) page. Of course, new ideas are always welcome.


",2
471,cltk-cltk, Gratitude,"Each major release of the CLTK is given a [DOI](http://en.wikipedia.org/wiki/Digital_object_identifier), a type of unique identity for digital documents. This DOI ought to be included in your citation, as it will allow researchers to reproduce your results should the CLTK's API or codebase change. To find the CLTK's current DOI, observe the blue `DOI` button in the repository's home on GitHub. To the end of your bibliographic entry, append `DOI ` plus the current identifier. You may also add version/release number, located in the `pypi` button at the project's GitHub repository homepage.

Thus, please cite core software as something like:
```
Kyle P. Johnson et al.. (2014-2019). CLTK: The Classical Language Toolkit. DOI 10.5281/zenodo.<current_release_id>
```

A style-neutral BibTeX entry would look like this:
```
@Misc{johnson2014,
author = {Kyle P. Johnson et al.},
title = {CLTK: The Classical Language Toolkit},
howpublished = {\url{https://github.com/cltk/cltk}},
note = {{DOI} 10.5281/zenodo.<current_release_id>},
year = {2014--2019},
}
```


[Many contributors](https://github.com/cltk/cltk/blob/master/contributors.md) have made substantial contributions to the CLTK. For scholarship about particular code, it might be proper to cite these individuals as authors of the work under discussion.


",2
472,cltk-cltk, License,"We are thankful for the following organizations that have offered support:

* Google Summer of Code (sponsoring two students, 2016, 2017; three students 2018)
* JetBrains (licenses for PyCharm)
* Google Cloud Platform (with credits for the Classical Language Archive and API)


",2
473,ICNet, ICNet for Real-Time Semantic Segmentation on High-Resolution Images,,2
474,ICNet, Introduction,"by Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, Jiaya Jia, details are in [project page](https://hszhao.github.io/projects/icnet).

",3
475,ICNet, Usage,"Based on [PSPNet](https://github.com/hszhao/PSPNet), this repository is build for evaluation in ICNet. For installation, please follow the description in PSPNet repository (support CUDA 7.0/7.5 + cuDNN v4).

",3
476,ICNet, Citation,"1. Clone the repository recursively:

   ```shell
   git clone --recursive https://github.com/hszhao/ICNet.git
   ```

2. Build Caffe and matcaffe:

   ```shell
   cd $ICNET_ROOT/PSPNet
   cp Makefile.config.example Makefile.config
   vim Makefile.config
   make -j8 && make matcaffe
   cd ..
   ```

3. Evaluation mIoU:

   - Evaluation code is in folder 'evaluation'.
   - Download trained models and put them in folder 'evaluation/model':
     - icnet_cityscapes_train_30k.caffemodel: [GoogleDrive](https://drive.google.com/open?id=0BzaU285cX7TCRXpXMnVIbXdfaW8) 

       (31M, md5: c7038630c4b6c869afaaadd811bdb539; train on trainset for 30k)

     - icnet_cityscapes_trainval_90k.caffemodel: [GoogleDrive](https://drive.google.com/open?id=0BzaU285cX7TCTFVpZWJINi1Iblk) 

       (31M, md5: 4f4dd9eecd465dd8de7e4cf88ba5d5d5; train on trainvalset for 90k)
   - Modify the related paths in 'eval_all.m':
     - Mainly variables 'data_root' and 'eval_list', and your image list for evaluation should be similar to that in folder 'evaluation/samplelist' if you use this evaluation code structure. 

   ```shell
   cd evaluation
   vim eval_all.m
   ```

   - Run the evaluation scripts:

   ```
   ./run.sh
   ```

4. Evaluation time:

   - To get inference time as accurate as possible, it's suggested to make sure the GPU card with specified ID in script 'test_time.sh' is empty (without other processes executing)

   - Run the evaluation scripts:

   ```
   ./test_time.sh
   ```

5. Results: 

   - Prediction results will show in folder 'evaluation/mc_result' and the expected scores are:
     - ICNet train on trainset for 30K, evaluated on valset (mIoU/pAcc): 67.7/94.5
     - ICNet train on trainvalset for 90K, evaluated on testset (mIoU): 69.5
   - Log information of inference time will be in file 'time.log', approximately 33~36ms on TitanX.

6. Demo video:

   - Video processed by ICNet on cityscapes dataset:
     - Alpha blending with value as 0.5: [Video](https://youtu.be/qWl9idsCuLQ)

",2
477,ICNet, Questions,"If ICNet is useful for your research, please consider citing:

    @article{zhao2017icnet,
      author = {Hengshuang Zhao and
                Xiaojuan Qi and
                Xiaoyong Shen and
                Jianping Shi and
                Jiaya Jia},
      title = {ICNet for Real-Time Semantic Segmentation on High-Resolution Images},
      journal={arXiv preprint arXiv:1704.08545},
      year = {2017}
    }
",3
478,tetgen, get cell centroids,"======

.. image:: https://travis-ci.org/pyvista/tetgen.svg?branch=master
    :target: https://travis-ci.org/pyvista/tetgen

.. image:: https://img.shields.io/pypi/v/tetgen.svg?logo=python&logoColor=white
   :target: https://pypi.org/project/tetgen/

This Python module is an interface to Hang Si's
`TetGen <https://github.com/ufz/tetgen>`__ C++ software.
This module combines speed of C++ with the portability and ease of installation
of Python along with integration to `PyVista <https://docs.pyvista.org>`_ for
3D visualization and analysis.
See the `TetGen <https://github.com/ufz/tetgen>`__ GitHub page for more details
on the original creator.

The last update to the original C++ software was on 19 January 2011, but the
software remains relevant today.  Brief description from
`Weierstrass Institute Software <http://wias-berlin.de/software/index.jsp?id=TetGen&lang=1>`__:

    TetGen is a program to generate tetrahedral meshes of any 3D polyhedral domains.
    TetGen generates exact constrained Delaunay tetrahedralization, boundary
    conforming Delaunay meshes, and Voronoi partitions.

    TetGen provides various features to generate good quality and adaptive
    tetrahedral meshes suitable for numerical methods, such as finite element or
    finite volume methods. For more information of TetGen, please take a look at a
    list of `features <http://wias-berlin.de/software/tetgen/features.html>`__.


Installation
------------

From `PyPI <https://pypi.python.org/pypi/tetgen>`__

.. code:: bash

    pip install tetgen

From source at `GitHub <https://github.com/pyvista/tetgen>`__

.. code:: bash

    git clone https://github.com/pyvista/tetgen
    cd tetgen
    pip install .


Basic Example
-------------

The features of the C++ TetGen software implemented in this module are primarily
focused on the tetrahedralization a manifold triangular surface.  This basic
example demonstrates how to tetrahedralize a manifold surface and plot part of
the mesh.

.. code:: python

    import pyvista as pv
    import tetgen
    import numpy as np
    pv.set_plot_theme('document')

    sphere = pv.Sphere()
    tet = tetgen.TetGen(sphere)
    tet.tetrahedralize(order=1, mindihedral=20, minratio=1.5)
    grid = tet.grid
    grid.plot(show_edges=True)

.. figure:: https://github.com/pyvista/tetgen/raw/master/docs/images/sphere.png
    :width: 300pt

    Tetrahedralized Sphere

Extract a portion of the sphere's tetrahedral mesh below the xy plane and plot
the mesh quality.

.. code:: python

    ",1
479,tetgen, extract cells below the 0 xy plane,"    cells = grid.cells.reshape(-1, 5)[:, 1:]
    cell_center = grid.points[cells].mean(1)

    ",1
480,tetgen, advanced plotting,"    mask = cell_center[:, 2] < 0
    cell_ind = mask.nonzero()[0]
    subgrid = grid.extract_cells(cell_ind)

    ",1
481,tetgen, plot quality,"    plotter = pv.Plotter()
    plotter.add_mesh(subgrid, 'lightgrey', lighting=True, show_edges=True)
    plotter.add_mesh(sphere, 'r', 'wireframe')
    plotter.add_legend([[' Input Mesh ', 'r'],
                        [' Tesselated Mesh ', 'black']])
    plotter.show()

.. image:: https://github.com/pyvista/tetgen/raw/master/docs/images/sphere_subgrid.png

Cell quality scalars can be obtained and plotted with:

.. code:: python

    cell_qual = subgrid.quality

    ",1
482,geojson-vt, geojson-vt &mdash; GeoJSON Vector Tiles,,2
483,geojson-vt, Demo,"[![Build Status](https://travis-ci.org/mapbox/geojson-vt.svg?branch=master)](https://travis-ci.org/mapbox/geojson-vt)

A highly efficient JavaScript library for **slicing GeoJSON data into vector tiles on the fly**,
primarily designed to enable rendering and interacting with large geospatial datasets
on the browser side (without a server).

Created to power GeoJSON in [Mapbox GL JS](https://github.com/mapbox/mapbox-gl-js),
but can be useful in other visualization platforms
like [Leaflet](https://github.com/Leaflet/Leaflet) and [d3](https://github.com/mbostock/d3),
as well as Node.js server applications.

Resulting tiles conform to the JSON equivalent
of the [vector tile specification](https://github.com/mapbox/vector-tile-spec/).
To make data rendering and interaction fast, the tiles are simplified,
retaining the minimum level of detail appropriate for each zoom level
(simplifying shapes, filtering out tiny polygons and polylines).

Read more on how the library works [on the Mapbox blog](https://www.mapbox.com/blog/introducing-geojson-vt/).

There's a C++11 port: [geojson-vt-cpp](https://github.com/mapbox/geojson-vt-cpp)

",3
484,geojson-vt, Usage,"Here's **geojson-vt** action in [Mapbox GL JS](https://github.com/mapbox/mapbox-gl-js),
dynamically loading a 100Mb US zip codes GeoJSON with 5.4 million points:

![](https://cloud.githubusercontent.com/assets/25395/5360312/86028d8e-7f91-11e4-811f-87f24acb09ca.gif)

There's a convenient [debug page](http://mapbox.github.io/geojson-vt/debug/) to test out **geojson-vt** on different data.
Just drag any GeoJSON on the page, watching the console.

![](https://cloud.githubusercontent.com/assets/25395/5363235/41955c6e-7fa8-11e4-9575-a66ef54cb6d9.gif)

",3
485,geojson-vt, Options,"```js
// build an initial index of tiles
var tileIndex = geojsonvt(geoJSON);

// request a particular tile
var features = tileIndex.getTile(z, x, y).features;

// show an array of tile coordinates created so far
console.log(tileIndex.tileCoords); // [{z: 0, x: 0, y: 0}, ...]
```

",3
486,geojson-vt, Install,"You can fine-tune the results with an options object,
although the defaults are sensible and work well for most use cases.

```js
var tileIndex = geojsonvt(data, {
	maxZoom: 14,  // max zoom to preserve detail on; can't be higher than 24
	tolerance: 3, // simplification tolerance (higher means simpler)
	extent: 4096, // tile extent (both width and height)
	buffer: 64,   // tile buffer on each side
	debug: 0,     // logging level (0 to disable, 1 or 2)
	lineMetrics: false, // whether to enable line metrics tracking for LineString/MultiLineString features
	promoteId: null,    // name of a feature property to promote to feature.id. Cannot be used with `generateId`
	generateId: false,  // whether to generate feature ids. Cannot be used with `promoteId`
	indexMaxZoom: 5,       // max zoom in the initial tile index
	indexMaxPoints: 100000 // max number of points per tile in the index
});
```

By default, tiles at zoom levels above `indexMaxZoom` are generated on the fly, but you can pre-generate all possible tiles for `data` by setting `indexMaxZoom` and `maxZoom` to the same value, setting `indexMaxPoints` to `0`, and then accessing the resulting tile coordinates from the `tileCoords` property of `tileIndex`.

The `promoteId` and `generateId` options ignore existing `id` values on the feature objects.

GeoJSON-VT only operates on zoom levels up to 24.

",3
487,DaSiamRPN, DaSiamRPN,":trophy:News: **We won the second place in the VOT-18 long-term challenge**

",1
488,DaSiamRPN, Introduction,"This repository includes PyTorch code for reproducing the results on VOT2018.

[**Distractor-aware Siamese Networks for Visual Object Tracking**](https://arxiv.org/pdf/1808.06048.pdf)  

Zheng Zhu<sup>\*</sup>, Qiang Wang<sup>\*</sup>, Bo Li<sup>\*</sup>, Wei Wu, Junjie Yan, and Weiming Hu 

*European Conference on Computer Vision (ECCV), 2018*



",2
489,DaSiamRPN, Prerequisites,"**SiamRPN** formulates the task of visual tracking as a task of localization and identification simultaneously, initially described in an [CVPR2018 spotlight paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf). (Slides at [CVPR 2018 Spotlight](https://drive.google.com/open?id=1OGIOUqANvYfZjRoQfpiDqhPQtOvPCpdq))

**DaSiamRPN** improves the performances of SiamRPN by (1) introducing an effective sampling strategy to control the imbalanced sample distribution, (2) designing a novel distractor-aware module to perform incremental learning, (3) making a long-term tracking extension. [ECCV2018](https://arxiv.org/pdf/1808.06048.pdf). (Slides at [VOT-18 Real-time challenge winners talk](https://drive.google.com/open?id=1dsEI2uYHDfELK0CW2xgv7R4QdCs6lwfr))

<div align=""center"">
  <img src=""votresult.png"" width=""700px"" />
</div>

",2
490,DaSiamRPN, Pretrained model for SiamRPN,"CPU: Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz
GPU: NVIDIA GTX1060

- python2.7
- pytorch == 0.3.1
- numpy
- opencv


",2
491,DaSiamRPN, Detailed steps to install the prerequisites,"In our tracker, we use an AlexNet variant as our backbone, which is end-to-end trained for visual tracking.
The pretrained model can be downloaded from google drive: [SiamRPNBIG.model](https://drive.google.com/file/d/1-vNVZxfbIplXHrqMHiJJYWXYWsOIvGsf/view?usp=sharing).
Then, you should copy the pretrained model file `SiamRPNBIG.model` to the subfolder './code', so that the tracker can find and load the pretrained_model.


",2
492,DaSiamRPN, Results,"- install pytorch, numpy, opencv following the instructions in the `run_install.sh`. Please do **not** use conda to install.
- you can alternatively modify `/PATH/TO/CODE/FOLDER/` in `tracker_SiamRPN.m` 
  If the tracker is ready, you will see the tracking results. (EAO: 0.3827)


",2
493,DaSiamRPN, Demo and Test on OTB2015,"All results can be downloaded from [Google Drive](https://drive.google.com/drive/folders/1HJOvl_irX3KFbtfj88_FVLtukMI1GTCR?usp=sharing).

| | <sub>VOT2015</br>A / R / EAO</sub> | <sub>VOT2016</br>A / R / EAO</sub> | <sub>VOT2017 & VOT2018</br>A / R / EAO</sub> | <sub>OTB2015</br>OP / DP</sub> | <sub>UAV123</br>AUC / DP</sub> | <sub>UAV20L</br>AUC / DP</sub> |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| <sub> **SiamRPN** </br> CVPR2017 </sub> | <sub>0.58 / 1.13 / 0.349<sub> | <sub>0.56 / 0.26 / 0.344<sub> | <sub>0.49 / 0.46 / 0.244<sub> | <sub>81.9 / 85.0<sub> | <sub>0.527 / 0.748<sub> | <sub>0.454 / 0.617<sub> |
| <sub> **DaSiamRPN** </br> ECCV2018 </sub> | <sub>**0.63** / **0.66** / **0.446**<sub> | <sub>**0.61** / **0.22** / **0.411**<sub> | <sub>0.56 / 0.34 / 0.326<sub> | <sub>**86.5** / **88.0**<sub> | <sub>**0.586** / **0.796**<sub> | <sub>**0.617** / **0.838**<sub> |
| <sub> **DaSiamRPN** </br> VOT2018 </sub> | <sub>-<sub> | <sub>-<sub>  | <sub>**0.59** / **0.28** / **0.383**<sub> | <sub>-<sub> | <sub>-<sub> | <sub>-<sub> |


",1
494,DaSiamRPN, License,"<div align=""center"">
  <img src=""code/data/bag.gif"" width=""400px"" />
</div>

- To reproduce the reuslts on paper, the pretrained model can be downloaded from [Google Drive](https://drive.google.com/open?id=1BtIkp5pB6aqePQGlMb2_Z7bfPy6XEj6H): `SiamRPNOTB.model`. <br />
:zap: :zap: This model is the **fastest** (~200fps) Siamese Tracker with AUC of 0.655 on OTB2015. :zap: :zap: 

- You must download OTB2015 dataset (download [script](code/data/get_otb_data.sh)) at first.

A simple test example.

```
cd code
python demo.py
```

If you want to test the performance on OTB2015, please using the follwing command.

```
cd code
python test_otb.py
python eval_otb.py OTB2015 ""Siam*"" 0 1
```


",1
495,DaSiamRPN, Citing DaSiamRPN,"Licensed under an MIT license.


",2
496,scikit-image-scikit-image, scikit-image: Image processing in Python,,1
497,scikit-image-scikit-image, Installation from binaries,"[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftags%2Fscikit-image.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tags/scikit-image)
[![Stackoverflow](https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg)](https://stackoverflow.com/questions/tagged/scikit-image)
[![project chat](https://img.shields.io/badge/zulip-join_chat-brightgreen.svg)](https://skimage.zulipchat.com)
[![codecov.io](https://codecov.io/github/scikit-image/scikit-image/coverage.svg?branch=master)](https://codecov.io/github/scikit-image/scikit-image?branch=master)

- **Website (including documentation):** [https://scikit-image.org/](https://scikit-image.org)
- **Mailing list:** [https://mail.python.org/mailman/listinfo/scikit-image](https://mail.python.org/mailman/listinfo/scikit-image)
- **Source:** [https://github.com/scikit-image/scikit-image](https://github.com/scikit-image/scikit-image)
- **Benchmarks:** [https://pandas.pydata.org/speed/scikit-image/](https://pandas.pydata.org/speed/scikit-image/)

",2
498,scikit-image-scikit-image, Installation from source,"- **Debian/Ubuntu:** ``sudo apt-get install python-skimage``
- **OSX:** ``pip install scikit-image``
- **Anaconda:** ``conda install -c conda-forge scikit-image``
- **Windows:** Download [Windows binaries](http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-image)

Also see [installing ``scikit-image``](INSTALL.rst).

",2
499,scikit-image-scikit-image, License (Modified BSD),"Install dependencies using:

```
pip install -r requirements.txt
```

Then, install scikit-image using:

```
$ pip install .
```

If you plan to develop the package, you may run it directly from source:

```
$ pip install -e .  #notes: Do this once to add package to Python path
```

Every time you modify Cython files, also run:

```
$ python setup.py build_ext -i  #notes: Build binary extensions
```

",2
500,scikit-image-scikit-image, Citation,"Copyright (C) 2011, the scikit-image team
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

 1. Redistributions of source code must retain the above copyright
    notice, this list of conditions and the following disclaimer.
 2. Redistributions in binary form must reproduce the above copyright
    notice, this list of conditions and the following disclaimer in
    the documentation and/or other materials provided with the
    distribution.
 3. Neither the name of skimage nor the names of its contributors may be
    used to endorse or promote products derived from this software without
    specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.

",2
501,geonotebook, GeoNotebook [![CircleCI](https://circleci.com/gh/OpenGeoscience/geonotebook.svg?style=shield)](https://circleci.com/gh/OpenGeoscience/geonotebook) [![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/OpenGeoscience/geonotebook),,2
502,geonotebook, Screenshots,"GeoNotebook is an application that provides client/server
environment with interactive visualization and analysis capabilities
using [Jupyter](http://jupyter.org), [GeoJS](http://www.github.com/OpenGeoscience/geojs) and other open source tools.
Jointly developed by  [Kitware](http://www.kitware.com) and
[NASA Ames](https://www.nasa.gov/centers/ames/home/index.html).

Documentation for GeoNotebook can be found at http://geonotebook.readthedocs.io.

",2
503,geonotebook, Installation,"![screen shot](https://data.kitware.com/api/v1/file/5898b1788d777f07219fcafb/download?contentDisposition=inline)

Checkout some additional [screenshots](screenshots/)


",2
504,geonotebook, System Prerequisites,,3
505,geonotebook, Clone the repo:,"For default tile serving
  + GDAL >= 2.1.0
  + mapnik >= 3.1.0
  + python-mapnik >= 0.1

",3
506,geonotebook," Make a virtualenv, install jupyter[notebook], install geonotebook","```bash
git clone https://github.com/OpenGeoscience/geonotebook.git
cd geonotebook
```
",3
507,geonotebook, Installing geonotebook for development,"```bash
mkvirtualenv -a . geonotebook

#notes: Numpy must be fully installed before rasterio
pip install -r prerequirements.txt

pip install -r requirements.txt

pip install .

#notes: Enable both the notebook and server extensions
jupyter serverextension enable --sys-prefix --py geonotebook
jupyter nbextension enable --sys-prefix --py geonotebook
```

*Note* The `serverextension` and `nbextension` commands accept flags that configure how
and where the extensions are installed.  See `jupyter serverextension --help` for more
information.

",3
508,geonotebook, Run the notebook:,"When developing geonotebook, it is often helpful to install packages as a reference to the
checked out repository rather than copying them to the system `site-packages`.  A ""development
install"" will allow you to make live changes to python or javascript without reinstalling the
package.
```bash
#notes: Install the geonotebook python package as ""editable""
pip install -e .

#notes: Install the notebook extension as a symlink
jupyter nbextension install --sys-prefix --symlink --py geonotebook

#notes: Enable the extension
jupyter serverextension enable --sys-prefix --py geonotebook
jupyter nbextension enable --sys-prefix --py geonotebook

#notes: Start the javascript builder
cd js
npm run watch
```

",3
509,geonotebook, Configure the notebook:,"```bash
cd notebooks/
jupyter notebook
```

",3
510,geonotebook, Run the tests,"Geonotebook relies on a configuration for several of its options. The system will merge configuration files in the following precedence:

+ /etc/geonotebook.ini
+ /usr/etc/geonotebook.ini
+ /usr/local/etc/geonotebook.ini
+ ```sys.prefix```/etc/geonotebook.ini 
  (e.g. /home/user/.virtual_environments/geonotebook/etc/geonotebook.inig)
+ ~/.geonotebook.ini
+ ```os.getcwd()```/.geonotebook.ini
+ any path specified in the ```GEONOTEBOOK_INI``` environment variable.

The [default configuration](config/geonotebook.ini) is installed in ```sys.prefix```/etc/geonotebook.ini


",3
511,geonotebook, Docker Container,"```bash
#notes: From the source root
pip install -r requirements-dev.txt
tox

#notes: Optionally only run tests on python 2.7
#notes: tox -e py27
```

",2
512,geonotebook, Vagrant Machine,"System requirements for running the notebook can sometimes prove burdensome to install. To ease these issues we have included a [docker container](devops/docker) that will run the notebook inside a containerized process. 

",2
513,geonotebook, Tile Server,"Additionally there is a `Vagrantfile` for standing up an instance of Geonotebook within a virtual machine, further instructions can be found [here](Vagrant.md).

",2
514,geonotebook, Use geoserver for tile serving,"By default geonotebook provides its own tile server based on [Mapnik](https://github.com/mapnik) and [GDAL](http://www.gdal.org/) as a Jupyter Notebook server extension. Assuming system pre-requisites are available this should not need to be configured. Alternately geonotebook may be configured to use a pre-existing [Geoserver](http://geoserver.org/) for serving tiles. A built in geoserver implementation is available as a virtual machine in devops/geoserver/.  

",3
515,generator-arcgis-js-app, generator-arcgis-js-app,"[![Dependencies Status](https://david-dm.org/odoe/generator-arcgis-js-app.svg)](https://david-dm.org/odoe/generator-arcgis-js-app)
",1
516,generator-arcgis-js-app, Getting Started,"> [Yeoman](http://yeoman.io) generator

This is a yeoman generator for [ArcGIS API for JavaScript applications](https://developers.arcgis.com/javascript/).

",2
517,generator-arcgis-js-app, What is Yeoman?,,3
518,generator-arcgis-js-app, Yeoman Generators,"Trick question. It's not a thing. It's this guy:

![](http://i.imgur.com/JHaAlBJ.png)

Basically, he wears a top hat, lives in your computer, and waits for you to tell him what kind of application you wish to create.

Not every new computer comes with a Yeoman pre-installed. He lives in the [npm](https://npmjs.org) package repository. You only have to ask for him once, then he packs up and moves into your hard drive. *Make sure you clean up, he likes new and shiny things.*

```bash
npm install -g yo
npm install -g bower
```

Bower is a required dependency of using the packages in the generated app.

",3
519,generator-arcgis-js-app, What is used?,"Yeoman travels light. He didn't pack any generators when he moved in. You can think of a generator like a plug-in. You get to choose what type of application you wish to create, such as a Backbone application or even a Chrome extension.

To install generator-arcgis-js-app from npm, run:

```bash
npm install -g generator-arcgis-js-app
```

Finally, initiate the generator inside application folder:

```bash
yo arcgis-js-app
```

or

```bash
yo arcgis-js-app application-name
```

You will be asked:
* Application name if not provided
* Description of application
* ArcGIS API Version (3.x or 4.x)
* Use Stylus or Sass
* Email to be used in package information

Will create component and tests. Updates `intern.js` with test suite.


",3
520,generator-arcgis-js-app, Usage,"* New use 3.x or 4.x of the [ArcGIS API for JavaScript](https://developers.arcgis.com/javascript/)
* Output application uses [GruntJS](http://gruntjs.com/) for running tasks
* All code is written in [ES6/ES2015](https://github.com/lukehoban/es6features) and transpiled with [babel](https://babeljs.io/)
* Uses [eslint](https://github.com/eslint/eslint) to lint code
* Uses [stylus](https://learnboost.github.io/stylus/) or [sass](http://sass-lang.com/) as a css preprocessor
* Uses [livereload](http://livereload.com/) for easier development workflow

",3
521,generator-arcgis-js-app, Still a beta,"`grunt` - default task, will output code to a `dist` folder with sourcemaps.

`grunt dev` - will start a local server on at `http://localhost:8282/` and watch for changes. Uses livereload to refresh browser with each update.

`http://localhost:8282/dist/` - application

`http://localhost:8282/node_modules/intern/client.html?config=tests/intern` - test suites

`grunt build` - build the application and output to a `release` folder.

`grunt e2e` - runs all tests using local [chromedriver](https://sites.google.com/a/chromium.org/chromedriver/).


",3
522,generator-arcgis-js-app, Notes,"* Considering implementing a [widgetloader](https://github.com/odoe/esri-widgetloader)
* Needs ability to inject code into [Application.js](generators/app/templates/src/app/templates/Appication.js)
* Guide on application architecture

",3
523,generator-arcgis-js-app, Getting To Know Yeoman,"Uses [theintern.io](https://theintern.github.io/) for testing.

It is recommended that you use NPM 3.x to install dependencies, as this [will reduce the time it takes for Babel to transpile ES2015 code](https://phabricator.babeljs.io/T6756#67810).

",3
524,generator-arcgis-js-app, License,"Yeoman has a heart of gold. He's a person with feelings and opinions, but he's very easy to work with. If you think he's too opinionated, he can be easily convinced.

If you'd like to get to know Yeoman better and meet some of his friends, [Grunt](http://gruntjs.com) and [Bower](http://bower.io), check out the complete [Getting Started Guide](https://github.com/yeoman/yeoman/wiki/Getting-Started).


",2
525,tilelive-mapnik, tilelive-mapnik,,1
526,tilelive-mapnik, Installation,"Renderer backend for [tilelive.js](http://github.com/mapbox/tilelive.js) that
uses [node-mapnik](http://github.com/mapnik/node-mapnik) to render tiles and
grids from a Mapnik XML file. `tilelive-mapnik` implements the
[Tilesource API](https://github.com/mapbox/tilelive.js/blob/master/API.md).

[![Build Status](https://secure.travis-ci.org/mapbox/tilelive-mapnik.png)](http://travis-ci.org/mapbox/tilelive-mapnik)
[![Build status](https://ci.appveyor.com/api/projects/status/6am7la0hiaei8qop)](https://ci.appveyor.com/project/Mapbox/tilelive-mapnik)

",2
527,tilelive-mapnik, Usage,"    npm install tilelive-mapnik

Though `tilelive` is not a dependency of `tilelive-mapnik` you will want to
install it to actually make use of `tilelive-mapnik` through a reasonable
API.


",2
528,map-vectorizer, An NYPL Labs project,"==============
",4
529,map-vectorizer, Like OCR for maps,"**Author:** Mauricio Giraldo Arteaga [@mgiraldo] / NYPL Labs [@nypl_labs]

Additional contributor: Thomas Levine [@thomaslevine]

A open-source map vectorizer. Provided **as is** by [NYPL Labs](http://www.nypl.org/collections/labs). Project based on a workflow suggested by Michael Resig.

The output of this process can be verified by volunteers with the [Building Inspector](https://github.com/nypl/building-inspector). 

[A paper](map-poly-paper.pdf) on this process was [published in the MapInteract '13 Proceedings of the 1st ACM SIGSPATIAL International Workshop on MapInteraction](http://dl.acm.org/citation.cfm?id=2534932&CFID=966849878&CFTOKEN=93984154) where it won the Best Paper Award.

",2
530,map-vectorizer, Example input,"This project aims to automate the manual process of geographic polygon and attribute data extraction from maps (i.e. georectified images) including those from [insurance atlases](http://digitalcollections.nypl.org/search/index?filters%5Btitle_uuid_s%5D%5B%5D=Maps%20of%20the%20city%20of%20New%20York%7C%7C323e4180-c603-012f-0c9f-58d385a7bc34&keywords=&layout=false#/?scroll=24) published in the 19th and early 20th centuries. [Here is some background](http://www.nypl.org/blog/2012/06/13/nyc-historical-gis-project) on why we're doing this and [here is one of the maps](http://digitalcollections.nypl.org/items/510d47e0-c7cc-a3d9-e040-e00a18064a99) we're extracting polygons from. This [example map layer](http://maps.nypl.org/warper/layers/859) shows what these atlases look like once geo-rectified, i.e. geographically normalized.

[The New York Public Library](http://www.nypl.org) has **hundreds of atlases** with **tens of thousands** of these sheets and there is no way we can extract data manually in a reasonable amount of time.

Just so you get an idea, it took NYPL staff coordinating a small army of volunteers **three years to produce** 170,000 polygons with attributes (from just four of hundreds of atlases at NYPL).

It now takes a period of time closer to **24 hours** to generate a comparable number of polygons with some basic metadata.

The goal is to extract the following data (✔ = mostly solved so far, ✢ = in progress):

* ✔ shape
* ✔ color
* ✢ dot presence
* ✢ dot count
* ✢ dot type (full vs outline)
* skylights
* numbers (not optimistic, but maybe **one of you** knows how extract numbers from these images)

",4
531,map-vectorizer, Example output,"![Example input map](https://raw.github.com/NYPL/map-vectorizer/master/example_input.png)

",4
532,map-vectorizer, Extra feature detection,"![The resulting shapefile output superimposed](https://raw.github.com/NYPL/map-vectorizer/master/example_output.png)

",4
533,map-vectorizer, Dependencies,"![Extra feature detection for the polygon](https://raw.github.com/NYPL/map-vectorizer/master/feature_detection.png)

",2
534,map-vectorizer, First run,"A few things to be installed in your system in order to work properly. So far it has been **tested on Mac OS X Lion** so these instructions apply to that configuration only. I am sure you will be able to adapt it to your current configuration.

* [Python] with [OpenCV] and [PIL] 
    * If you use [PIP](https://pypi.python.org/pypi) (recommended) you will get the necessary Python packages with: `pip install -r requirements.txt`
* [R] - Make sure it is in your PATH (so you can run it via command-line by typing `R`).
* You'll need the following R packages. On OS X simply navigate to `Packages & Data`, choose your CRAN mirror region, then search for and install:
    * `alphahull` (you will need `tripack`, `sgeostat`, `splancs` as dependencies)
    * `igraph`
    * `shapefiles`
    * `rgdal` (download the [binary for your OS](http://cran.r-project.org/web/packages/rgdal/index.html) then run `R CMD INSTALL --configure-args="""" path/to/rgdal.tar.gz`)
    * You can also install the requirements by running this in the R CLI (by typing `R` in a terminal window):

```
    install.packages('rgdal')
    install.packages('alphahull')
    install.packages('igraph')
    install.packages('shapefiles')
```

* Test that everything in R is installed, on the CLI you should be able to run this with no errors:

```
    library(rgdal)
    library(alphahull)
    library(igraph)
    library(shapefiles)
    quit() #notes: this will quit R
```

* [GIMP]
* [GDAL Tools], on OS X try [version 1.9](http://www.kyngchaos.com/files/software/frameworks/GDAL_Complete-1.9.dmg). Per [MapBox](https://www.mapbox.com/tilemill/docs/guides/gdal/): The first time you install the GDAL package there is one additional step to make sure you can access these programs. In Mac OS, Open the Terminal application and run the following commands:

```
    echo 'export PATH=/Library/Frameworks/GDAL.framework/Programs:$PATH' >> ~/.bash_profile
    source ~/.bash_profile
```

* It is also a good idea to install [QGIS] to test your results

",2
535,map-vectorizer, Configuring,"These step by step instructions should work as-is. If not, **check all the above are working** before submitting an issue.

1. Take note of the path where the GIMP executable is installed (the default value in the vectorizer is the Mac OS location: `/Applications/Gimp.app/Contents/MacOS/gimp-2.8`).
2. Run the script on the provided test GeoTIFF:
`python vectorize_map.py test.tif`
3. Accept the GIMP folder location or input a different one and press ENTER.

**NOTE:** The vectorizer has problems with *filenames that contain spaces*. This will be supported eventually.

This should take about 70 seconds to process. **If it takes less there might be an error** (or your machine rulez). Take a look at the console output to find the possible culprit.

If it works, you will see a `test` folder with a `test-traced` set of files (`.shp`, `.dbf`, `.prj` and `.shx`) and two log files.

",2
536,map-vectorizer, Required argument,"`vectorize_map.py` supports a few configuration options.

```bash
usage: vectorize_map.py [-h] --gimp-path GIMP_PATH [--chunksize CHUNKSIZE]
                        [--image-processing-configuration-file VECTORIZE_CONFIG]
                        <input file or dir>
```

",3
537,map-vectorizer, Semi-optional arguments,"- `<input file or dir>` path to file (or folder with files) to vectorize

",3
538,map-vectorizer, Optional arguments,"This are really required because 1) you won't have GIMP installed in the same folder as us and 2) your maps look different from the test map and you will want to have a config file.

- `--gimp-path GIMP_PATH` path to GIMP executable (defaults to `/Applications/Gimp.app/Contents/MacOS/gimp-2.8`)
- `--image-processing-configuration-file VECTORIZE_CONFIG, -p VECTORIZE_CONFIG` path to map image processing configuration file (defaults to `vectorize_config_default.txt`)

",3
539,map-vectorizer, Customizing The Vectorizer to your own maps,"- `-h, --help` show help message and exit
- `--chunksize CHUNKSIZE` (ignore this but determines how to split temp file… but really, ignore it)

",2
540,map-vectorizer, Templates and other files,"The Vectorizer was made to work with the [NYPL map atlases](http://digitalcollections.nypl.org/search/index?filters%5Btitle_uuid_s%5D%5B%5D=Maps%20of%20the%20city%20of%20New%20York%7C%7C323e4180-c603-012f-0c9f-58d385a7bc34&keywords=&layout=false#/?scroll=24). It is likely that your maps have different quality and colors. In order for this to work in your maps, you first need to do some minor config adjustments to generate a proper threshold file for your set (assuming it is a map set similar to the provided example `test.tif`):

1. Your map needs to be in **WGS84 projection**. Other projections might be supported in the future. Use `gdalwarp` to make this conversion like so:

```
gdalwarp -t_srs ""EPSG:4326"" input.tif output.tif
```

1. Open a representative from map (or maps) in GIMP
1. With the color picker, **select the color that most represents the paper/background color** (using a 5-pixel averaging pick would be best). Make note of the **red, green and blue values** (0-255).
1. Do the **same for the building colors** (like the pink, green, blue in the example).

You now want to produce a neat black-white image where **lines are black and all the rest is white**:

1. Apply `Colors > Brightness-Contrast...` looking to make the lines darker and buildings/paper brighter. The default values are **-50 brightness** and **95 contrast**. These may or may not work for you. Make note of the values that work best.
1. Now apply `Colors > Threshold...`. This takes a black and a white value. Anything darker/lighter than these values will become black/white respectively. The default values are **160 black** and **255 white**. Make note of the values that work best.

You now have the configuration values for your maps (map color list, brightness-contrast values, threshold values). Open `vectorize_config_default.txt` and replace the default values with your custom values. Save it as `vectorize_config.txt` (and *keep the default just in case*). Your config file should look like:

```
BRIGHTNESS_VALUE,CONTRAST_VALUE,BLACK_VALUE,WHITE_VALUE, brightness-contrast-thresholdblack-thresholdwhite
PAPER_RED,PAPER_GREEN,PAPER_BLUE,paper
BLDG_RED,BLDG_GREEN,BLDG_BLUE,somebuildingcolor
BLDG_RED,BLDG_GREEN,BLDG_BLUE,someotherbuildingcolor
...
```

It should **always start** with brightness/contrast/threshold in the first line and paper in the second line. There should also be **at least one building color**. You can add as many building colors as you wish (since our maps at NYPL are hand-colored, colors are not uniform so we have lighter/darker versions to compensate that).

When you run the vectorizer again, it will find this config file and use those values instead of the defaults.

It is likely that the vectorizer won't produce excellent results in the first try. It is a matter of adjusting these color values to generalize as much as possible to your map set.


",2
541,map-vectorizer, Other scripts,"`map_vectorizer/templates` contains images that we use in OpenCV to match crosses and other marks on maps.

`test.tif` is a file for testing.

The other images in the root are for the readme.

`simplify_map.R` is used by `vectorize_map.py`

",2
542,map-vectorizer, Acknowledgements,"*bin/consolidator.py* for consolidating multiple outputs of a `vectorize_map.py` process into a single shapefile/GeoJSON. For example, when you run `vectorize_map.py` on a folder full of GeoTIFFs and then need to group all the individual folder outputs into a master file.

Usage: `./bin/consolidator.py path/to/folder` (no trailing slash)

",2
543,map-vectorizer, Change log,"* Michael Resig
* Chris Garrard for his [sample code to assemble and disassemble shapefiles](http://cosmicproject.org/OGR/cris_example_write.html)
* Barry Rowlingson for his [tutorial on converting alpha shapes to polygons](http://rpubs.com/geospacedman/alphasimple)

",2
544,LapSRN, Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution (CVPR 2017),,1
545,LapSRN, Table of Contents,"[Wei-Sheng Lai](http://graduatestudents.ucmerced.edu/wlai24/), 
[Jia-Bin Huang](https://filebox.ece.vt.edu/~jbhuang/), 
[Narendra Ahuja](http://vision.ai.illinois.edu/ahuja.html), 
and [Ming-Hsuan Yang](http://faculty.ucmerced.edu/mhyang/)

IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017

",3
546,LapSRN, Introduction,"1. [Introduction](#introduction)
1. [Citation](#citation)
1. [Requirements and Dependencies](#requirements-and-dependencies)
1. [Installation](#installation)
1. [Test Pre-trained Models](#test-pre-trained-models)
1. [Training LapSRN](#training-lapsrn)
1. [Training MS-LapSRN](#training-ms-lapsrn)
1. [Third-Party Implementation](#third-party-implementation)

",3
547,LapSRN, Citation,"The Laplacian Pyramid Super-Resolution Network (LapSRN) is a progressive super-resolution model that super-resolves an low-resolution images in a coarse-to-fine Laplacian pyramid framework.
Our method is fast and achieves state-of-the-art performance on five benchmark datasets for 4x and 8x SR.
For more details and evaluation results, please check out our [project webpage](http://vllab.ucmerced.edu/wlai24/LapSRN/) and [paper](http://vllab.ucmerced.edu/wlai24/LapSRN/papers/cvpr17_LapSRN.pdf).

![teaser](http://vllab.ucmerced.edu/wlai24/LapSRN/images/emma_text.gif)



",3
548,LapSRN, Requirements and Dependencies,"If you find the code and datasets useful in your research, please cite:
    
    @inproceedings{LapSRN,
        author    = {Lai, Wei-Sheng and Huang, Jia-Bin and Ahuja, Narendra and Yang, Ming-Hsuan}, 
        title     = {Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution}, 
        booktitle = {IEEE Conferene on Computer Vision and Pattern Recognition},
        year      = {2017}
    }
    

",3
549,LapSRN, Installation,"- MATLAB (we test with MATLAB R2017a on Ubuntu 16.04 and Windows 7)
- Cuda & Cudnn (we test with Cuda 8.0 and Cudnn 5.1)

",3
550,LapSRN, Start MATLAB,"Download repository:

    $ git clone https://github.com/phoenix104104/LapSRN.git

Run install.m in MATLAB to compile MatConvNet:

    ",1
551,LapSRN, Test Pre-trained Models,"    $ matlab
    >> install
   
If you install MatConvNet in your own path, you need to change the corresponding path in `install.m`, `train_LapSRN.m` and `test_LapSRN.m`.

",3
552,LapSRN, Training LapSRN,"To test LapSRN / MS-LapSRN on a single-image:

    >> demo_LapSRN
    >> demo_MSLapSRN

This script will load the pretrained LapSRN / MS-LapSRN model and apply SR on emma.jpg.

To test LapSRN / MS-LapSRN on benchmark datasets, first download the testing datasets:

    $ cd datasets
    $ wget http://vllab1.ucmerced.edu/~wlai24/LapSRN/results/SR_testing_datasets.zip
    $ unzip SR_testing_datasets.zip
    $ cd ..

Then choose the evaluated dataset and upsampling scale in `evaluate_LapSRN_dataset.m` and `evaluate_MSLapSRN_dataset.m`, and run:

    >> evaluate_LapSRN_dataset
    >> evaluate_MSLapSRN_dataset

which can reproduce the results in our paper.


",3
553,LapSRN, Training MS-LapSRN,"To train LapSRN from scratch, first download the training datasets:

    $ cd datasets
    $ wget http://vllab1.ucmerced.edu/~wlai24/LapSRN/results/SR_training_datasets.zip
    $ unzip SR_train_datasets.zip
    $ cd ..

or use the provided bash script to download all datasets and unzip at once:

    $ cd datasets
    $ ./download_SR_datasets.sh
    $ cd ..

Then, setup training options in `init_LapSRN_opts.m`, and run `train_LapSRN(scale, depth, gpuID)`. For example, to train LapSRN with depth = 10 for 4x SR using GPU ID = 1:

    >> train_LapSRN(4, 10, 1)
    
Note that we only test our code on single-GPU mode. MatConvNet supports training with multiple GPUs but you may need to modify our script and options (e.g., `opts.gpu`).

To test your trained LapSRN model, use `test_LapSRN(model_name, epoch, dataset, test_scale, gpu)`. For example, test LapSRN with depth = 10, scale = 4, epoch = 10 on Set5:

    >> test_LapSRN('LapSRN_x4_depth10_L1_train_T91_BSDS200_pw128_lr1e-05_step50_drop0.5_min1e-06_bs64', 10, 'Set5', 4, 1)

which will report the PSNR and SSIM.


",3
554,LapSRN, Third-Party Implementation,"Setup training options in `init_MSLapSRN_opts.m`, and run `train_MSLapSRN(scales, depth, recursive, gpuID)`, where `scales` should be a vector, e.g., [2, 4, 8]. For example, to train MS-LapSRN with D = 5, R = 2 for 2x, 4x and 8x SR:

    >> train_MSLapSRN([2, 4, 8], 5, 2, 1)
    
To test your trained MS-LapSRN model, use `test_MS-LapSRN(model_name, model_scale, epoch, dataset, test_scale, gpu)`, where `model_scale` is used to define the number of pyramid levels. `test_scale` could be different from `model_scale`. For example, test MS-LapSRN-D5R2 with two pyramid levels (`model_scale = 4`), epoch = 10, on Set5 for 3x SR:

    >> test_MSLapSRN('MSLapSRN_x248_SS_D5_R2_fn64_L1_train_T91_BSDS200_pw128_lr5e-06_step100_drop0.5_min1e-06_bs64', 4, 10, 'Set5', 3, 1)

which will report the PSNR and SSIM.

",3
555,RESCAN, RESCAN: Recurrent Squeeze-and-Excitation Context Aggregation Net,,1
556,RESCAN, Prerequisite,"Xia Li, Jianlong Wu, [Zhouchen Lin][2], [Hong Liu][3], [Hongbin Zha][4]<br>

Key Laboratory of Machine Perception, Shenzhen Graduate School, Peking University<br>
Key Laboratory of Machine Perception (MOE), School of EECS, Peking University<br>
Cooperative Medianet Innovation Center, Shanghai Jiao Tong University<br>
{[ethanlee][5], [jlwu1992][6], [zlin][7], [hongliu][8]}@pku.edu.cn, zha@cis.pku.edu.cn

Rain streaks can severely degrade the visibility, which causes many current computer vision algorithms fail to work. So it is necessary to remove the rain from images. We propose a novel deep network architecture based on deep convolutional and recurrent neural networks for single image deraining. As contextual information is very important for rain removal, we first adopt the dilated convolutional neural network to acquire large receptive field. To better fit the rain removal task, we also modify the network. In heavy rain, rain streaks have various directions and shapes, which can be regarded as the accumulation of multiple rain streak layers. We assign different alpha-values to various rain streak layers according to the intensity and transparency by incorporating the squeeze-and-excitation block. Since rain streak layers overlap with each other, it is not easy to remove the rain in one stage. So we further decompose the rain removal into multiple stages. Recurrent neural network is incorporated to preserve the useful information in previous stages and benefit the rain removal in later stages. We conduct extensive experiments on both synthetic and real-world datasets. Our proposed method outperforms the state-of-the-art approaches under all evaluation metrics.

Paper Link: http://openaccess.thecvf.com/content_ECCV_2018/papers/Xia_Li_Recurrent_Squeeze-and-Excitation_Context_ECCV_2018_paper.pdf

",2
557,RESCAN, Project Structure,"- Python>=3.6
- Pytorch>=4.1.0
- Opencv>=3.1.0
- tensorboardX

",2
558,RESCAN, Best Practices,"- config: contains all codes
    - cal_ssim.py
    - clean.sh
    - dataset.py
    - main.py
    - model.py
    - settings.py
    - show.py
    - tensorboard.sh
- explore.sh
- logdir: holds patches generated in training process
- models: holds checkpoints
- showdir: holds images predicted by the model

",2
559,RESCAN, Default Dataset settings,"Hold every experiment in an independent folder, and assign a long name to it.
We recommend list the important parameters in the folder name, for example: RESCAN.ConvRNN.Full.d_7.c_24(d: depth, c: channel).

",2
560,RESCAN," Train, Test and Show","Rain100H: [http://www.icst.pku.edu.cn/struct/Projects/joint_rain_removal.html][9]<br>
Rain800: [https://drive.google.com/drive/folders/0Bw2e6Q0nQQvGbi1xV1Yxd09rY2s][10]

We concatenate the two images(B and O) together as default inputs. If you want to change this setting, just modify config/dataset.py.
Moreover, there should be three folders 'train', 'val', 'test' in the dataset folder.
After download the datasets, don't forget to transform the format!

Update: Rain100H has updated its testing set, from origin 100 images(test100) to 200(test200) images. We update the performance of RESCAN + GRU as follow:

|         | PSNR  | SSIM  |
| :------:| :---: | :---: |
| test100 | 26.45 | 0.8458 |
| test200 | 25.92 | 0.8411 |

",2
561,RESCAN, Scripts,"    python train.py
    python eval.py
    python show.py

",2
562,RESCAN, Cite,"- explore.sh: Show the predicted images in browser
- config/tensorboard.sh: Open the tensorboard server
- config/clean.sh: Clear all the training records in the folder

",2
563,RESCAN, Recent Works,"If you use our code, please refer this repo.
If you publish your paper that refer to our paper, please cite:

    @inproceedings{li2018recurrent,  
        title={Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining},  
        author={Li, Xia and Wu, Jianlong and Lin, Zhouchen and Liu, Hong and Zha, Hongbin},  
        booktitle={European Conference on Computer Vision},  
        pages={262--277},  
        year={2018},  
        organization={Springer}  
    }


  [2]: http://cis.pku.edu.cn/faculty/vision/zlin/zlin.htm
  [3]: http://robotics.pkusz.edu.cn/team/leader/
  [4]: http://cis.pku.edu.cn/vision/Visual&Robot/people/zha/
  [5]: ethanlee@pku.edu.cn
  [6]: jlwu1992@pku.edu.cn
  [7]: zlin@pku.edu.cn
  [8]: hongliu@pku.edu.cn
  [9]: http://www.icst.pku.edu.cn/struct/Projects/joint_rain_removal.html
  [10]: https://drive.google.com/drive/folders/0Bw2e6Q0nQQvGbi1xV1Yxd09rY2s
  
",2
564,sg2im, sg2im,,1
565,sg2im, Model,"This is the code for the paper

**<a href=""https://arxiv.org/abs/1804.01622"">Image Generation from Scene Graphs</a>**
<br>
<a href=""http://cs.stanford.edu/people/jcjohns/"">Justin Johnson</a>,
<a href=""http://web.stanford.edu/~agrim/"">Agrim Gupta</a>,
<a href=""http://vision.stanford.edu/feifeili/"">Li Fei-Fei</a>
<br>
Presented at [CVPR 2018](http://cvpr2018.thecvf.com/)

Please note that this is not an officially supported Google product.

A **scene graph** is a structured representation of a visual scene where nodes represent *objects* in the scene and edges represent *relationships* between objects. In this paper we present and end-to-end neural network model that inputs a scene graph and outputs an image.

Below we show some example scene graphs along with images generated from those scene graphs using our model. By modifying the input scene graph we can exercise fine-grained control over the objects in the generated image.

<div align='center'>
  <img src='images/sheep/sg000000.png' width='112px'>
  <img src='images/sheep/sg000001.png' width='112px'>
  <img src='images/sheep/sg000002.png' width='112px'>
  <img src='images/sheep/sg000003.png' width='112px'>
  <img src='images/sheep/sg000004.png' width='112px'>
  <img src='images/sheep/sg000005.png' width='112px'>
  <img src='images/sheep/sg000006.png' width='112px'>
  <br>
  <img src='images/sheep/img000000.png' height='112px'>
  <img src='images/sheep/img000001.png' height='112px'>
  <img src='images/sheep/img000002.png' height='112px'>
  <img src='images/sheep/img000003.png' height='112px'>
  <img src='images/sheep/img000004.png' height='112px'>
  <img src='images/sheep/img000005.png' height='112px'>
  <img src='images/sheep/img000006.png' height='112px'>
</div>

If you find this code useful in your research then please cite
```
@inproceedings{johnson2018image,
  title={Image Generation from Scene Graphs},
  author={Johnson, Justin and Gupta, Agrim and Fei-Fei, Li},
  booktitle={CVPR},
  year={2018}
}
```

",2
566,sg2im, Setup,"The input scene graph is processed with a *graph convolution network* which passes information along edges to compute embedding vectors for all objects. These vectors are used to predict bounding boxes and segmentation masks for all objects, which are combined to form a coarse *scene layout*. The layout is passed to a *cascaded refinement network* (Chen an Koltun, ICCV 2017) which generates an output image at increasing spatial scales. The model is trained adversarially against a pair of *discriminator networks* which ensure that output images look realistic.

<div align='center'>
  <img src='images/system.png' width='1000px'>
</div>

",2
567,sg2im, Pretrained Models,"All code was developed and tested on Ubuntu 16.04 with Python 3.5 and PyTorch 0.4.

You can setup a virtual environment to run the code like this:

```bash
python3 -m venv env               #notes: Create a virtual environment
source env/bin/activate           #notes: Activate virtual environment
pip install -r requirements.txt   #notes: Install dependencies
echo $PWD > env/lib/python3.5/site-packages/sg2im.pth  #notes: Add current directory to python path
#notes: Work for a while ...
deactivate  #notes: Exit virtual environment
```

",2
568,sg2im, Running Models,"You can download pretrained models by running the script `bash scripts/download_models.sh`. This will download the following models, and will require about 355 MB of disk space:

- `sg2im-models/coco64.pt`: Trained to generate 64 x 64 images on the COCO-Stuff dataset. This model was used to generate the COCO images in Figure 5 from the paper.
- `sg2im-models/vg64.pt`: Trained to generate 64 x 64 images on the Visual Genome dataset. This model was used to generate the Visual Genome images in Figure 5 from the paper.
- `sg2im-models/vg128.pt`: Trained to generate 128 x 128 images on the Visual Genome dataset. This model was used to generate the images in Figure 6 from the paper.

Table 1 in the paper presents an ablation study where we disable various components of the full model. You can download the additional models used in this ablation study by running the script `bash scripts/download_ablated_models.sh`. This will download 12 additional models, requiring and additional 1.25 GB of disk space.

",2
569,sg2im, (Optional): GraphViz,"You can use the script `scripts/run_model.py` to easily run any of the pretrained models on new scene graphs using a simple human-readable JSON format. For example you can replicate the sheep images above like this:

```bash
python scripts/run_model.py \
  --checkpoint sg2im-models/vg128.pt \
  --scene_graphs scene_graphs/figure_6_sheep.json \
  --output_dir outputs
```

The generated images will be saved to the directory specified by the `--output_dir` flag. You can control whether the model runs on CPU or GPU using py passing the flag `--device cpu` or `--device gpu`.

We provide JSON files and pretrained models allowing you to recreate all images from Figures 5 and 6 from the paper.

",4
570,sg2im, Training new models,"This script can also draw images for the scene graphs themselves using [GraphViz](http://www.graphviz.org/); to enable this option just add the flag `--draw_scene_graphs 1` and the scene graph images will also be saved in the output directory. For this option to work you must install GraphViz; on Ubuntu 16.04 you can simply run `sudo apt-get install graphviz`.

",2
571,pymeshfix, Test of pymeshfix without VTK module,"=========

.. image:: https://travis-ci.org/pyvista/pymeshfix.svg?branch=master
   :target: https://travis-ci.org/pyvista/pymeshfix

.. image:: https://img.shields.io/pypi/v/pymeshfix.svg?logo=python&logoColor=white
   :target: https://pypi.org/project/pymeshfix/

Python/Cython wrapper of Marco Attene's wonderful, award-winning
`MeshFix <https://github.com/MarcoAttene/MeshFix-V2.0>`__ software.
This module brings the speed of C++ with the portability and ease of
installation of Python.

This software takes as input a polygon mesh and produces a copy of the input
where all the occurrences of a specific set of ""defects"" are corrected.
MeshFix has been designed to correct typical flaws present in raw digitized
mesh models, thus it might fail or produce coarse results
if run on other sorts of input meshes (e.g. tessellated CAD models).

The input is assumed to represent a single closed solid object, thus the output
will be a single watertight triangle mesh bounding a polyhedron.
All the singularities, self-intersections and degenerate elements are removed
from the input, while regions of the surface without defects are left
unmodified.

Installation
------------

From `PyPI <https://pypi.python.org/pypi/pymeshfix>`__

.. code:: bash

    pip install pymeshfix

From source at `GitHub <https://github.com/pyvista/pymeshfix>`__

.. code:: bash

    git clone https://github.com/pyvista/pymeshfix
    cd pymeshfix
    pip install .


Dependencies
------------
Requires ``numpy`` and ``pyvista``


Examples
--------
Test installation with the following from Python:

.. code:: python

    from pymeshfix import examples

    ",1
572,pymeshfix, Performs same mesh repair while leveraging VTK's plotting/mesh loading,"    examples.native()

    ",1
573,pymeshfix, Read mesh from infile and output cleaned mesh to outfile,"    examples.with_vtk()


Easy Example
------------
This example uses the Cython wrapper directly. No bells or whistles here:

.. code:: python

    from pymeshfix import _meshfix

    ",1
574,pymeshfix, Generate vertex and face arrays of cleaned mesh,"    _meshfix.clean_from_file(infile, outfile)


This example assumes the user has vertex and faces arrays in Python.

.. code:: python

    from pymeshfix import _meshfix

    ",1
575,pymeshfix, where v and f are numpy arrays or python lists,    ,1
576,pymeshfix, Create object from vertex and face arrays,"    vclean, fclean = _meshfix.clean_from_arrays(v, f)


Complete Examples with and without VTK
--------------------------------------

One of the main reasons to bring MeshFix to Python is to allow the library to
communicate to other python programs without having to use the hard drive.
Therefore, this example assumes that you have a mesh within memory and wish to
repair it using MeshFix.

.. code:: python

    import pymeshfix

    ",1
577,pymeshfix, Plot input,"    meshfix = pymeshfix.MeshFix(v, f)

    ",1
578,pymeshfix, Repair input mesh,"    meshfix.plot()

    ",1
579,pymeshfix, Access the repaired mesh with vtk,"    meshfix.repair()

    ",1
580,pymeshfix," Or, access the resulting arrays directly from the object","    mesh = meshfix.mesh

    ",1
581,pymeshfix, numpy np.float array,    meshfix.v ,1
582,pymeshfix, numpy np.int32 array,    meshfix.f ,1
583,pymeshfix, View the repaired mesh (requires vtkInterface),    ,1
584,pymeshfix, Save the mesh,"    meshfix.plot()

    ",1
585,pymeshfix, Create TMesh object,"    meshfix.write('out.ply')

Alternatively, the user could use the Cython wrapper of MeshFix directly if
vtk is unavailable or they wish to have more control over the cleaning
algorithm.

.. code:: python

    from pymeshfix import _meshfix

    ",1
586,pymeshfix," tin.load_array(v, f)  or read arrays from memory","    tin = _meshfix.PyTMesh()

    tin.LoadFile(infile)
    ",2
587,pymeshfix, Attempt to join nearby components,    ,1
588,pymeshfix, tin.join_closest_components(),    ,1
589,pymeshfix, Fill holes,    ,1
590,pymeshfix, Clean (removes self intersections),"    tin.fill_small_boundaries()
    print('There are {:d} boundaries'.format(tin.boundaries())

    ",1
591,pymeshfix, Check mesh for holes again,"    tin.clean(max_iters=10, inner_loops=3)

    ",1
592,pymeshfix, Clean again if necessary...,"    print('There are {:d} boundaries'.format(tin.boundaries())

    ",1
593,pymeshfix, Output mesh,    ,1
594,pymeshfix, or return numpy arrays,"    tin.save_file(outfile)

     ",1
595,neural-motifs, neural-motifs,,1
596,neural-motifs," Like this work, or scene understanding in general? You might be interested in checking out my brand new dataset VCR: Visual Commonsense Reasoning, at [visualcommonsense.com](https://visualcommonsense.com)!",,3
597,neural-motifs, Bibtex,"This repository contains data and code for the paper [Neural Motifs: Scene Graph Parsing with Global Context (CVPR 2018)](https://arxiv.org/abs/1711.06640v2) For the project page (as well as links to the baseline checkpoints), check out [rowanzellers.com/neuralmotifs](https://rowanzellers.com/neuralmotifs). If the paper significantly inspires you, we request that you cite our work:

",3
598,neural-motifs, Setup,"```
@inproceedings{zellers2018scenegraphs,
  title={Neural Motifs: Scene Graph Parsing with Global Context},
  author={Zellers, Rowan and Yatskar, Mark and Thomson, Sam and Choi, Yejin},
  booktitle = ""Conference on Computer Vision and Pattern Recognition"",  
  year={2018}
}
```
",1
599,neural-motifs, help,"0. Install python3.6 and pytorch 3. I recommend the [Anaconda distribution](https://repo.continuum.io/archive/). To install PyTorch if you haven't already, use
 ```conda install pytorch=0.3.0 torchvision=0.2.0 cuda90 -c pytorch```.
 
1. Update the config file with the dataset paths. Specifically:
    - Visual Genome (the VG_100K folder, image_data.json, VG-SGG.h5, and VG-SGG-dicts.json). See data/stanford_filtered/README.md for the steps I used to download these.
    - You'll also need to fix your PYTHONPATH: ```export PYTHONPATH=/home/rowan/code/scene-graph``` 

2. Compile everything. run ```make``` in the main directory: this compiles the Bilinear Interpolation operation for the RoIs as well as the Highway LSTM.

3. Pretrain VG detection. The old version involved pretraining COCO as well, but we got rid of that for simplicity. Run ./scripts/pretrain_detector.sh
Note: You might have to modify the learning rate and batch size, particularly if you don't have 3 Titan X GPUs (which is what I used). [You can also download the pretrained detector checkpoint here.](https://drive.google.com/open?id=11zKRr2OF5oclFL47kjFYBOxScotQzArX)

4. Train VG scene graph classification: run ./scripts/train_models_sgcls.sh 2 (will run on GPU 2). OR, download the MotifNet-cls checkpoint here: [Motifnet-SGCls/PredCls](https://drive.google.com/open?id=12qziGKYjFD3LAnoy4zDT3bcg5QLC0qN6).
5. Refine for detection: run ./scripts/refine_for_detection.sh 2 or download the [Motifnet-SGDet](https://drive.google.com/open?id=1thd_5uSamJQaXAPVGVOUZGAOfGCYZYmb) checkpoint.
6. Evaluate: Refer to the scripts ./scripts/eval_models_sg[cls/det].sh.

",1
600,Flow-Guided-Feature-Aggregation, Flow-Guided Feature Aggregation for Video Object Detection,,1
601,Flow-Guided-Feature-Aggregation, Introduction,"This repository is implemented by [Yuqing Zhu](https://github.com/jeremy43), [Shuhao Fu](https://github.com/howardmumu), and [Xizhou Zhu](https://github.com/einsiedler0408), when they are interns at MSRA.

",2
602,Flow-Guided-Feature-Aggregation, Disclaimer,"**Flow-Guided Feature Aggregation (FGFA)** is initially described in an [ICCV 2017 paper](https://arxiv.org/abs/1703.10025). It provides an accurate and end-to-end learning framework for video object detection. The proposed FGFA method, together with our previous work of [Deep Feature Flow](https://github.com/msracver/Deep-Feature-Flow), powered the winning entry of [ImageNet VID 2017](http://image-net.org/challenges/LSVRC/2017/results). It is worth noting that:

* FGFA improves the per-frame features by aggregating nearby frame features along the motion paths. It significantly improves the object detection accuracy in videos, especially for fast moving objects.
* FGFA is end-to-end trainable for the task of video object detection, which is vital for improving the recognition accuracy.
* We proposed to evaluate the detection accuracy for slow, medium and fast moving objects respectively, for better understanding and analysis of video object detection. The [motion-specific evaluation code](lib/dataset/imagenet_vid_eval_motion.py) is included in this repository.

***Click image to watch our demo video***

[![Demo Video on YouTube](https://media.giphy.com/media/7D9tmDgzB10HK/giphy.gif)](https://www.youtube.com/watch?v=R2h3DbTPvVg)

***Example object instances with slow, medium and fast motions***

![Instance Motion](instance_motion.png)

",2
603,Flow-Guided-Feature-Aggregation, License,"This is an official implementation for [Flow-Guided Feature Aggregation for Video Recognition](https://arxiv.org/abs/1703.10025) (FGFA) based on MXNet. It is worth noticing that:

  * The original implementation is based on our internal Caffe version on Windows. There are slight differences in the final accuracy and running time due to the plenty details in platform switch.
  * One-phase training is performed on the mixture of ImageNet DET+VID, instead of two-phase training as in the original paper (on ImageNet DET first, followed by ImageNet VID).
  * The code is tested on official [MXNet@(v0.10.0)](https://github.com/apache/incubator-mxnet/tree/v0.10.0) with the extra operators for Flow-guided Feature Aggregation.
  * We trained our model based on the ImageNet pre-trained [ResNet-v1-101](https://github.com/KaimingHe/deep-residual-networks) model and [Flying Chairs](https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html) pre-trained [FlowNet](https://lmb.informatik.uni-freiburg.de/resources/binaries/dispflownet/dispflownet-release-1.2.tar.gz) model using a [model converter](https://github.com/dmlc/mxnet/tree/430ea7bfbbda67d993996d81c7fd44d3a20ef846/tools/caffe_converter). The converted [ResNet-v1-101](https://github.com/KaimingHe/deep-residual-networks) model produces slightly lower accuracy (Top-1 Error on ImageNet val: 24.0% v.s. 23.6%).
  * This repository used code from [MXNet rcnn example](https://github.com/apache/incubator-mxnet/tree/master/example/rcnn) and [mx-rfcn](https://github.com/giorking/mx-rfcn).



",2
604,Flow-Guided-Feature-Aggregation, Citing Flow-Guided Feature Aggregation,"© Microsoft, 2017. Licensed under the [MIT](LICENSE) License.

",2
605,Flow-Guided-Feature-Aggregation, Main Results,"If you find Flow-Guided Feature Aggregation useful in your research, please consider citing:
```
@inproceedings{zhu17fgfa,
    Author = {Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, Yichen Wei},
    Title = {Flow-Guided Feature Aggregation for Video Object Detection},
    Conference = {ICCV},
    Year = {2017}
}

@inproceedings{dai16rfcn,
    Author = {Jifeng Dai, Yi Li, Kaiming He, Jian Sun},
    Title = {{R-FCN}: Object Detection via Region-based Fully Convolutional Networks},
    Conference = {NIPS},
    Year = {2016}
}
```

",2
606,Flow-Guided-Feature-Aggregation, Requirements: Software,"|                                 | <sub>training data</sub>     | <sub>testing data</sub> | <sub>mAP(%)</sub> | <sub>mAP(%)</br>(slow)</sub>  | <sub>mAP(%)</br>(medium)</sub> | <sub>mAP(%)</br>(fast)</sub> |
|---------------------------------|-------------------|--------------|---------|---------|--------|--------|
| <sub>Single-frame baseline</br>(R-FCN, ResNet-v1-101)</sub>   | <sub>ImageNet DET train</br> + VID train</sub> | <sub>ImageNet VID validation</sub> | 74.1 | 83.6 | 71.6 | 51.2 |
| <sub>FGFA</br>(R-FCN, ResNet-v1-101, FlowNet)</sub>           | <sub>ImageNet DET train</br> + VID train</sub> | <sub>ImageNet VID validation</sub> | 77.1 | 85.9 | 75.7 | 56.1 |
| <sub>FGFA + SeqNMS</br>(R-FCN, ResNet-v1-101, FlowNet)</sub>  | <sub>ImageNet DET train</br> + VID train</sub> | <sub>ImageNet VID validation</sub> | 78.9 | 86.8 | 77.9 | 57.9 |



*Detection accuracy of slow (motion IoU > 0.9), medium (0.7 ≤ motion IoU ≤ 0.9), and fast (motion IoU < 0.7) moving object instances.*

**[Motion-specific evaluation code](lib/dataset/imagenet_vid_eval_motion.py) is available!**


",2
607,Flow-Guided-Feature-Aggregation, Requirements: Hardware,"1. MXNet from [the offical repository](https://github.com/apache/incubator-mxnet). We tested our code on [MXNet@(v0.10.0)](https://github.com/apache/incubator-mxnet/tree/v0.10.0). Due to the rapid development of MXNet, it is recommended to checkout this version if you encounter any issues. We may maintain this repository periodically if MXNet adds important feature in future release.

2. Python packages might missing: cython, opencv-python >= 3.2.0, easydict. If `pip` is set up on your system, those packages should be able to be fetched and installed by running
	```
	pip install Cython
	pip install opencv-python==3.2.0.6
	pip install easydict==1.6
	```
3. For Windows users, Visual Studio 2015 is needed to compile cython module.


",2
608,Flow-Guided-Feature-Aggregation, Installation,"Any NVIDIA GPUs with at least 8GB memory should be OK.

",2
609,Flow-Guided-Feature-Aggregation, Demo,"1. Clone the Flow-Guided Feature Aggregation repository, and we call the directory that you cloned as ${FGFA_ROOT}.

~~~
git clone https://github.com/msracver/Flow-Guided-Feature-Aggregation.git
~~~
2. For Windows users, run ``cmd .\init.bat``. For Linux user, run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.

3. Install MXNet:

	3.1 Clone MXNet and checkout to [MXNet@(v0.10.0)](https://github.com/apache/incubator-mxnet/tree/v0.10.0) by
	```
	git clone --recursive https://github.com/apache/incubator-mxnet.git
	cd incubator-mxnet
	git checkout v0.10.0
	git submodule update
	```
	3.2 Copy operators in `$(FGFA_ROOT)/fgfa_rfcn/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by
	```
	cp -r $(FGFA_ROOT)/fgfa_rfcn/operator_cxx/* $(MXNET_ROOT)/src/operator/contrib/
	```
	3.3 Compile MXNet
	```
	cd ${MXNET_ROOT}
	make -j4
	```
	3.4 Install the MXNet Python binding by

	***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***
	```
	cd python
	sudo python setup.py install
	```
	3.5 For advanced users, you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`, and modify `MXNET_VERSION` in `./experiments/fgfa_rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.


",2
610,Flow-Guided-Feature-Aggregation, Preparation for Training & Testing,"1. To run the demo with our trained model (on ImageNet DET + VID train), please download the model manually from [OneDrive](https://1drv.ms/u/s!AqfHNsil2nOiiwDiKev7DB6L9ay7), and put it under folder `model/`.

	Make sure it looks like this:
	```
	./model/rfcn_fgfa_flownet_vid-0000.params
	```
2. Run
	```
	python ./fgfa_rfcn/demo.py
	```

",2
611,Flow-Guided-Feature-Aggregation, Usage,"1. Please download ILSVRC2015 DET and ILSVRC2015 VID dataset, and make sure it looks like this:

	```
	./data/ILSVRC2015/
	./data/ILSVRC2015/Annotations/DET
	./data/ILSVRC2015/Annotations/VID
	./data/ILSVRC2015/Data/DET
	./data/ILSVRC2015/Data/VID
	./data/ILSVRC2015/ImageSets
	```

2. Please download ImageNet pre-trained ResNet-v1-101 model and Flying-Chairs pre-trained FlowNet model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMOBdCBiNaKbcjPrA), and put it under folder `./model`. Make sure it looks like this:
	```
	./model/pretrained_model/resnet_v1_101-0000.params
	./model/pretrained_model/flownet-0000.params
	```

",2
612,Flow-Guided-Feature-Aggregation, Misc.,"1. All of our experiment settings (GPU #, dataset, etc.) are kept in yaml config files at folder `./experiments/fgfa_rfcn/cfgs`.

2. Two config files have been provided so far, namely, frame baseline (R-FCN) and the proposed FGFA  for ImageNet VID. We use 4 GPUs to train models on ImageNet VID.

3. To perform experiments, run the python script with the corresponding config file as input. For example, to train and test FGFA with R-FCN, use the following command
    ```
    python experiments/fgfa_rfcn/fgfa_rfcn_end2end_train_test.py --cfg experiments/fgfa_rfcn/cfgs/resnet_v1_101_flownet_imagenet_vid_rfcn_end2end_ohem.yaml
    ```
	A cache folder would be created automatically to save the model and the log under `output/fgfa_rfcn/imagenet_vid/`.

4. Please find more details in config files and in our code.

",2
613,Flow-Guided-Feature-Aggregation, FAQ,"Code has been tested under:

- Windows Server 2012 R2 with 4 K40 GPUs and Intel Xeon CPU E5-2650 v2 @ 2.60GHz
- Windows Server 2012 R2 with 2 Pascal Titan X GPUs and Intel Xeon CPU E5-2670 v2 @ 2.50GHz

",2
614,facebookresearch-pyrobot, What can you do with PyRobot?,"[PyRobot](https://www.pyrobot.org/) is a light weight, high-level interface which provides hardware independent APIs for robotic manipulation and navigation. This repository also contains the low-level stack for [LoCoBot](http://locobot.org), a low cost mobile manipulator hardware platform.

- [What can you do with PyRobot?](#what-can-you-do-with-pyrobot)
- [Installation](#installation)
- [Getting Started](#getting-started)
- [The Team](#the-team)
- [Citation](#citation)
- [License](#license)
- [Future features](#Future-features)

",2
615,facebookresearch-pyrobot, Installation,"<p align=""center"">
    <img src=""https://thumbs.gfycat.com/FickleSpeedyChimneyswift-size_restricted.gif"", height=""180"">
    <img src=""https://thumbs.gfycat.com/FinishedWeirdCockerspaniel-size_restricted.gif"", height=""180"">
    <img src=""https://thumbs.gfycat.com/WeightyLeadingGrub-size_restricted.gif"", height=""180"">
</p>

",2
616,facebookresearch-pyrobot, Installing both PyRobot and LoCoBot dependencies,,3
617,facebookresearch-pyrobot, Installing just PyRobot,"* Install **Ubuntu 16.04**

* Download the installation script
```bash
sudo apt update
sudo apt-get install curl
curl 'https://raw.githubusercontent.com/facebookresearch/pyrobot/master/robots/LoCoBot/install/locobot_install_all.sh' > locobot_install_all.sh
```

* Run the script to install everything (ROS, realsense driver, etc.). **Please connect the nuc machine to a realsense camera before running the following commands**.
```bash
chmod +x locobot_install_all.sh 
./locobot_install_all.sh
```

",3
618,facebookresearch-pyrobot, Getting Started,"* Install **Ubuntu 16.04** 

* Install [ROS kinetic](http://wiki.ros.org/kinetic/Installation/Ubuntu)

* Install KDL

```bash
sudo apt-get -y install ros-kinetic-orocos-kdl ros-kinetic-kdl-parser-py ros-kinetic-python-orocos-kdl ros-kinetic-trac-ik
```

* Install Python virtual environment

```bash
sudo apt-get -y install python-virtualenv
virtualenv_name=""pyenv_pyrobot""
VIRTUALENV_FOLDER=~/${virtualenv_name}
virtualenv --system-site-packages -p python2.7 $VIRTUALENV_FOLDER
```

* Install PyRobot 

```bash
cd ~
mkdir -p low_cost_ws/src
cd ~/low_cost_ws/src
source ~/${virtualenv_name}/bin/activate
git clone --recurse-submodules https://github.com/facebookresearch/pyrobot.git
cd pyrobot/
pip install .
```

**Warning**: As realsense keeps updating, compatibility issues might occur if you accidentally update 
realsense-related packages from `Software Updater` in ubuntu. Therefore, we recommend you not to update
any libraries related to realsense. Check the list of updates carefully when ubuntu prompts software udpates.

",2
619,facebookresearch-pyrobot, The Team,"Please refer to [pyrobot.org](https://pyrobot.org/) and [locobot.org](http://locobot.org)

",2
620,facebookresearch-pyrobot, Future features,"[Adithya Murali](http://adithyamurali.com/), [Tao Chen](https://taochenshh.github.io), [Dhiraj Gandhi](http://www.cs.cmu.edu/~dgandhi/), Kalyan Vasudev, [Lerrel Pinto](http://www.cs.cmu.edu/~lerrelp/), [Saurabh Gupta](http://saurabhg.web.illinois.edu) and [Abhinav Gupta](http://www.cs.cmu.edu/~abhinavg/). We would also like to thank everyone who has helped PyRobot in any way.

",2
621,facebookresearch-pyrobot, Citation,"We are planning several features, namely:
* Interfacing with other simulators like [AI Habitat](https://aihabitat.org)
* Gravity compensation
* PyRobot interface for [UR5](https://www.universal-robots.com)

",2
622,facebookresearch-pyrobot, License,"```
@article{pyrobot2019,
  title={PyRobot: An Open-source Robotics Framework for Research and Benchmarking},
  author={Adithyavairavan Murali and Tao Chen and Kalyan Vasudev Alwala and Dhiraj Gandhi and Lerrel Pinto and Saurabh Gupta and Abhinav Gupta},
  journal={arXiv preprint arXiv:1906.08236},
  year={2019}
}
```
",2
623,Detectron, Detectron,,1
624,Detectron, Introduction,"Detectron is Facebook AI Research's software system that implements state-of-the-art object detection algorithms, including [Mask R-CNN](https://arxiv.org/abs/1703.06870). It is written in Python and powered by the [Caffe2](https://github.com/caffe2/caffe2) deep learning framework.

At FAIR, Detectron has enabled numerous research projects, including: [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144), [Mask R-CNN](https://arxiv.org/abs/1703.06870), [Detecting and Recognizing Human-Object Interactions](https://arxiv.org/abs/1704.07333), [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002), [Non-local Neural Networks](https://arxiv.org/abs/1711.07971), [Learning to Segment Every Thing](https://arxiv.org/abs/1711.10370), [Data Distillation: Towards Omni-Supervised Learning](https://arxiv.org/abs/1712.04440), [DensePose: Dense Human Pose Estimation In The Wild](https://arxiv.org/abs/1802.00434), and [Group Normalization](https://arxiv.org/abs/1803.08494).

<div align=""center"">
  <img src=""demo/output/33823288584_1d21cf0a26_k_example_output.jpg"" width=""700px"" />
  <p>Example Mask R-CNN output.</p>
</div>

",2
625,Detectron, Update,"The goal of Detectron is to provide a high-quality, high-performance
codebase for object detection *research*. It is designed to be flexible in order
to support rapid implementation and evaluation of novel research. Detectron
includes implementations of the following object detection algorithms:

- [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017*
- [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017*
- [Faster R-CNN](https://arxiv.org/abs/1506.01497)
- [RPN](https://arxiv.org/abs/1506.01497)
- [Fast R-CNN](https://arxiv.org/abs/1504.08083)
- [R-FCN](https://arxiv.org/abs/1605.06409)

using the following backbone network architectures:

- [ResNeXt{50,101,152}](https://arxiv.org/abs/1611.05431)
- [ResNet{50,101,152}](https://arxiv.org/abs/1512.03385)
- [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt)
- [VGG16](https://arxiv.org/abs/1409.1556)

Additional backbone architectures may be easily implemented. For more details about these models, please see [References](#references) below.

",2
626,Detectron, License,"- 4/2018: Support Group Normalization - see [`GN/README.md`](./projects/GN/README.md)

",2
627,Detectron, Citing Detectron,"Detectron is released under the [Apache 2.0 license](https://github.com/facebookresearch/detectron/blob/master/LICENSE). See the [NOTICE](https://github.com/facebookresearch/detectron/blob/master/NOTICE) file for additional details.

",2
628,Detectron, Model Zoo and Baselines,"If you use Detectron in your research or wish to refer to the baseline results published in the [Model Zoo](MODEL_ZOO.md), please use the following BibTeX entry.

```
@misc{Detectron2018,
  author =       {Ross Girshick and Ilija Radosavovic and Georgia Gkioxari and
                  Piotr Doll\'{a}r and Kaiming He},
  title =        {Detectron},
  howpublished = {\url{https://github.com/facebookresearch/detectron}},
  year =         {2018}
}
```

",2
629,Detectron, Installation,"We provide a large set of baseline results and trained models available for download in the [Detectron Model Zoo](MODEL_ZOO.md).

",2
630,Detectron, Quick Start: Using Detectron,"Please find installation instructions for Caffe2 and Detectron in [`INSTALL.md`](INSTALL.md).

",2
631,Detectron, Getting Help,"After installation, please see [`GETTING_STARTED.md`](GETTING_STARTED.md) for brief tutorials covering inference and training with Detectron.

",2
632,Detectron, References,"To start, please check the [troubleshooting](INSTALL.md#troubleshooting) section of our installation instructions as well as our [FAQ](FAQ.md). If you couldn't find help there, try searching our GitHub issues. We intend the issues page to be a forum in which the community collectively troubleshoots problems.

If bugs are found, **we appreciate pull requests** (including adding Q&A's to `FAQ.md` and improving our installation instructions and troubleshooting documents). Please see [CONTRIBUTING.md](CONTRIBUTING.md) for more information about contributing to Detectron.

",2
633,DeepMVS, DeepMVS: Learning Multi-View Stereopsis,,1
634,DeepMVS, Training,"[![License](https://img.shields.io/badge/License-BSD%202--Clause-orange.svg)](https://opensource.org/licenses/BSD-2-Clause)

**DeepMVS** is a Deep Convolutional Neural Network which learns to estimate pixel-wise disparity maps from a sequence of an arbitrary number of unordered images with the camera poses already known or estimated. 

![teaser1](img/teaser1.gif)
![teaser2](img/teaser2.gif)

If you use our codes or datasets in your work, please cite:
```
@inproceedings{DeepMVS,
  author       = ""Huang, Po-Han and Matzen, Kevin and Kopf, Johannes and Ahuja, Narendra and Huang, Jia-Bin"",
  title        = ""DeepMVS: Learning Multi-View Stereopsis"",
  booktitle    = ""IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"",
  year         = ""2018""
}
```

For the paper and other details of DeepMVS or the MYS-Synth Dataset, please see our [project webpage](https://phuang17.github.io/DeepMVS/index.html).


",2
635,DeepMVS, Requirements,,3
636,DeepMVS, Instructions,"- **python 2.7**
- **numpy 1.13.1**
- **pytorch 0.3.0** and **torchvision**: Follow the instructions from [their website](http://pytorch.org/).
- **opencv 3.1.0**: Run ``conda install -c menpo opencv`` or ``pip install opencv-python``.
- **imageio 2.2.0** (with freeimage plugin): Run ``conda install -c conda-forge imageio`` or ``pip install imageio``. To install freeimage plugin, run the following Python script once:
    ```python 
    import imageio
    imageio.plugins.freeimage.download()
    ```
- **h5py 2.7.0**: Run ``conda install h5py`` or ``pip install h5py``.
- **lz4 0.23.1**: Run ``pip install lz4``.
- **cuda 8.0.61** and **16GB GPU RAM** (required for gpu support): The training codes use up to 14GB of the GPU RAM with the default configuration. We train our model with an NVIDIA Tesla P100 GPU. To reduce GPU RAM usage, feel free to try smaller ``--patch_width``, ``--patch_height``, ``--num_depths``, and ``--max_num_neighbors``. However, the resulting model may not show the efficacy as appeared in our paper.

",3
637,DeepMVS, Testing,"1. Download the training datasets.
    ```bash
    python python/download_training_datasets.py #notes: This may take up to 1-2 days to complete.
    ```
    **Update: The training datasets have been updated on May 18, 2018 because of some errors in camera poses. Please remove the files and download them again if you have downloaded the old version.**
2. Train the network.
    ```bash
    python python/train.py #notes: This may take up to 4-6 days to complete, depending on which GPU is used.
    ```

",2
638,DeepMVS, Requirements,,3
639,DeepMVS, Instructions,"- **python 2.7**
- **numpy 1.13.1**
- **pytorch 0.3.0** and **torchvision**: Follow the instructions from [their website](http://pytorch.org/).
- **opencv 3.1.0**: Run ``conda install -c menpo opencv`` or ``pip install opencv-python``.
- **imageio 2.2.0**: Run ``conda install -c conda-forge imageio`` or ``pip install imageio``.
- **pyquaternion 0.9.0**: Run ``pip install pyquaternion``.
- **pydensecrf**: Run ``pip install pydensecrf``.
- **cuda 8.0.61** and **6GB GPU RAM** (required for gpu support): The testing codes use up to 4GB of the GPU RAM with the default configuration. 
- **COLMAP 3.2**: Follow the instructions from [their website](https://colmap.github.io/). 
 
",3
640,DeepMVS, License,"1. Download the trained model.
    ```bash
    python python/download_trained_model.py
    ```

2. Run the sparse reconstruction and the ``image_undistorter`` using [COLMAP](https://colmap.github.io/). The ``image_undistorter`` will generate a ``images`` folder which contains undistorted images and a ``sparse`` folder which contains three ``.bin`` files.

3. Run the testing script with the paths to the undistorted images and the sparse construction model.
    ```bash
    python python/test.py --load_bin --image_path path/to/images --sparse_path path/to/sparse --output_path path/to/output/directory
    ```
    By default, the script resizes the images to be 540px in height to reduce the running time. If you would like to run the model with other resolutions, please pass the arguments `--image_width XXX` and `--image_height XXX`.
    If your COLMAP outputs ``.txt`` files instead of ``.bin`` files for the sparse reconstruction, simply remove the `--load_bin` flag.

4. To evaluate the predicted results, run
    ```bash
    python python/eval.py --load_bin --image_path path/to/images --sparse_path path/to/sparse --output_path path/to/output/directory --gt_path path/to/gt/directory --image_width 810 --image_height 540 --size_mismatch crop_pad
    ```
    In ``gt_path``, the ground truth disparity maps should be stored in npy format with filenames being ``<image_name>.depth.npy``. If the ground truths are depth maps instead of disparity maps, please add ``--gt_type depth`` flag.

",2
641,readgssi, readgssi,,1
642,readgssi, requirements,"*Copyleft 🄯 2017-2019*

![Example Radargram](https://github.com/iannesbitt/readgssi/raw/master/examples/main.png)

[![PyPI version](https://img.shields.io/pypi/v/readgssi.svg?colorB=limegreen&label=pypi%20package)](https://badge.fury.io/py/readgssi)
[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.1439119.svg)](https://dx.doi.org/10.5281/zenodo.1439119)
[![License](https://img.shields.io/badge/license-GNU%20Affero%203.0-lightgrey.svg)](https://github.com/iannesbitt/readgssi/blob/master/LICENSE)
[![Build Status](https://travis-ci.org/iannesbitt/readgssi.svg?branch=master)](https://travis-ci.org/iannesbitt/readgssi)
[![Downloads per month](https://img.shields.io/pypi/dm/readgssi.svg)](https://pypi.org/project/readgssi/)

`readgssi` is a tool intended for use as an open-source reader and preprocessing module for subsurface data collected with Geophysical Survey Systems Incorporated (GSSI) ground-penetrating georadar (GPR) devices. It has the capability to read DZT and DZG files with the same pre-extension name and plot the data contained in those files. `readgssi` is also currently able to translate most DZT files to CSV and will be able to translate to other output formats including HDF5  (see [future](#future)). Matlab code donated by [Gabe Lewis](https://earthsciences.dartmouth.edu/people/gabriel-lewis), Dartmouth College Department of Earth Sciences. Python adaptation written with permission by Ian Nesbitt, University of Maine School of Earth and Climate Sciences.

The file read parameters are based on GSSI's DZT file description, similar to the ones available on pages 55-57 of the [SIR-3000 manual](https://support.geophysical.com/gssiSupport/Products/Documents/Control%20Unit%20Manuals/GSSI%20-%20SIR-3000%20Operation%20Manual.pdf). File structure is, unfortunately, prone to change at any time, and although I've been able to test with files from several systems, I have not encountered every iteration of file header yet. If you run into trouble, please [create a github issue](https://github.com/iannesbitt/readgssi/issues).

Questions, feature requests, and bugs: please [open a github issue](https://github.com/iannesbitt/readgssi/issues). Kindly provide the error output, describe what you are attempting to do, and attach the DZT/DZG file(s) causing you trouble.

",2
643,readgssi, installation,"Strongly recommended to install via [anaconda](https://www.anaconda.com/download):
- [`obspy`](https://obspy.org/)
- [`matplotlib`](https://matplotlib.org/)
- [`numpy`](http://www.numpy.org/)
- [`pandas`](https://pandas.pydata.org/)
- [`h5py`](https://www.h5py.org/)

Install via `pip`:
- [`pynmea2`](https://pypi.org/project/pynmea2/)
- [`geopy`](https://pypi.org/project/geopy/)
- [`pytz`](https://pypi.org/project/pytz/)

",2
644,readgssi, installing from source:,"Once you have [anaconda](https://www.anaconda.com/download) running, installing requirements is pretty easy.

```bash
conda config --add channels conda-forge
conda create -n readgssi python==3.7 pandas h5py pytz obspy
conda activate readgssi
pip install readgssi
```

That should allow you to run the commands below.

",4
645,readgssi, usage,"If you choose to install a specific commit rather than the [latest working release of this software](https://pypi.org/project/readgssi), you may download this package, unzip to your home folder, open a command line, then install in the following way:

```bash
pip install ~/readgssi
```

",2
646,readgssi, basic functionality,"To display the help text:

```bash
$ readgssi -h

usage:
readgssi -i input.DZT [OPTIONS]

optional flags:
     OPTION     |      ARGUMENT       |       FUNCTIONALITY
-o, --output    | file:  /dir/f.ext   |  specify an output file
-f, --format    | string, eg. ""csv""   |  specify output format (csv is the only working format currently)
-p, --plot      | +integer or ""auto""  |  plot will be x inches high (dpi=150), or ""auto"". default: 10
-x, --xscale    | string, eg. ""dist""  |  readgssi will attempt to convert the x-axis to distance, time, or traces based on header values
-z, --zscale    | string, eg. ""time""  |  readgssi will attempt to convert the x-axis to depth, time, or samples based on header values
-n, --noshow    |                     |  suppress matplotlib popup window and simply save a figure (useful for multiple file processing)
-c, --colormap  | string, eg. ""Greys"" |  specify the colormap (https://matplotlib.org/users/colormaps.html#notes:grayscale-conversion)
-g, --gain      | positive (+)integer |  gain value (higher=greater contrast, default: 1)
-r, --bgr       |                     |  horizontal background removal algorithm (useful to remove ringing)
-R, --reverse   |                     |  reverse (flip radargram horizontally)
-w, --dewow     |                     |  trinomial dewow algorithm
-t, --bandpass  | +int-+int (MHz)     |  butterworth bandpass filter (positive integer range in megahertz; ex. 100-145)
-b, --colorbar  |                     |  add a colorbar to the radar figure
-a, --antfreq   | positive integer    |  specify antenna frequency (read automatically if not given)
-s, --stack     | +integer or ""auto""  |  specify trace stacking value or ""auto"" to autostack to ~2.5:1 x:y axis ratio
-N, --normalize |                     |  reads a .DZG NMEA data if it exists; otherwise tries to read a csv file with lat, lon, and time fields to distance normalize with
-d, --spm       | positive float      |  specify the samples per meter (spm) manually. overrides header value.
-m, --histogram |                     |  produce a histogram of data values
-E, --epsr      | float > 1.0         |  user-defined epsilon sub r (sometimes referred to as ""dielectric""; ignores value in DZT header)
-Z, --zero      | positive integer    |  skip this many samples from the top of the trace downward (useful for removing transceiver delay)

naming scheme for exports:
   CHARACTERS   |      MEANING
    c0          |  Profile from channel 0 (can range from 0 - 3)
    Dn          |  Distance normalization
    Tz233       |  Time zero at 233 samples
    S8          |  Stacked 8 times
    Rv          |  Profile read in reverse (flipped horizontally)
    Bgr         |  Background removal filter
    Dw          |  Dewow filter
    Bp100-145   |  2-corner bandpass filter applied from 100 to 145 MHz
    G30         |  30x contrast gain
```

From a unix command line:
```bash
readgssi -i DZT__001.DZT
```
Simply specifying an input DZT file like in the above command (`-i file`) will display a host of data about the file including:
- name of GSSI control unit
- antenna model
- antenna frequency
- samples per trace
- bits per sample
- traces per second
- L1 dielectric as entered during survey
- sampling depth
- speed of light at given dielectric
- number of traces
- number of seconds

",2
647,readgssi, CSV output,,3
648,readgssi, plotting,"```bash
readgssi -i DZT__001.DZT -o test.csv -f CSV
```
Translates radar data array to CSV format, if that's your cup of tea. One might use this to export to Matlab. One CSV will be written per channel. The script will rename the output to 'test_100MHz.csv' automatically. No header information is included in the CSV.

```bash
readgssi -i DZT__001.DZT -s 8 -w -r -o test.csv -f CSV
```
Applies 8x stacking, dewow, and background removal filters before exporting to CSV.

",3
649,readgssi, example 1A,,4
650,readgssi, example 1B,"```bash
readgssi -i DZT__001.DZT -p 5 -s auto -c viridis -m
```
The above command will cause `readgssi` to save and show a plot named ""DZT__001_100MHz.png"" with a y-size of 6 inches at 150 dpi (`-p 6`) and the autostacking algorithm will stack the x-axis to some multiple of times shorter than the original data array for optimal viewing on a monitor, approximately 2.5\*y (`-s auto`). The plot will be rendered in the viridis color scheme, which is the default for matplotlib. The `-m` flag will draw a histogram for each data channel.
![Example 1a](https://github.com/iannesbitt/readgssi/raw/master/examples/1a.png)
![Example 1a histogram](https://github.com/iannesbitt/readgssi/raw/master/examples/1a-h.png)

",4
651,readgssi, example 1C: gain can be tricky depending on your colormap,"```bash
readgssi -i DZT__001.DZT -o 1b.png -p 5 -s auto -c viridis -g 50 -m -r -w
```
This will cause `readgssi` to create a plot from the same file, but matplotlib will save the plot as ""1b.png"" (`-o 1b.png`). The script will plot the y-axis size (`-p 5`) and automatically stack the x-axis to (`-s auto`). The script will plot the data with a gain value of 50 (`-g 50`), which will increase the plot contrast by a factor of 50. Next `readgssi` will run the background removal (`-r`) and dewow (`-w`) filters. Finally, the `-m` flag will draw a histogram for each data channel. Note how the histogram changes when filters are applied.
![Example 1b](https://github.com/iannesbitt/readgssi/raw/master/examples/1b.png)
![Example 1b histogram](https://github.com/iannesbitt/readgssi/raw/master/examples/1b-h.png)

",4
652,readgssi, example 2A: no background removal,"```bash
readgssi -i DZT__001.DZT -o 1c.png -p 5 -s auto -r -w -c seismic
```
Here, background removal and dewow filters are applied, but no gain adjustments are made (equivalent to `-g 1`). The script uses matplotlib's ""seismic"" colormap (`-c seismic`) which is specifically designed for this type of waterfall array plotting. Even without gain, you will often be able to easily see very slight signal perturbations. It is not colorblind-friendly for either of the two most common types of human colorblindness, however, which is why it is not the default colormap.
![Example 1c](https://github.com/iannesbitt/readgssi/raw/master/examples/1c.png)

",4
653,readgssi, example 2B: horizontal mean BGR algorithm applied,"```bash
readgssi -i DZT__002.DZT -o 2a.png -p 10 -s 3 -n
```
Here `readgssi` will create a plot of size 10 and stack 3x (`-p 10 -s 3`). Matplotlib will use the default ""Greys"" colormap and save a PNG of the figure, but the script will suppress the matplotlib window (`-n`, useful for processing an entire directory full of DZTs at once).
![Example 2a](https://github.com/iannesbitt/readgssi/raw/master/examples/2a.png)

",4
654,readgssi, contributors,"```bash
readgssi -i DZT__002.DZT -o 2b.png -p 10 -s 3 -n -r
```
The script does the same thing, except it applies horizontal mean background removal `-r`. Note the difference in ringing artifacts between examples 2a and 2b.
![Example 2b](https://github.com/iannesbitt/readgssi/raw/master/examples/2b.png)


",2
655,readgssi, citation suggestion:,"- Ian Nesbitt ([@iannesbitt](https://github.com/iannesbitt), author)
- François-Xavier Simon ([@fxsimon](https://github.com/fxsimon))
- Thomas Paulin ([@thomaspaulin](https://github.com/thomaspaulin))

",3
656,readgssi, known bugs:,"Ian M. Nesbitt, François-Xavier Simon, Thomas Paulin, 2018. readgssi - an open-source tool to read and plot GSSI ground-penetrating radar data. [doi:10.5281/zenodo.1439119](https://dx.doi.org/10.5281/zenodo.1439119)

",4
657,readgssi, future,"- color bar shows up too large on some plots (matplotlib bug)

",2
658,integral-human-pose, Integral Human Pose Regression,,1
659,integral-human-pose, Introduction,"The major contributors of this repository include [Xiao Sun](https://jimmysuen.github.io/), [Chuankang Li](https://github.com/lck1201), [Bin Xiao](https://github.com/leoxiaobin), [Fangyin Wei](https://weify627.github.io/), [Yichen Wei](https://github.com/YichenWei).

",2
660,integral-human-pose, Disclaimer,"**Integral Regression** is initially described in an [ECCV 2018 paper](https://arxiv.org/abs/1711.08229). ([Slides](https://jimmysuen.github.io/slides/xiaosun_integral_human_pose_regression.pptx)).

We build a [3D pose estimation system](https://arxiv.org/abs/1809.06079) based mainly on the Integral Regression, placing second in the [ECCV2018 3D Human Pose Estimation Challenge](http://vision.imar.ro/human3.6m/ranking.php). Note that, the winner [Sarandi et al.](https://arxiv.org/pdf/1809.04987.pdf) also uses the Integral Regression (or soft-argmax) with a better [augmented 3D dataset](https://github.com/isarandi/synthetic-occlusion) in their method indicating the Integral Regression is the currently state-of-the-art 3D human pose estimation method.

The Integral Regression is also known as soft-argmax. Please refer to two contemporary works ([Luvizon et al.](https://arxiv.org/abs/1710.02322) and [Nibali et al.](https://arxiv.org/abs/1801.07372)) for a better comparision and more comprehensive understanding.


<img src='figure/overview5.png' width='800'>
<img src='figure/examples2.png' width='800'>

",2
661,integral-human-pose, License,"This is an official implementation for [Integral Human Pose Regression](https://arxiv.org/abs/1711.08229) based on Pytorch. It is worth noticing that:

  * The original implementation is based on our internal Mxnet version. There are slight differences in the final accuracy and running time due to the plenty details in platform switch.

",2
662,integral-human-pose, Citing Papers,"© Microsoft, 2017. Licensed under an MIT license.


",2
663,integral-human-pose, Main Results,"If you find Integral Regression useful in your research, please consider citing:
```
@article{sun2017integral,
  title={Integral human pose regression},
  author={Sun, Xiao and Xiao, Bin and Liang, Shuang and Wei, Yichen},
  journal={arXiv preprint arXiv:1711.08229},
  year={2017}
}
```
```
@article{sun2018integral,
  title={An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge},
  author={Sun, Xiao and Li, Chuankang and Lin, Stephen},
  journal={arXiv preprint arXiv:1809.06079},
  year={2018}
}
```

",2
664,integral-human-pose, Environment,"Train on CHALL_H80K train, evaluate on CHALL_H80K val.

|description        |box det.   |dataset        |backbone   |patch size |flip test  |ensemble   |MPJPE(mm)  | model                                     |
|-------------------|-----------|---------------|-----------|-----------|-----------|-----------|-----------|-------------------------------------------|
|original baseline  |No         |HM36           |ResNet-50  |  -        |No         |No         |115.9      |                                           |
|+person box det.   |Yes        |HM36           |ResNet-50  |256*256    |No         |No         |86.5(25.4%)|                                           |
|+MPII data         |Yes        |HM36+MPII      |ResNet-50  |256*256    |No         |No         |62.2(28.1%)|                                           |
|+deeper            |Yes        |HM36+MPII      |ResNet-152 |256*256    |No         |No         |61.2(1.6%) |                                           |
|+larger image      |Yes        |HM36+MPII      |ResNet-152 |288*384    |No         |No         |58.5(4.4%) |                                           |
|+COCO data         |Yes        |HM36+MPII+COCO |ResNet-152 |288*384    |No         |No         |57.5(1.7%) |[download](https://www.dropbox.com/s/hfz5nkd39uisvbr/model_chall_train_152ft_384x288.pth.tar?dl=0)|
|+flip test         |Yes        |HM36+MPII+COCO |ResNet-152 |288*384    |Yes        |No         |56.9(1.0%) |[download](https://www.dropbox.com/s/hfz5nkd39uisvbr/model_chall_train_152ft_384x288.pth.tar?dl=0)|
|+model ensemble    |Yes        |HM36+MPII+COCO |ResNet-152 |288*384    |Yes        |Yes        |55.3(2.8%) |[download](https://www.dropbox.com/s/hfz5nkd39uisvbr/model_chall_train_152ft_384x288.pth.tar?dl=0)|

Train on CHALL_H80K train+val, evaluate on CHALL_H80K test.

|description        |box det.   |dataset        |backbone   |patch size |flip test  |ensemble   |MPJPE(mm)  | model                                     |
|-------------------|-----------|---------------|-----------|-----------|-----------|-----------|-----------|-------------------------------------------|
|challenge result   |Yes        |HM36+MPII+COCO |ResNet-152 |288*384    |Yes        |Yes        |47         | [download](https://www.dropbox.com/s/o1xhp3vocnvtgu3/model_chall_trainval_152ft_384x288.pth.tar?dl=0)|

",2
665,integral-human-pose, Installation,"Python Version: 3.6 <br/>
OS: CentOs7 (Other Linux system is also OK) <br/>
CUDA: 9.0 (least 8.0) <br/>
PyTorch:0.4.0(see issue https://github.com/JimmySuen/integral-human-pose/issues/4)


",2
666,integral-human-pose, Preparation for Training & Testing,"We recommend installing python from [Anaconda](https://www.anaconda.com/), installing pytorch following guide on [PyTorch](https://pytorch.org/) according to your specific CUDA & python version.
In addition, you need to install dependencies below.
```
pip install scipy
pip install matplotlib
pip install opencv-python
pip install easydict
pip install pyyaml
``` 


",2
667,integral-human-pose, Extensions,"1. Download Human3.6M(ECCV18 Challenge) image from [Human3.6M Dataset](http://vision.imar.ro/human3.6m/description.php) and our processed annotation from [Baidu Disk](https://pan.baidu.com/s/1Qg4dH8PBXm8SzApI-uu0GA) (code: kfsm) or [Google Drive](https://drive.google.com/file/d/1wZynXUq91yECVRTFV8Tetvo271BXzxwI/view?usp=sharing)
2. Download MPII image from [MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/)
3. Download COCO2017 image from [COCO Dataset](http://cocodataset.org/#home)
4. Download cache file from [Dropbox](https://www.dropbox.com/sh/uouev0a1ao84ofd/AADAjJUdr_Fm-eubk7c_s2JTa?dl=0)
5. Organize data like this
```
${PROJECT_ROOT}
 `-- data
     `-- coco
        |-- images
        |-- annotations
        |-- COCO_train2017_cache
     `-- mpii
        |-- images
        |-- annot
        |-- mpii_train_cache
        |-- mpii_valid_cache
     `-- hm36
        |-- images
        |-- annot
        |-- HM36_train_cache
        |-- HM36_validmin_cache
     `-- hm36_eccv_challenge
        `-- Train
            |-- IMG
            |-- POSE
        `-- Val
            |-- IMG
            |-- POSE
        `-- Test
            |-- IMG
        |-- HM36_eccv_challenge_Train_cache
        |-- HM36_eccv_challenge_Test_cache
        |-- HM36_eccv_challenge_Val_cache
```

#notes:#notes: Usage
We have placed some example config files in *experiments* folder, and you can use them straight forward. Don't modify them unless you know exactly what it means.
#notes:#notes:#notes: Train 
For [Integral Human Pose Regression](https://arxiv.org/abs/1711.08229), cd to *pytorch_projects/integral_human_pose* <br/>
**Integral Regression**
```bash
python train.py --cfg=experiments/hm36/resnet50v1_ft/d-mh_ps-256_deconv256x3_min-int-l1_adam_bs32-4gpus_x300-270-290/lr1e-3.yaml --dataroot=../../data/  
```
**Direct Joint Regression**
```bash
python train.py --cfg=experiments/hm36/resnet50v1_ft/d-mh_ps-256_dj_l1_adam_bs32-4gpus_x140-90-120/lr1e-3.yaml --dataroot=../../data/
```

For [3D pose estimation system](https://arxiv.org/abs/1809.06079) of ECCV18 Challenge, cd to *pytorch_projects/hm36_challenge*
```bash
python train.py --cfg=experiments/hm36/resnet152v1_ft/d-mh_ps-256_deconv256x3_min-int-l1_adam_bs24-4gpus_x300-270-290/lr1e-3.yaml --dataroot=../../data/
```

By default, logging and model will be saved to *log* and *output* folder respectively.

#notes:#notes:#notes: Test
To run evaluation on CHALL_H80K Val dataset
1. Download [model](https://www.dropbox.com/s/hfz5nkd39uisvbr/model_chall_train_152ft_384x288.pth.tar?dl=0)
2. Place it under $project_root/model/hm36_challenge
3. cd to *$project_root/pytorch_projects/hm36_challenge*
4. execute command below
```bash
python test.py --cfg experiments/hm36/resnet152v1_ft/d-mch_384x288_deconv256x3_min-int-l1_adam_bs12-4gpus/lr1e-4_x300-270-290.yaml --model=../../model/hm36_challenge/model_chall_train_152ft_384x288.pth.tar
```
",2
668,gdal-docker, GDAL Docker Images,,1
669,gdal-docker, Usage,"NB: As of GDAL version 1.11.2 the image has been renamed from `homme/gdal` to
`geodata/gdal`.

This is an Ubuntu derived image containing the Geospatial Data Abstraction
Library (GDAL) compiled with a broad range of drivers. The build process is
based on that defined in the
[GDAL TravisCI tests](https://github.com/OSGeo/gdal/blob/trunk/.travis.yml).

Each branch in the git repository corresponds to a supported GDAL version
(e.g. `1.11.2`) with the master branch following GDAL master. These branch names
are reflected in the image tags on the Docker Index (e.g. branch `1.11.2`
corresponds to the image `geodata/gdal:1.11.2`).

",2
670,pyro-ppl-pyro, Installing,"  <a href=""http://pyro.ai""> <img width=""220px"" height=""220px"" src=""docs/source/_static/img/pyro_logo_with_text.png""></a>
</div>

-----------------------------------------

[![Build Status](https://travis-ci.com/pyro-ppl/pyro.svg?branch=dev)](https://travis-ci.com/uber/pyro)
[![codecov.io](https://codecov.io/github/pyro-ppl/pyro/branch/dev/graph/badge.svg)](https://codecov.io/github/uber/pyro)
[![Latest Version](https://badge.fury.io/py/pyro-ppl.svg)](https://pypi.python.org/pypi/pyro-ppl)
[![Documentation Status](https://readthedocs.org/projects/pyro-ppl/badge/?version=dev)](http://pyro-ppl.readthedocs.io/en/stable/?badge=dev)

[Getting Started](http://pyro.ai/examples) |
[Documentation](http://docs.pyro.ai/) |
[Community](http://forum.pyro.ai/) |
[Contributing](https://github.com/pyro-ppl/pyro/blob/master/CONTRIBUTING.md)

Pyro is a flexible, scalable deep probabilistic programming library built on PyTorch.  Notably, it was designed with these principles in mind:

- **Universal**: Pyro is a universal PPL - it can represent any computable probability distribution.
- **Scalable**: Pyro scales to large data sets with little overhead compared to hand-written code.
- **Minimal**: Pyro is agile and maintainable. It is implemented with a small core of powerful, composable abstractions.
- **Flexible**: Pyro aims for automation when you want it, control when you need it. This is accomplished through high-level abstractions to express generative and inference models, while allowing experts easy-access to customize inference.

Pyro is in a beta release.  It is developed and maintained by [Uber AI Labs](http://uber.ai) and community contributors.
For more information, check out our [blog post](http://eng.uber.com/pyro).

",2
671,pyro-ppl-pyro, Installing a stable Pyro release,,3
672,pyro-ppl-pyro, Installing Pyro dev branch,"**Install using pip:**

Pyro supports Python 3.4+.

```sh
pip install pyro-ppl
```

**Install from source:**
```sh
git clone git@github.com:pyro-ppl/pyro.git
cd pyro
git checkout master  #notes: master is pinned to the latest release
pip install .
```

**Install with extra packages:**

To install the dependencies required to run the probabilistic models included in the `examples`/`tutorials` directories, please use the following command:
```sh
pip install pyro-ppl[extras] 
```
Make sure that the models come from the same release version of the [Pyro source code](https://github.com/pyro-ppl/pyro/releases) as you have installed.

",3
673,pyro-ppl-pyro, Running Pyro from a Docker Container,"For recent features you can install Pyro from source.

**Install using pip:**

```sh
pip install git+https://github.com/pyro-ppl/pyro.git
```

or, with the `extras` dependency to run the probabilistic models included in the `examples`/`tutorials` directories:
```sh
pip install git+https://github.com/pyro-ppl/pyro.git#notes:egg=project[extras]
```

**Install from source:**

```sh
git clone https://github.com/pyro-ppl/pyro
cd pyro
pip install .  #notes: pip install .[extras] for running models in examples/tutorials
```

",2
674,pyro-ppl-pyro, Citation,"Refer to the instructions [here](docker/README.md).

",2
675,3D-ResNets-PyTorch, 3D ResNets for Action Recognition,,1
676,3D-ResNets-PyTorch, Update (2018/2/21),,2
677,3D-ResNets-PyTorch, Update (2018/01/16),"Our paper ""Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?"" is accepted to CVPR2018!  
We update the paper information.

",2
678,3D-ResNets-PyTorch, Update (2017/11/27),"We uploaded some of fine-tuned models on UCF-101 and HMDB-51.

* ResNeXt-101 fine-tuned on UCF-101 (split1)
* ResNeXt-101 (64 frame inputs) fine-tuned on UCF-101 (split1)
* ResNeXt-101 fine-tuned on HMDB-51 (split1)
* ResNeXt-101 (64 frame inputs) fine-tuned on HMDB-51 (split1)

",2
679,3D-ResNets-PyTorch, Summary,"We published [a new paper](https://arxiv.org/abs/1711.09577) on arXiv.  
We also added the following new models and their Kinetics pretrained models in this repository.  

* ResNet-50, 101, 152, 200
* Pre-activation ResNet-200
* Wide ResNet-50
* ResNeXt-101
* DenseNet-121, 201

In addition, we supported new datasets (UCF-101 and HDMB-51) and fine-tuning functions.

Some minor changes are included.

* Outputs are normalized by softmax in test.
  * If you do not want to perform the normalization, please use ```--no_softmax_in_test``` option.

",2
680,3D-ResNets-PyTorch, Citation,"This is the PyTorch code for the following papers:

[
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh,  
""Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?"",  
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6546-6555, 2018.
](http://openaccess.thecvf.com/content_cvpr_2018/html/Hara_Can_Spatiotemporal_3D_CVPR_2018_paper.html)

[
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh,  
""Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition"",  
Proceedings of the ICCV Workshop on Action, Gesture, and Emotion Recognition, 2017.
](http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w44/Hara_Learning_Spatio-Temporal_Features_ICCV_2017_paper.pdf)

This code includes training, fine-tuning and testing on Kinetics, ActivityNet, UCF-101, and HMDB-51.  
**If you want to classify your videos or extract video features of them using our pretrained models,
use [this code](https://github.com/kenshohara/video-classification-3d-cnn-pytorch).**

**The Torch (Lua) version of this code is available [here](https://github.com/kenshohara/3D-ResNets).**  
Note that the Torch version only includes ResNet-18, 34, 50, 101, and 152.

",2
681,3D-ResNets-PyTorch, Pre-trained models,"If you use this code or pre-trained models, please cite the following:

```bibtex
@inproceedings{hara3dcnns,
  author={Kensho Hara and Hirokatsu Kataoka and Yutaka Satoh},
  title={Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={6546--6555},
  year={2018},
}
```

",2
682,3D-ResNets-PyTorch, Performance of the models on Kinetics,"Pre-trained models are available [here](https://drive.google.com/drive/folders/1zvl89AgFAApbH0At-gMuZSeQB_LpNP-M?usp=sharing).  
All models are trained on Kinetics.  
ResNeXt-101 achieved the best performance in our experiments. (See [paper](https://arxiv.org/abs/1711.09577) in details.)

```misc
resnet-18-kinetics.pth: --model resnet --model_depth 18 --resnet_shortcut A
resnet-34-kinetics.pth: --model resnet --model_depth 34 --resnet_shortcut A
resnet-34-kinetics-cpu.pth: CPU ver. of resnet-34-kinetics.pth
resnet-50-kinetics.pth: --model resnet --model_depth 50 --resnet_shortcut B
resnet-101-kinetics.pth: --model resnet --model_depth 101 --resnet_shortcut B
resnet-152-kinetics.pth: --model resnet --model_depth 152 --resnet_shortcut B
resnet-200-kinetics.pth: --model resnet --model_depth 200 --resnet_shortcut B
preresnet-200-kinetics.pth: --model preresnet --model_depth 200 --resnet_shortcut B
wideresnet-50-kinetics.pth: --model wideresnet --model_depth 50 --resnet_shortcut B --wide_resnet_k 2
resnext-101-kinetics.pth: --model resnext --model_depth 101 --resnet_shortcut B --resnext_cardinality 32
densenet-121-kinetics.pth: --model densenet --model_depth 121
densenet-201-kinetics.pth: --model densenet --model_depth 201
```

Some of fine-tuned models on UCF-101 and HMDB-51 (split 1) are also available.

```misc
resnext-101-kinetics-ucf101_split1.pth: --model resnext --model_depth 101 --resnet_shortcut B --resnext_cardinality 32
resnext-101-64f-kinetics-ucf101_split1.pth: --model resnext --model_depth 101 --resnet_shortcut B --resnext_cardinality 32 --sample_duration 64
resnext-101-kinetics-hmdb51_split1.pth: --model resnext --model_depth 101 --resnet_shortcut B --resnext_cardinality 32
resnext-101-64f-kinetics-hmdb51_split1.pth: --model resnext --model_depth 101 --resnet_shortcut B --resnext_cardinality 32 --sample_duration 64
```

",3
683,3D-ResNets-PyTorch, Requirements,"This table shows the averaged accuracies over top-1 and top-5 on Kinetics.

| Method | Accuracies |
|:---|:---:|
| ResNet-18 | 66.1 |
| ResNet-34 | 71.0 |
| ResNet-50 | 72.2 |
| ResNet-101 | 73.3 |
| ResNet-152 | 73.7 |
| ResNet-200 | 73.7 |
| ResNet-200 (pre-act) | 73.4 |
| Wide ResNet-50 | 74.7 |
| ResNeXt-101 | 75.4 |
| DenseNet-121 | 70.8 |
| DenseNet-201 | 72.3 |

",2
684,3D-ResNets-PyTorch, Preparation,"* [PyTorch](http://pytorch.org/)

```bash
conda install pytorch torchvision cuda80 -c soumith
```

* FFmpeg, FFprobe

```bash
wget http://johnvansickle.com/ffmpeg/releases/ffmpeg-release-64bit-static.tar.xz
tar xvf ffmpeg-release-64bit-static.tar.xz
cd ./ffmpeg-3.3.3-64bit-static/; sudo cp ffmpeg ffprobe /usr/local/bin;
```

* Python 3

",2
685,3D-ResNets-PyTorch, ActivityNet,,3
686,3D-ResNets-PyTorch, Kinetics,"* Download videos using [the official crawler](https://github.com/activitynet/ActivityNet/tree/master/Crawler).
* Convert from avi to jpg files using ```utils/video_jpg.py```

```bash
python utils/video_jpg.py avi_video_directory jpg_video_directory
```

* Generate fps files using ```utils/fps.py```

```bash
python utils/fps.py avi_video_directory jpg_video_directory
```

",3
687,3D-ResNets-PyTorch, UCF-101,"* Download videos using [the official crawler](https://github.com/activitynet/ActivityNet/tree/master/Crawler/Kinetics).
  * Locate test set in ```video_directory/test```.
* Convert from avi to jpg files using ```utils/video_jpg_kinetics.py```

```bash
python utils/video_jpg_kinetics.py avi_video_directory jpg_video_directory
```

* Generate n_frames files using ```utils/n_frames_kinetics.py```

```bash
python utils/n_frames_kinetics.py jpg_video_directory
```

* Generate annotation file in json format similar to ActivityNet using ```utils/kinetics_json.py```
  * The CSV files (kinetics_{train, val, test}.csv) are included in the crawler.

```bash
python utils/kinetics_json.py train_csv_path val_csv_path test_csv_path dst_json_path
```

",3
688,3D-ResNets-PyTorch, HMDB-51,"* Download videos and train/test splits [here](http://crcv.ucf.edu/data/UCF101.php).
* Convert from avi to jpg files using ```utils/video_jpg_ucf101_hmdb51.py```

```bash
python utils/video_jpg_ucf101_hmdb51.py avi_video_directory jpg_video_directory
```

* Generate n_frames files using ```utils/n_frames_ucf101_hmdb51.py```

```bash
python utils/n_frames_ucf101_hmdb51.py jpg_video_directory
```

* Generate annotation file in json format similar to ActivityNet using ```utils/ucf101_json.py```
  * ```annotation_dir_path``` includes classInd.txt, trainlist0{1, 2, 3}.txt, testlist0{1, 2, 3}.txt

```bash
python utils/ucf101_json.py annotation_dir_path
```

",3
689,3D-ResNets-PyTorch, Running the code,"* Download videos and train/test splits [here](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/).
* Convert from avi to jpg files using ```utils/video_jpg_ucf101_hmdb51.py```

```bash
python utils/video_jpg_ucf101_hmdb51.py avi_video_directory jpg_video_directory
```

* Generate n_frames files using ```utils/n_frames_ucf101_hmdb51.py```

```bash
python utils/n_frames_ucf101_hmdb51.py jpg_video_directory
```

* Generate annotation file in json format similar to ActivityNet using ```utils/hmdb51_json.py```
  * ```annotation_dir_path``` includes brush_hair_test_split1.txt, ...

```bash
python utils/hmdb51_json.py annotation_dir_path
```

",2
690,striplog, Of course you don't need this one if you didn't install it yet.,"========

.. image:: https://img.shields.io/travis/agile-geoscience/striplog.svg
    :target: https://travis-ci.org/agile-geoscience/striplog
    :alt: Travis build status
    
.. image:: https://img.shields.io/pypi/status/striplog.svg
    :target: https://pypi.python.org/pypi/striplog/
    :alt: Development status

.. image:: https://img.shields.io/pypi/v/striplog.svg
    :target: https://pypi.python.org/pypi/striplog/
    :alt: Latest version
    
.. image:: https://img.shields.io/pypi/pyversions/striplog.svg
    :target: https://pypi.python.org/pypi/striplog/
    :alt: Python version
    
.. image:: https://coveralls.io/repos/agile-geoscience/striplog/badge.svg?branch=master&service=github
    :target: https://coveralls.io/github/agile-geoscience/striplog?branch=master
    :alt: Coveralls

.. image:: https://img.shields.io/codacy/ad9af103cba14d33abd5b327727ff644.svg 
    :target: https://www.codacy.com/app/matt/striplog/dashboard
    :alt: Codacy code review

.. image:: https://img.shields.io/pypi/l/striplog.svg
    :target: http://www.apache.org/licenses/LICENSE-2.0
    :alt: License

Lithology and stratigraphic logs for wells and outcrop. 

* `A blog post about striplog <http://www.agilegeoscience.com/blog/2015/4/15/striplog>`_
* `Another one, with a video <http://www.agilegeoscience.com/blog/2015/7/10/geophysics-at-scipy-2015>`_


Docs
----

* `Read The Docs <https://striplog.readthedocs.org/>`_


Dependencies
------------

These are best installed with Anaconda, see **Install**, below.

* `NumPy <http://www.numpy.org/>`_
* `matplotlib <http://matplotlib.org/>`_


Install
-------

* ``pip install striplog``

I recommend setting up a virtual environment:

* Install `Anaconda <http://docs.continuum.io/anaconda/install>`_ if you don't have it already
* Then do this to create an environment called ``myenv`` (or whatever you like), answering Yes to the confirmation question::

    conda create -n myenv python=3.5 numpy matplotlib
    source activate myenv

* Then you can do::

    pip install striplog


Development: setting up for testing
-----------

There are other requirements for testing, as listed in ``setup.py``. They should install with::

    python setup.py test

But I had better luck doing ``conda install pytest`` first.

The tests can be run with::

    python run_tests.py


Development: running the bleeding edge
-----------

To run the latest version of the code, you should be on the `develop` branch:

    git clone https://github.com/agile-geoscience/striplog.git
    cd striplog
    git checkout develop
    
You probably want to continue in your virtual environment (see above).

Then I use these commands, which you can join with `;` if you like, to keep the software up to date:

    /usr/bin/yes | pip uninstall striplog     ",1
691,striplog, Or whatever was the last version to build.,"    python setup.py sdist
    pip install dist/striplog-0.6.1.tar.gz    ",1
692,harismuneer-Ultimate-Facebook-Scraper, 🔥 Ultimate Facebook Scrapper ,,1
693,harismuneer-Ultimate-Facebook-Scraper, Note,"[![DOI](https://zenodo.org/badge/145763277.svg)](https://zenodo.org/badge/latestdoi/145763277)

[![Build Status](https://img.shields.io/badge/Build-Passing-brightgreen.svg?style=for-the-badge&logo=appveyor)](#)
[![Open Source Love svg1](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](#)
[![GitHub Forks](https://img.shields.io/github/forks/harismuneer/Ultimate-Facebook-Scraper.svg?style=social&label=Fork&maxAge=2592000)](https://www.github.com/harismuneer/Ultimate-Facebook-Scraper/fork)
[![GitHub Issues](https://img.shields.io/github/issues/harismuneer/Ultimate-Facebook-Scraper.svg?style=flat&label=Issues&maxAge=2592000)](https://www.github.com/harismuneer/Ultimate-Facebook-Scraper/issues)
[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat&label=Contributions&colorA=red&colorB=black	)](#)


A bot which scrapes almost everything about a facebook user's profile including

* uploaded photos
* tagged photos
* videos
* friends list and their profile photos (including Followers, Following, Work Friends, College Friends etc)
* and all public posts/statuses available on the user's timeline.

The best thing about this scraper is that the data is scraped in an organized format so that it can be used for educational/research purpose by researchers. Moreover, this scraper does not use Facebook's Graph API so there are no rate limiting issues as such. 

**This tool is being used by thousands of developers weekly and we are pretty amazed at this response! Thankyou guys!🎉**

For details regarding **citing/referencing** this tool for your research, check the 'Citation' section below.

",2
694,harismuneer-Ultimate-Facebook-Scraper, Sample,"At its core, this tool uses xpaths of **'divs'** to extract data from them. Since Facebook keeps on updating its site frequently and the 'divs' get changed. Consequently, we have to update the divs accordingly to correctly scrape the data. 

The developers of this tool have devoted a lot of time and effort in developing and most importantly maintaining this tool for quite a lot time now. **In order to keep this amazing tool alive, we need support from you geeks.**

The code is pretty intuitive and easy to understand, so you can update the relevant xpaths in the code when you feel that you have tried many profiles and the data isn't being scraped for any of them (that's a hint that Facebook has updated their site) and generate a pull request. That's quite an easy thing to do. Thanks!

",2
695,harismuneer-Ultimate-Facebook-Scraper, Screenshot,"<p align=""middle"">
  <img src=""../master/images/main.png"" width=""700""/>
 </p>


",2
696,harismuneer-Ultimate-Facebook-Scraper, Usage,"<p align=""middle"">
  <img src=""../master/images/screenshot.png"" width=""700""/>
 </p>


----------------------------------------------------------------------------------------------------------------------------------------
",2
697,harismuneer-Ultimate-Facebook-Scraper, Installation,,3
698,harismuneer-Ultimate-Facebook-Scraper, How to Run,"You will need to install latest version of [Google Chrome](https://www.google.com/chrome/). Moreover, you need to install selenium module as well using

```
pip install selenium
```

Run the code using Python 3. Also, the code is multi-platform and is tested on both Windows and Linux.
The tool uses latest version of [Chrome Web Driver](http://chromedriver.chromium.org/downloads). I have placed the webdriver along with the code but if that version doesn't work then replace the chrome web driver with the latest one.

",3
699,harismuneer-Ultimate-Facebook-Scraper, Citation,"There's a file named ""input.txt"". You can add as many profiles as you want in the following format with each link on a new line:

```
https://www.facebook.com/andrew.ng.96
https://www.facebook.com/zuck
```

Make sure the link only contains the username or id number at the end and not any other stuff. Make sure its in the format mentioned above.

Note: There are two modes to download Friends Profile Pics and the user's Photos: Large Size and Small Size. You can change the following variables. By default they are set to Small Sized Pics because its really quick while Large Size Mode takes time depending on the number of pictures to download

```
#notes: whether to download the full image or its thumbnail (small size)
#notes: if small size is True then it will be very quick else if its False then it will open each photo to download it
#notes: and it will take much more time
friends_small_size = True
photos_small_size = True
```
----------------------------------------------------------------------------------------------------------------------------------------

",2
700,harismuneer-Ultimate-Facebook-Scraper, Important Message,"[![DOI](https://zenodo.org/badge/145763277.svg)](https://zenodo.org/badge/latestdoi/145763277)

If you use this tool for your research, then kindly cite it. Click the above badge for more information regarding the complete citation for this tool and diffferent citation formats like IEEE, APA etc.




----------------------------------------------------------------------------------------------------------------------------------------

",2
701,harismuneer-Ultimate-Facebook-Scraper, Authors,"This tool is for research purposes only. Hence, the developers of this tool won't be responsible for any misuse of data collected using this tool. 

----------------------------------------------------------------------------------------------------------------------------------------

",2
702,harismuneer-Ultimate-Facebook-Scraper, Haris Muneer,"You can get in touch with us on our LinkedIn Profiles:

",4
703,harismuneer-Ultimate-Facebook-Scraper, Hassaan Elahi,"[![LinkedIn Link](https://img.shields.io/badge/Connect-harismuneer-blue.svg?logo=linkedin&longCache=true&style=social&label=Connect
)](https://www.linkedin.com/in/harismuneer)

You can also follow my GitHub Profile to stay updated about my latest projects: [![GitHub Follow](https://img.shields.io/badge/Connect-harismuneer-blue.svg?logo=Github&longCache=true&style=social&label=Follow)](https://github.com/harismuneer)

",4
704,harismuneer-Ultimate-Facebook-Scraper, Contributions Welcome,"[![LinkedIn Link](https://img.shields.io/badge/Connect-Hassaan--Elahi-blue.svg?logo=linkedin&longCache=true&style=social&label=Connect)](https://www.linkedin.com/in/hassaan-elahi/)

You can also follow my GitHub Profile to stay updated about my latest projects:[![GitHub Follow](https://img.shields.io/badge/Connect-Hassaan--Elahi-blue.svg?logo=Github&longCache=true&style=social&label=Follow)](https://github.com/Hassaan-Elahi)


If you liked the repo then kindly support it by giving it a star ⭐!

",2
705,harismuneer-Ultimate-Facebook-Scraper, Issues,"[![forthebadge](https://forthebadge.com/images/badges/built-with-love.svg)](#)

If you find any bug in the code or have any improvements in mind then feel free to generate a pull request.

",2
706,harismuneer-Ultimate-Facebook-Scraper, License,"[![GitHub Issues](https://img.shields.io/github/issues/harismuneer/Ultimate-Facebook-Scraper.svg?style=flat&label=Issues&maxAge=2592000)](https://www.github.com/harismuneer/Ultimate-Facebook-Scraper/issues)

If you face any issue, you can create a new issue in the Issues Tab and I will be glad to help you out.

",2
707,nextflow-io-nextflow, Contents,"*""Dataflow variables are spectacularly expressive in concurrent programming""*
<br>[Henri E. Bal , Jennifer G. Steiner , Andrew S. Tanenbaum](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.145.7873)


[![Chat on Gitter](https://img.shields.io/gitter/room/nextflow-io/nextflow.svg?colorB=26af64&style=popout)](https://gitter.im/nextflow-io/nextflow)
[![Nextflow version](https://img.shields.io/github/release/nextflow-io/nextflow.svg?colorB=26af64&style=popout)](https://github.com/nextflow-io/nextflow/releases/latest)
[![Nextflow Twitter](https://img.shields.io/twitter/url/https/nextflowio.svg?colorB=26af64&&label=%40nextflow&style=popout)](https://twitter.com/nextflowio)
[![Nextflow Publication](https://img.shields.io/badge/Published-Nature%20Biotechnology-26af64.svg?colorB=26af64&style=popout)](https://www.nature.com/articles/nbt.3820)
[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?colorB=26af64&style=popout)](http://bioconda.github.io/recipes/nextflow/README.html)
[![Nextflow license](https://img.shields.io/github/license/nextflow-io/nextflow.svg?colorB=26af64&style=popout)](https://github.com/nextflow-io/nextflow/blob/master/COPYING)

Quick overview
==============
Nextflow is a bioinformatics workflow manager that enables the development of portable and reproducible workflows.
It supports deploying workflows on a variety of execution platforms including local, HPC schedulers, AWS Batch,
Google Genomics Pipelines, and Kubernetes. Additionally, it provides support for manage your workflow dependencies
through built-in support for Conda, Docker, Singularity, and Modules.

",2
708,GANimation, GANimation: Anatomically-aware Facial Animation from a Single Image,,1
709,GANimation, [[Project]](http://www.albertpumarola.com/research/GANimation/index.html)[ [Paper]](https://arxiv.org/abs/1807.09251) ,,3
710,GANimation, Prerequisites,"Official implementation of [GANimation](http://www.albertpumarola.com/research/GANimation/index.html). In this work we introduce a novel GAN conditioning scheme based on Action Units (AU) annotations, which describe in a continuous manifold the anatomical facial movements defining a human expression. Our approach permits controlling the magnitude of activation of each AU and combine several of them. For more information please refer to the [paper](https://arxiv.org/abs/1807.09251).

This code was made public to share our research for the benefit of the scientific community. Do NOT use it for immoral purposes.

![GANimation](http://www.albertpumarola.com/images/2018/GANimation/teaser.png)

",2
711,GANimation, Data Preparation,"- Install PyTorch (version 0.3.1), Torch Vision and dependencies from http://pytorch.org
- Install requirements.txt (```pip install -r requirements.txt```)

",2
712,GANimation, Run,"The code requires a directory containing the following files:
- `imgs/`: folder with all image
- `aus_openface.pkl`: dictionary containing the images action units.
- `train_ids.csv`: file containing the images names to be used to train.
- `test_ids.csv`: file containing the images names to be used to test.

An example of this directory is shown in `sample_dataset/`.

To generate the `aus_openface.pkl` extract each image Action Units with [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units) and store each output in a csv file the same name as the image. Then run:
```
python data/prepare_au_annotations.py
```

",2
713,GANimation, Citation,"To train:
```
bash launch/run_train.sh
```
To test:
```
python test --input_path path/to/img
```

",2
714,two-stream-dyntex-synth, Two-Stream Convolutional Networks for Dynamic Texture Synthesis,,1
715,two-stream-dyntex-synth, Requirements,"![Dynamic texture synthesis](teaser.gif ""Dynamic texture synthesis"")

",2
716,two-stream-dyntex-synth, Setup,"- Tensorflow 1.3 (or latest, although not tested)
- Preferably a Titan X for synthesizing 12 frames
- Appearance-stream [tfmodel](https://drive.google.com/open?id=19KkFi92oWLzuOWnGo6Zsqe-2CCXFAoXZ)
- Dynamics-stream [tfmodel](https://drive.google.com/open?id=1DHnzoNO-iTgMUTbUOLrigEmpPHmn_mT1)
- [Dynamic textures](https://drive.google.com/open?id=0B5T9jWfa9iDySWJHZnpNZ2dHWUk)
- [Static textures](https://drive.google.com/open?id=11yMiPXiuYvLCyoLfQf_dEG6kuav8h6_3) (for dynamics style transfer)

",2
717,two-stream-dyntex-synth, Dynamic texture synthesis,"1. Store the appearance-stream tfmodel in `./models`.
2. Store the dynamics-stream tfmodel in `./models`. The filepath to this model is your `--dynamics_model` path.

",2
718,two-stream-dyntex-synth, Example usage,"```
python synthesize.py --type=dts --gpu=<NUMBER> --runid=<NAME> --dynamics_target=data/dynamic_textures/<FOLDER> --dynamics_model=models/<TFMODEL>
```

Store your chosen dynamic texture image sequence in a folder in `/data/dynamic_textures`. This folder is your `--dynamics_target` path.

",4
719,two-stream-dyntex-synth, Dynamics style transfer,"```
python synthesize.py --type=dts --gpu=0 --runid=""my_cool_fish"" --dynamics_target=data/dynamic_textures/fish --dynamics_model=models/MSOEnet_ucf101train01_6e-4_allaug_exceptscale_randorder.tfmodel
```

",2
720,two-stream-dyntex-synth, Example usage,"```
python synthesize.py --type=dst --gpu=<NUMBER> --runid=<NAME> --dynamics_target=data/dynamic_textures/<FOLDER> --dynamics_model=models/<TFMODEL> --appearance_target=data/textures/<IMAGE>
```

Store your chosen static texture in `./data/textures`. The filepath to this texture is your `--appearance_target` path.

",4
721,two-stream-dyntex-synth, Temporally-endless dynamic texture synthesis,"```
python synthesize.py --type=dst --gpu=0 --runid=""whoa_water!"" --dynamics_target=data/dynamic_textures/water_4 --appearance_target=data/textures/water_paint_cropped.jpeg --dynamics_model=models/MSOEnet_ucf101train01_6e-4_allaug_exceptscale_randorder.tfmodel
```

",2
722,two-stream-dyntex-synth, Incremental dynamic texture synthesis,"```
python synthesize.py --type=inf --gpu=<NUMBER> --runid=<NAME> --dynamics_target=data/dynamic_textures/<FOLDER> --dynamics_model=models/<TFMODEL>
```

",2
723,two-stream-dyntex-synth, Static texture synthesis,"```
python synthesize.py --type=inc --gpu=<NUMBER> --runid=<NAME> --dynamics_target=data/dynamic_textures/<FOLDER> --dynamics_model=models/<TFMODEL> --appearance_target=data/textures/<IMAGE>
```

Store your chosen static texture in `/data/textures`. The filepath to this texture is your `--appearance_target` path. This texture should be the last frame of a previously generated sequence.

",2
724,two-stream-dyntex-synth, Notes,"```
python synthesize.py --type=sta --gpu=<NUMBER> --runid=<NAME> --appearance_target=data/textures/<IMAGE>
```

[Gatys et al.'s](https://arxiv.org/abs/1505.07376) method of texture synthesis.

",2
725,two-stream-dyntex-synth, Citation,"The network's output is saved at `data/out/<RUNID>`.

Use `./useful_scripts/makegif.sh` to create a gif from a folder of images, e.g.,
```
./useful_scripts/makegif.sh ""data/out/calm_water/iter_6000*"" calm_water.gif
```
will create the gif `calm_water.gif` from the images `iter_6000*` in the `calm_water` output folder.

Logs and snapshots are created and stored in `./logs/<RUNID>` and `./snapshots/<RUNID>`, respectively. You can view the loss progress for a particular run in Tensorboard.

",2
726,two-stream-dyntex-synth, License,"```
@inproceedings{tesfaldet2018,
  author = {Matthew Tesfaldet and Marcus A. Brubaker and Konstantinos G. Derpanis},
  title = {Two-Stream Convolutional Networks for Dynamic Texture Synthesis},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2018}
}
```

",2
727,ipyleaflet, ipyleaflet,,1
728,ipyleaflet, Usage,"[![Documentation](http://readthedocs.org/projects/ipyleaflet/badge/?version=latest)](https://ipyleaflet.readthedocs.io/en/latest/?badge=latest)
[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jupyter-widgets/ipyleaflet/stable?filepath=examples)
[![Join the Gitter Chat](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/jupyter-widgets/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

A Jupyter / Leaflet bridge enabling interactive maps in the Jupyter notebook.

",2
729,ipyleaflet, Installation,"**Selecting a basemap for a leaflet map:**

![Basemap Screencast](basemap.gif)

**Loading a geojson map:**

![GeoJSON Screencast](geojson.gif)

**Making use of leafletjs primitives:**

![Primitives Screencast](primitives.gif)

**Using the splitmap control:**

![Splitmap Screencast](splitmap.gif)

**Displaying velocity data on the top of a map:**

![Velocity Screencast](velocity.gif)

**Choropleth layer:**

![Choropleth Screencast](choropleth.gif)

",2
730,ipyleaflet, Installation from sources,"Using conda:

```
$ conda install -c conda-forge ipyleaflet
```

Using pip:

```
$ pip install ipyleaflet
$ jupyter nbextension enable --py --sys-prefix ipyleaflet  #notes: can be skipped for
notebook 5.3 and above
```

If you have JupyterLab, you will also need to install the JupyterLab extension:

```
$ jupyter labextension install jupyter-leaflet
```

Some users have found that the ``jupyterlab-manager`` is also required
in jupyterlab if the map does not display.

```
$ jupyter labextension install @jupyter-widgets/jupyterlab-manager
```

",2
731,ipyleaflet, Documentation,"For a development installation (requires npm):

```
$ git clone https://github.com/jupyter-widgets/ipyleaflet.git
$ cd ipyleaflet
$ pip install -e .
$ jupyter nbextension install --py --symlink --sys-prefix ipyleaflet
$ jupyter nbextension enable --py --sys-prefix ipyleaflet
$ jupyter labextension install js  #notes: If you are developing on JupyterLab
```

Note for developers:

- the ``-e`` pip option allows one to modify the Python code in-place. Restart the kernel in order to see the changes.
- the ``--symlink`` argument on Linux or OS X allows one to modify the JavaScript code in-place. This feature is not available with Windows.

    For automatically building the JavaScript code every time there is a change, run the following command from the ``ipyleaflet/js/`` directory:

    ```
    $ npm run watch
    ```

    If you are on JupyterLab you also need to run the following in a separate terminal:

    ```
    $ jupyter lab --watch
    ```

    Every time a JavaScript build has terminated you need to refresh the Notebook page in order to load the JavaScript code again.

",2
732,ipyleaflet, License,"To get started with using `ipyleaflet`, check out the full documentation

https://ipyleaflet.readthedocs.io/

",2
733,ipyleaflet, Related projects,"We use a shared copyright model that enables all contributors to maintain the
copyright on their contributions.

This software is licensed under the BSD-3-Clause license. See the [LICENSE](LICENSE) file for details.

",2
734,rasterio, Read raster bands directly to Numpy arrays.,"Rasterio
========

Rasterio reads and writes geospatial raster data.

.. image:: https://travis-ci.org/mapbox/rasterio.png?branch=master
   :target: https://travis-ci.org/mapbox/rasterio

.. image:: https://coveralls.io/repos/github/mapbox/rasterio/badge.svg?branch=master
   :target: https://coveralls.io/github/mapbox/rasterio?branch=master

Geographic information systems use GeoTIFF and other formats to organize and
store gridded, or raster, datasets. Rasterio reads and writes these formats and
provides a Python API based on N-D arrays.

Rasterio 1.0.x works with Python versions 2.7.x and 3.5.0 through 3.7.x, and GDAL
versions 1.11.x through 2.4.x. Official binary packages for Linux and Mac OS X are
available on PyPI. Unofficial binary packages for Windows are available through other
channels.

Rasterio 1.0.x is not compatible with GDAL versions 3.0.0 or greater.

Read the documentation for more details: https://rasterio.readthedocs.io/.

Example
=======

Here's an example of some basic features that Rasterio provides. Three bands
are read from an image and averaged to produce something like a panchromatic
band.  This new band is then written to a new single band TIFF.

.. code-block:: python

    import numpy as np
    import rasterio

    ",1
735,rasterio, Combine arrays in place. Expecting that the sum will,"    #
    with rasterio.open('tests/data/RGB.byte.tif') as src:
        r, g, b = src.read()

    ",1
736,rasterio," temporarily exceed the 8-bit integer range, initialize it as",    ,1
737,rasterio, a 64-bit float (the numpy default) array. Adding other,    ,1
738,rasterio," arrays to it in-place converts those arrays ""up"" and",    ,1
739,rasterio, preserves the type of the total array.,    ,1
740,rasterio, Write the product as a raster band to a new 8-bit file. For,"    total = np.zeros(r.shape)
    for band in r, g, b:
        total += band
    total /= 3

    ",1
741,rasterio," the new file's profile, we start with the meta attributes of",    ,1
742,rasterio," the source file, but then change the band count to 1, set the",    ,1
743,rasterio," dtype to uint8, and specify LZW compression.",    ,1
744,rasterio, Printed:,"    profile = src.profile
    profile.update(dtype=rasterio.uint8, count=1, compress='lzw')

    with rasterio.open('example-total.tif', 'w', **profile) as dst:
        dst.write(total.astype(rasterio.uint8), 1)

The output:

.. image:: http://farm6.staticflickr.com/5501/11393054644_74f54484d9_z_d.jpg
   :width: 640
   :height: 581

API Overview
============

Rasterio gives access to properties of a geospatial raster file.

.. code-block:: python

    with rasterio.open('tests/data/RGB.byte.tif') as src:
        print(src.width, src.height)
        print(src.crs)
        print(src.transform)
        print(src.count)
        print(src.indexes)

    ",1
745,rasterio," (791, 718)",    ,1
746,rasterio," {u'units': u'm', u'no_defs': True, u'ellps': u'WGS84', u'proj': u'utm', u'zone': 18}",    ,1
747,rasterio," Affine(300.0379266750948, 0.0, 101985.0,",    ,1
748,rasterio,"        0.0, -300.041782729805, 2826915.0)",    ,1
749,rasterio, 3,    ,1
750,rasterio," [1, 2, 3]",    ,1
751,rasterio, Printed:,"A rasterio dataset also provides methods for getting extended array slices given
georeferenced coordinates.


.. code-block:: python

    with rasterio.open('tests/data/RGB.byte.tif') as src:
        print src.window(**src.window_bounds(((100, 200), (100, 200))))

    ",1
752,rasterio," ((100, 200), (100, 200))",    ,1
753,puppeteer, Puppeteer,,1
754,puppeteer, [API](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md) | [FAQ](faq) | [Contributing](https://github.com/GoogleChrome/puppeteer/blob/master/CONTRIBUTING.md) | [Troubleshooting](https://github.com/GoogleChrome/puppeteer/blob/master/docs/troubleshooting.md),"<!-- [START badges] -->
[![Linux Build Status](https://img.shields.io/travis/com/GoogleChrome/puppeteer/master.svg)](https://travis-ci.com/GoogleChrome/puppeteer) [![Windows Build Status](https://img.shields.io/appveyor/ci/aslushnikov/puppeteer/master.svg?logo=appveyor)](https://ci.appveyor.com/project/aslushnikov/puppeteer/branch/master) [![Build Status](https://api.cirrus-ci.com/github/GoogleChrome/puppeteer.svg)](https://cirrus-ci.com/github/GoogleChrome/puppeteer) [![NPM puppeteer package](https://img.shields.io/npm/v/puppeteer.svg)](https://npmjs.org/package/puppeteer)
<!-- [END badges] -->

<img src=""https://user-images.githubusercontent.com/10379601/29446482-04f7036a-841f-11e7-9872-91d1fc2ea683.png"" height=""200"" align=""right"">

#",6
755,puppeteer, What can I do?,"> Puppeteer is a Node library which provides a high-level API to control Chrome or Chromium over the [DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/). Puppeteer runs [headless](https://developers.google.com/web/updates/2017/04/headless-chrome) by default, but can be configured to run full (non-headless) Chrome or Chromium.

<!-- [START usecases] -->
#",5
756,puppeteer, Getting Started,"Most things that you can do manually in the browser can be done using Puppeteer! Here are a few examples to get you started:

* Generate screenshots and PDFs of pages.
* Crawl a SPA (Single-Page Application) and generate pre-rendered content (i.e. ""SSR"" (Server-Side Rendering)).
* Automate form submission, UI testing, keyboard input, etc.
* Create an up-to-date, automated testing environment. Run your tests directly in the latest version of Chrome using the latest JavaScript and browser features.
* Capture a [timeline trace](https://developers.google.com/web/tools/chrome-devtools/evaluate-performance/reference) of your site to help diagnose performance issues.
* Test Chrome Extensions.
<!-- [END usecases] -->

Give it a spin: https://try-puppeteer.appspot.com/

<!-- [START getstarted] -->
",2
757,puppeteer, Installation,,3
758,puppeteer, puppeteer-core,"To use Puppeteer in your project, run:

```bash
npm i puppeteer
#notes: or ""yarn add puppeteer""
```

Note: When you install Puppeteer, it downloads a recent version of Chromium (~170MB Mac, ~282MB Linux, ~280MB Win) that is guaranteed to work with the API. To skip the download, see [Environment variables](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#environment-variables).


",3
759,puppeteer, Usage,"Since version 1.7.0 we publish the [`puppeteer-core`](https://www.npmjs.com/package/puppeteer-core) package,
a version of Puppeteer that doesn't download Chromium by default.

```bash
npm i puppeteer-core
#notes: or ""yarn add puppeteer-core""
```

`puppeteer-core` is intended to be a lightweight version of Puppeteer for launching an existing browser installation or for connecting to a remote one. Be sure that the version of puppeteer-core you install is compatible with the
browser you intend to connect to.

See [puppeteer vs puppeteer-core](https://github.com/GoogleChrome/puppeteer/blob/master/docs/api.md#puppeteer-vs-puppeteer-core).

",3
760,puppeteer, Default runtime settings,"Note: Puppeteer requires at least Node v6.4.0, but the examples below use async/await which is only supported in Node v7.6.0 or greater.

Puppeteer will be familiar to people using other browser testing frameworks. You create an instance
of `Browser`, open pages, and then manipulate them with [Puppeteer's API](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#).

**Example** - navigating to https://example.com and saving a screenshot as *example.png*:

Save file as **example.js**

```js
const puppeteer = require('puppeteer');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.goto('https://example.com');
  await page.screenshot({path: 'example.png'});

  await browser.close();
})();
```

Execute script on the command line

```bash
node example.js
```

Puppeteer sets an initial page size to 800px x 600px, which defines the screenshot size. The page size can be customized  with [`Page.setViewport()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#pagesetviewportviewport).

**Example** - create a PDF.

Save file as **hn.js**

```js
const puppeteer = require('puppeteer');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.goto('https://news.ycombinator.com', {waitUntil: 'networkidle2'});
  await page.pdf({path: 'hn.pdf', format: 'A4'});

  await browser.close();
})();
```

Execute script on the command line

```bash
node hn.js
```

See [`Page.pdf()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#pagepdfoptions) for more information about creating pdfs.

**Example** - evaluate script in the context of the page

Save file as **get-dimensions.js**

```js
const puppeteer = require('puppeteer');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.goto('https://example.com');

  // Get the ""viewport"" of the page, as reported by the page.
  const dimensions = await page.evaluate(() => {
    return {
      width: document.documentElement.clientWidth,
      height: document.documentElement.clientHeight,
      deviceScaleFactor: window.devicePixelRatio
    };
  });

  console.log('Dimensions:', dimensions);

  await browser.close();
})();
```

Execute script on the command line

```bash
node get-dimensions.js
```

See [`Page.evaluate()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#pageevaluatepagefunction-args) for more information on `evaluate` and related methods like `evaluateOnNewDocument` and `exposeFunction`.

<!-- [END getstarted] -->

<!-- [START runtimesettings] -->
",2
761,puppeteer, Resources,"**1. Uses Headless mode**

Puppeteer launches Chromium in [headless mode](https://developers.google.com/web/updates/2017/04/headless-chrome). To launch a full version of Chromium, set the ['headless' option](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#puppeteerlaunchoptions) when launching a browser:

```js
const browser = await puppeteer.launch({headless: false}); // default is true
```

**2. Runs a bundled version of Chromium**

By default, Puppeteer downloads and uses a specific version of Chromium so its API
is guaranteed to work out of the box. To use Puppeteer with a different version of Chrome or Chromium,
pass in the executable's path when creating a `Browser` instance:

```js
const browser = await puppeteer.launch({executablePath: '/path/to/Chrome'});
```

See [`Puppeteer.launch()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#puppeteerlaunchoptions) for more information.

See [`this article`](https://www.howtogeek.com/202825/what%E2%80%99s-the-difference-between-chromium-and-chrome/) for a description of the differences between Chromium and Chrome. [`This article`](https://chromium.googlesource.com/chromium/src/+/master/docs/chromium_browser_vs_google_chrome.md) describes some differences for Linux users.

**3. Creates a fresh user profile**

Puppeteer creates its own Chromium user profile which it **cleans up on every run**.

<!-- [END runtimesettings] -->

",2
762,puppeteer, Debugging tips,"- [API Documentation](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md)
- [Examples](https://github.com/GoogleChrome/puppeteer/tree/master/examples/)
- [Community list of Puppeteer resources](https://github.com/transitive-bullshit/awesome-puppeteer)


<!-- [START debugging] -->

",2
763,puppeteer, Basic verbose logging,"1. Turn off headless mode - sometimes it's useful to see what the browser is
   displaying. Instead of launching in headless mode, launch a full version of
   the browser using  `headless: false`:

        const browser = await puppeteer.launch({headless: false});

2. Slow it down - the `slowMo` option slows down Puppeteer operations by the
   specified amount of milliseconds. It's another way to help see what's going on.

        const browser = await puppeteer.launch({
          headless: false,
          slowMo: 250 // slow down by 250ms
        });

3. Capture console output - You can listen for the `console` event.
   This is also handy when debugging code in `page.evaluate()`:

        page.on('console', msg => console.log('PAGE LOG:', msg.text()));

        await page.evaluate(() => console.log(`url is ${location.href}`));

4. Use debugger in application code browser

    There are two execution context: node.js that is running test code, and the browser
    running application code being tested. This lets you debug code in the
    application code browser; ie code inside `evaluate()`.

    - Use `{devtools: true}` when launching Puppeteer:

        `const browser = await puppeteer.launch({devtools: true});`

    - Change default test timeout:

        jest: `jest.setTimeout(100000);`

        jasmine: `jasmine.DEFAULT_TIMEOUT_INTERVAL = 100000;`

        mocha: `this.timeout(100000);` (don't forget to change test to use [function and not '=>'](https://stackoverflow.com/a/23492442))

    - Add an evaluate statement with `debugger` inside / add  `debugger` to an existing evaluate statement:

      `await page.evaluate(() => {debugger;});`

       The test will now stop executing in the above evaluate statement, and chromium will stop in debug mode.

5. Use debugger in node.js

    This will let you debug test code. For example, you can step over `await page.click()` in the node.js script and see the click happen in the application code browser.

    Note that you won't be able to run `await page.click()` in
    DevTools console due to this [Chromium bug](https://bugs.chromium.org/p/chromium/issues/detail?id=833928). So if
    you want to try something out, you have to add it to your test file.

    - Add `debugger;` to your test, eg:
      ```
      debugger;
      await page.click('a[target=_blank]');
      ```
    - Set `headless` to `false`
    - Run `node --inspect-brk`, eg `node --inspect-brk node_modules/.bin/jest tests`
    - In Chrome open `chrome://inspect/#devices` and click `inspect`
    - In the newly opened test browser, type `F8` to resume test execution
    - Now your `debugger` will be hit and you can debug in the test browser


6. Enable verbose logging - internal DevTools protocol traffic
   will be logged via the [`debug`](https://github.com/visionmedia/debug) module under the `puppeteer` namespace.

        ",1
764,puppeteer, Protocol traffic can be rather noisy. This example filters out all Network domain messages,"        env DEBUG=""puppeteer:*"" node script.js

        ",1
765,puppeteer, Contributing to Puppeteer,"        env DEBUG=""puppeteer:*"" env DEBUG_COLORS=true node script.js 2>&1 | grep -v '""Network'

7. Debug your Puppeteer (node) code easily, using [ndb](https://github.com/GoogleChromeLabs/ndb)

  - `npm install -g ndb` (or even better, use [npx](https://github.com/zkat/npx)!)

  - add a `debugger` to your Puppeteer (node) code

  - add `ndb` (or `npx ndb`) before your test command. For example:

    `ndb jest` or `ndb mocha` (or `npx ndb jest` / `npx ndb mocha`)

  - debug your test inside chromium like a boss!


<!-- [END debugging] -->

",2
766,puppeteer, FAQ,"Check out [contributing guide](https://github.com/GoogleChrome/puppeteer/blob/master/CONTRIBUTING.md) to get an overview of Puppeteer development.

<!-- [START faq] -->

",1
767,puppeteer, Q: Who maintains Puppeteer?,,4
768,puppeteer, Q: What are Puppeteer’s goals and principles?,"The Chrome DevTools team maintains the library, but we'd love your help and expertise on the project!
See [Contributing](https://github.com/GoogleChrome/puppeteer/blob/master/CONTRIBUTING.md).

",4
769,puppeteer, Q: Is Puppeteer replacing Selenium/WebDriver?,"The goals of the project are:

- Provide a slim, canonical library that highlights the capabilities of the [DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/).
- Provide a reference implementation for similar testing libraries. Eventually, these other frameworks could adopt Puppeteer as their foundational layer.
- Grow the adoption of headless/automated browser testing.
- Help dogfood new DevTools Protocol features...and catch bugs!
- Learn more about the pain points of automated browser testing and help fill those gaps.

We adapt [Chromium principles](https://www.chromium.org/developers/core-principles) to help us drive product decisions:
- **Speed**: Puppeteer has almost zero performance overhead over an automated page.
- **Security**: Puppeteer operates off-process with respect to Chromium, making it safe to automate potentially malicious pages.
- **Stability**: Puppeteer should not be flaky and should not leak memory.
- **Simplicity**: Puppeteer provides a high-level API that’s easy to use, understand, and debug.

",4
770,puppeteer, Q: Why doesn’t Puppeteer v.XXX work with Chromium v.YYY?,"**No**. Both projects are valuable for very different reasons:
- Selenium/WebDriver focuses on cross-browser automation; its value proposition is a single standard API that works across all major browsers.
- Puppeteer focuses on Chromium; its value proposition is richer functionality and higher reliability.

That said, you **can** use Puppeteer to run tests against Chromium, e.g. using the community-driven [jest-puppeteer](https://github.com/smooth-code/jest-puppeteer). While this probably shouldn’t be your only testing solution, it does have a few good points compared to WebDriver:

- Puppeteer requires zero setup and comes bundled with the Chromium version it works best with, making it [very easy to start with](https://github.com/GoogleChrome/puppeteer/#getting-started). At the end of the day, it’s better to have a few tests running chromium-only, than no tests at all.
- Puppeteer has event-driven architecture, which removes a lot of potential flakiness. There’s no need for evil “sleep(1000)” calls in puppeteer scripts.
- Puppeteer runs headless by default, which makes it fast to run. Puppeteer v1.5.0 also exposes browser contexts, making it possible to efficiently parallelize test execution.
- Puppeteer shines when it comes to debugging: flip the “headless” bit to false, add “slowMo”, and you’ll see what the browser is doing. You can even open Chrome DevTools to inspect the test environment.

",4
771,puppeteer, Q: Which Chromium version does Puppeteer use?,"We see Puppeteer as an **indivisible entity** with Chromium. Each version of Puppeteer bundles a specific version of Chromium – **the only** version it is guaranteed to work with.

This is not an artificial constraint: A lot of work on Puppeteer is actually taking place in the Chromium repository. Here’s a typical story:
- A Puppeteer bug is reported: https://github.com/GoogleChrome/puppeteer/issues/2709
- It turned out this is an issue with the DevTools protocol, so we’re fixing it in Chromium: https://chromium-review.googlesource.com/c/chromium/src/+/1102154
- Once the upstream fix is landed, we roll updated Chromium into Puppeteer: https://github.com/GoogleChrome/puppeteer/pull/2769

However, oftentimes it is desirable to use Puppeteer with the official Google Chrome rather than Chromium. For this to work, you should install a `puppeteer-core` version that corresponds to the Chrome version.

For example, in order to drive Chrome 71 with puppeteer-core, use `chrome-71` npm tag:
```bash
npm install puppeteer-core@chrome-71
```

",4
772,puppeteer, Q: What’s considered a “Navigation”?,"Look for `chromium_revision` in [package.json](https://github.com/GoogleChrome/puppeteer/blob/master/package.json).

",4
773,puppeteer," Q: What’s the difference between a “trusted"" and ""untrusted"" input event?","From Puppeteer’s standpoint, **“navigation” is anything that changes a page’s URL**.
Aside from regular navigation where the browser hits the network to fetch a new document from the web server, this includes [anchor navigations](https://www.w3.org/TR/html5/single-page.html#scroll-to-fragid) and [History API](https://developer.mozilla.org/en-US/docs/Web/API/History_API) usage.

With this definition of “navigation,” **Puppeteer works seamlessly with single-page applications.**

",4
774,puppeteer, Q: What features does Puppeteer not support?,"In browsers, input events could be divided into two big groups: trusted vs. untrusted.

- **Trusted events**: events generated by users interacting with the page, e.g. using a mouse or keyboard.
- **Untrusted event**: events generated by Web APIs, e.g. `document.createEvent` or `element.click()` methods.

Websites can distinguish between these two groups:
- using an [`Event.isTrusted`](https://developer.mozilla.org/en-US/docs/Web/API/Event/isTrusted) event flag
- sniffing for accompanying events. For example, every trusted `'click'` event is preceded by `'mousedown'` and `'mouseup'` events.

For automation purposes it’s important to generate trusted events. **All input events generated with Puppeteer are trusted and fire proper accompanying events.** If, for some reason, one needs an untrusted event, it’s always possible to hop into a page context with `page.evaluate` and generate a fake event:

```js
await page.evaluate(() => {
  document.querySelector('button[type=submit]').click();
});
```

",4
775,puppeteer, Q: I am having trouble installing / running Puppeteer in my test environment. Where should I look for help?,"You may find that Puppeteer does not behave as expected when controlling pages that incorporate audio and video. (For example, [video playback/screenshots is likely to fail](https://github.com/GoogleChrome/puppeteer/issues/291).) There are two reasons for this:

* Puppeteer is bundled with Chromium--not Chrome--and so by default, it inherits all of [Chromium's media-related limitations](https://www.chromium.org/audio-video). This means that Puppeteer does not support licensed formats such as AAC or H.264. (However, it is possible to force Puppeteer to use a separately-installed version Chrome instead of Chromium via the [`executablePath` option to `puppeteer.launch`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#puppeteerlaunchoptions). You should only use this configuration if you need an official release of Chrome that supports these media formats.)
* Since Puppeteer (in all configurations) controls a desktop version of Chromium/Chrome, features that are only supported by the mobile version of Chrome are not supported. This means that Puppeteer [does not support HTTP Live Streaming (HLS)](https://caniuse.com/#feat=http-live-streaming).

",4
776,puppeteer, Q: How do I try/test a prerelease version of Puppeteer?,"We have a [troubleshooting](https://github.com/GoogleChrome/puppeteer/blob/master/docs/troubleshooting.md) guide for various operating systems that lists the required dependencies.

",4
777,puppeteer, Q: I have more questions! Where do I ask?,"You can check out this repo or install the latest prerelease from npm:

```bash
npm i --save puppeteer@next
```

Please note that prerelease may be unstable and contain bugs.

",4
778,tensorflow, Installation,"  <img src=""https://www.tensorflow.org/images/tf_logo_social.png"">
</div>

-----------------


| **`Documentation`** |
|-----------------|
| [![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |

**TensorFlow** is an open source software library for numerical computation
using data flow graphs. The graph nodes represent mathematical operations, while
the graph edges represent the multidimensional data arrays (tensors) that flow
between them. This flexible architecture enables you to deploy computation to
one or more CPUs or GPUs in a desktop, server, or mobile device without
rewriting code. TensorFlow also includes
[TensorBoard](https://github.com/tensorflow/tensorboard), a data visualization
toolkit.

TensorFlow was originally developed by researchers and engineers
working on the Google Brain team within Google's Machine Intelligence Research
organization for the purposes of conducting machine learning and deep neural
networks research.  The system is general enough to be applicable in a wide
variety of other domains, as well.

TensorFlow provides stable Python and C APIs as well as non-guaranteed backwards
compatible API's for C++, Go, Java, JavaScript, and Swift.

Keep up to date with release announcements and security updates by
subscribing to
[announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).

",2
779,tensorflow, *Try your first TensorFlow program*,"To install the current release for CPU-only:

```
pip install tensorflow
```

Use the GPU package for CUDA-enabled GPU cards:

```
pip install tensorflow-gpu
```

*See [Installing TensorFlow](https://www.tensorflow.org/install) for detailed
instructions, and how to build from source.*

People who are a little more adventurous can also try our nightly binaries:

**Nightly pip packages** * We are pleased to announce that TensorFlow now offers
nightly pip packages under the
[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and
[tf-nightly-gpu](https://pypi.python.org/pypi/tf-nightly-gpu) project on PyPi.
Simply run `pip install tf-nightly` or `pip install tf-nightly-gpu` in a clean
environment to install the nightly TensorFlow build. We support CPU and GPU
packages on Linux, Mac, and Windows.

",4
780,tensorflow, Contribution guidelines,"```shell
$ python
```

```python
>>> import tensorflow as tf
>>> tf.enable_eager_execution()
>>> tf.add(1, 2).numpy()
3
>>> hello = tf.constant('Hello, TensorFlow!')
>>> hello.numpy()
'Hello, TensorFlow!'
```

Learn more examples about how to do specific tasks in TensorFlow at the
[tutorials page of tensorflow.org](https://www.tensorflow.org/tutorials/).

",2
781,tensorflow, Continuous build status,"**If you want to contribute to TensorFlow, be sure to review the [contribution
guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's
[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to
uphold this code.**

**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for
tracking requests and bugs, please see
[TensorFlow Discuss](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)
for general questions and discussion, and please direct specific questions to
[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**

The TensorFlow project strives to abide by generally accepted best practices in open-source software development:

[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)

",2
782,tensorflow, Official Builds,,3
783,tensorflow, Community Supported Builds,"| Build Type      | Status | Artifacts |
| ---             | ---    | ---       |
| **Linux CPU**   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html) | [pypi](https://pypi.org/project/tf-nightly/) |
| **Linux GPU**   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html) | [pypi](https://pypi.org/project/tf-nightly-gpu/) |
| **Linux XLA**   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html) | TBA |
| **MacOS**       | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html) | [pypi](https://pypi.org/project/tf-nightly/) |
| **Windows CPU** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html) | [pypi](https://pypi.org/project/tf-nightly/) |
| **Windows GPU** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html) | [pypi](https://pypi.org/project/tf-nightly-gpu/) |
| **Android**     | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html) | [![Download](https://api.bintray.com/packages/google/tensorflow/tensorflow/images/download.svg)](https://bintray.com/google/tensorflow/tensorflow/_latestVersion) |
| **Raspberry Pi 0 and 1** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py2.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py2.html) [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html) | [Py2](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp27-none-linux_armv6l.whl) [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl) |
| **Raspberry Pi 2 and 3** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py2.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py2.html) [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html) | [Py2](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp27-none-linux_armv7l.whl) [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl) |


",3
784,tensorflow, For more information,"Build Type                                                                        | Status                                                                                                                                                                                        | Artifacts
--------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------
**Linux s390x** Nightly                                                           | [![Build Status](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/badge/icon)](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/)                                                             | [Nightly](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/)
**Linux s390x CPU** Stable Release                                                | [![Build Status](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/badge/icon)](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/)                                      | [Release](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/)
**Linux ppc64le CPU** Nightly                                                     | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/)                                       | [Nightly](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Nightly_Artifact/)
**Linux ppc64le CPU** Stable Release                                              | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/)                       | [Release](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/)
**Linux ppc64le GPU** Nightly                                                     | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/)                                       | [Nightly](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Nightly_Artifact/)
**Linux ppc64le GPU** Stable Release                                              | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/)                       | [Release](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/)
**Linux CPU with Intel® MKL-DNN** Nightly                                         | [![Build Status](https://tensorflow-ci.intel.com/job/tensorflow-mkl-linux-cpu/badge/icon)](https://tensorflow-ci.intel.com/job/tensorflow-mkl-linux-cpu/)                                     | [Nightly](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/)
**Linux CPU with Intel® MKL-DNN** <br> **Supports Python 2.7, 3.4, 3.5, and 3.6** | [![Build Status](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-release-whl/badge/icon)](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-release-whl/lastStableBuild)      | [1.13.1 pypi](https://pypi.org/project/intel-tensorflow/)
**Red Hat® Enterprise Linux® 7.6 CPU & GPU** <br> Python 2.7, 3.6                 | [![Build Status](https://jenkins-tensorflow.apps.ci.centos.org/buildStatus/icon?job=tensorflow-rhel7-3.6&build=2)](https://jenkins-tensorflow.apps.ci.centos.org/job/tensorflow-rhel7-3.6/2/) | [1.13.1 pypi](https://tensorflow.pypi.thoth-station.ninja/index/)

",2
785,tensorflow, License,"*   [TensorFlow Website](https://www.tensorflow.org)
*   [TensorFlow Tutorials](https://www.tensorflow.org/tutorials/)
*   [TensorFlow Model Zoo](https://github.com/tensorflow/models)
*   [TensorFlow Twitter](https://twitter.com/tensorflow)
*   [TensorFlow Blog](https://medium.com/tensorflow)
*   [TensorFlow Course at Stanford](https://web.stanford.edu/class/cs20si)
*   [TensorFlow Roadmap](https://www.tensorflow.org/community/roadmap)
*   [TensorFlow White Papers](https://www.tensorflow.org/about/bib)
*   [TensorFlow YouTube Channel](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ)
*   [TensorFlow Visualization Toolkit](https://github.com/tensorflow/tensorboard)

Learn more about the TensorFlow community at the [community page of tensorflow.org](https://www.tensorflow.org/community) for a few ways to participate.

",2
786,vue, What's the difference between Patreon and OpenCollective?,"<p align=""center"">
  <a href=""https://circleci.com/gh/vuejs/vue/tree/dev""><img src=""https://img.shields.io/circleci/project/github/vuejs/vue/dev.svg"" alt=""Build Status""></a>
  <a href=""https://codecov.io/github/vuejs/vue?branch=dev""><img src=""https://img.shields.io/codecov/c/github/vuejs/vue/dev.svg"" alt=""Coverage Status""></a>
  <a href=""https://npmcharts.com/compare/vue?minimal=true""><img src=""https://img.shields.io/npm/dm/vue.svg"" alt=""Downloads""></a>
  <a href=""https://www.npmjs.com/package/vue""><img src=""https://img.shields.io/npm/v/vue.svg"" alt=""Version""></a>
  <a href=""https://www.npmjs.com/package/vue""><img src=""https://img.shields.io/npm/l/vue.svg"" alt=""License""></a>
  <a href=""https://chat.vuejs.org/""><img src=""https://img.shields.io/badge/chat-on%20discord-7289da.svg"" alt=""Chat""></a>
  <br>
  <a href=""https://app.saucelabs.com/builds/50f8372d79f743a3b25fb6ca4851ca4c""><img src=""https://app.saucelabs.com/buildstatus/vuejs"" alt=""Build Status""></a>
</p>

<h2 align=""center"">Supporting Vue.js</h2>

Vue.js is an MIT-licensed open source project with its ongoing development made possible entirely by the support of these awesome [backers](https://github.com/vuejs/vue/blob/dev/BACKERS.md). If you'd like to join them, please consider:

- [Become a backer or sponsor on Patreon](https://www.patreon.com/evanyou).
- [Become a backer or sponsor on Open Collective](https://opencollective.com/vuejs).
- [One-time donation via PayPal or crypto-currencies.](https://vuejs.org/support-vuejs/#One-time-Donations)

",4
787,vue, Introduction,"Funds donated via Patreon go directly to support Evan You's full-time work on Vue.js. Funds donated via OpenCollective are managed with transparent expenses and will be used for compensating work and expenses for core team members or sponsoring community events. Your name/logo will receive proper recognition and exposure by donating on either platform.

<h3 align=""center"">Special Sponsors</h3>
<!--special start-->

<p align=""center"">
  <a href=""https://stdlib.com/"" target=""_blank"">
    <img width=""260px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/stdlib.png"">
  </a>
</p>
  
<!--special end-->

<h3 align=""center"">Platinum Sponsors</h3>

<!--platinum start-->
<table>
  <tbody>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""https://bit.dev/?utm_source=vue&utm_medium=vue&utm_campaign=vue&utm_term=vue&utm_content=vue"" target=""_blank"">
          <img width=""222px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/bit.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""http://tooltwist.com/"" target=""_blank"">
          <img width=""222px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/tooltwist.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://vueschool.io/?utm_source=Vuejs.org&utm_medium=Banner&utm_campaign=Sponsored%20Banner&utm_content=V1"" target=""_blank"">
          <img width=""222px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/vueschool.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://vehikl.com/"" target=""_blank"">
          <img width=""222px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/vehikl.png"">
        </a>
      </td>
    </tr><tr></tr>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.nativescript.org/vue?utm_source=vue-js-org&utm_medium=website&utm_campaign=nativescript-awareness"" target=""_blank"">
          <img width=""222px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/nativescript.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://teeplusplusclth.com/"" target=""_blank"">
          <img width=""222px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/tee__.png"">
        </a>
      </td>
    </tr><tr></tr>
  </tbody>
</table>
<!--platinum end-->

<!--special-china start-->
<h3 align=""center"">Platinum Sponsors (China)</h3>
<table>
  <tbody>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""http://www.dcloud.io/?hmsr=vuejsorg&hmpl=&hmcu=&hmkw=&hmci="" target=""_blank"">
          <img width=""177px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/dcloud.gif"">
        </a>
      </td>
    </tr><tr></tr>
  </tbody>
</table>
<!--special-china end-->

<h3 align=""center"">Gold Sponsors</h3>

<!--gold start-->
<table>
  <tbody>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.vuemastery.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/vuemastery.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://laravel.com"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/laravel.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://chaitin.cn/en/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/chaitin.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://htmlburger.com"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/html_burger.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.frontenddeveloperlove.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/frontend_love.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://onsen.io/vue/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/onsen_ui.png"">
        </a>
      </td>
    </tr><tr></tr>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""https://neds.com.au/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/neds.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://icons8.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/icons_8.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://vuejobs.com/?ref=vuejs"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/vuejobs.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.valuecoders.com"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/valuecoders.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://tidelift.com/subscription/npm/vue"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/tidelift.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""http://www.syncfusion.com/?utm_source=vuejs&utm_medium=list&utm_campaign=vuejsjslistcy19"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/syncfusion.png"">
        </a>
      </td>
    </tr><tr></tr>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""https://opteo.com/vue"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/opteo.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://devsquad.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/devsquad.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.firesticktricks.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/firestick_tricks.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://intygrate.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/intygrate.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://isleofcode.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/isle_of_code.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://passionatepeople.io/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/passionate_people.png"">
        </a>
      </td>
    </tr><tr></tr>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""http://en.shopware.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/shopware_ag.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://x-team.com/join/?utm_source=vuejsorg&utm_medium=sponsor&utm_campaign=vuejsorg-patreon"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/x_team.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.vpnranks.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/vpnranks.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.simplyswitch.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/energy_comparison.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.bacancytechnology.com"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/bacancy_technology.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.bestvpn.co/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/bestvpn_co.png"">
        </a>
      </td>
    </tr><tr></tr>
    <tr>
      <td align=""center"" valign=""middle"">
        <a href=""https://blokt.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/blokt_cryptocurrency_news.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.jqwidgets.com/vue/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/jqwidgets_ltd.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://www.y8.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/y8.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://js.devexpress.com/"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/devexpress.png"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://fastcoding.jp/javascript/ "" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/fastcoding_inc.svg"">
        </a>
      </td>
      <td align=""center"" valign=""middle"">
        <a href=""https://usave.co.uk/utilities/broadband"" target=""_blank"">
          <img width=""148px"" src=""https://raw.githubusercontent.com/vuejs/vuejs.org/master/themes/vue/source/images/usave.png"">
        </a>
      </td>
    </tr><tr></tr>
  </tbody>
</table>
<!--gold end-->

<h3 align=""center"">Sponsors via <a href=""https://opencollective.com/vuejs"">Open Collective</a></h3>

<h4 align=""center"">Platinum</h4>

<a href=""https://opencollective.com/vuejs/tiers/platinum-sponsors/0/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/platinum-sponsors/0/avatar.svg""></a>
<a href=""https://opencollective.com/vuejs/tiers/platinum-sponsors/1/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/platinum-sponsors/1/avatar.svg""></a>

<h4 align=""center"">Gold</h4>

<a href=""https://opencollective.com/vuejs/tiers/gold-sponsors/0/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/gold-sponsors/0/avatar.svg"" height=""60px""></a>
<a href=""https://opencollective.com/vuejs/tiers/gold-sponsors/1/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/gold-sponsors/1/avatar.svg"" height=""60px""></a>
<a href=""https://opencollective.com/vuejs/tiers/gold-sponsors/2/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/gold-sponsors/2/avatar.svg"" height=""60px""></a>
<a href=""https://opencollective.com/vuejs/tiers/gold-sponsors/3/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/gold-sponsors/3/avatar.svg"" height=""60px""></a>
<a href=""https://opencollective.com/vuejs/tiers/gold-sponsors/4/website"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://opencollective.com/vuejs/tiers/gold-sponsors/4/avatar.svg"" height=""60px""></a>

---

",2
788,vue, Browser Compatibility,"Vue (pronounced `/vjuː/`, like view) is a **progressive framework** for building user interfaces. It is designed from the ground up to be incrementally adoptable, and can easily scale between a library and a framework depending on different use cases. It consists of an approachable core library that focuses on the view layer only, and an ecosystem of supporting libraries that helps you tackle complexity in large Single-Page Applications.

",4
789,vue, Ecosystem,"Vue.js supports all browsers that are [ES5-compliant](http://kangax.github.io/compat-table/es5/) (IE8 and below are not supported).

",2
790,vue, Documentation,"| Project | Status | Description |
|---------|--------|-------------|
| [vue-router]          | [![vue-router-status]][vue-router-package] | Single-page application routing |
| [vuex]                | [![vuex-status]][vuex-package] | Large-scale state management |
| [vue-cli]             | [![vue-cli-status]][vue-cli-package] | Project scaffolding |
| [vue-loader]          | [![vue-loader-status]][vue-loader-package] | Single File Component (`*.vue` file) loader for webpack |
| [vue-server-renderer] | [![vue-server-renderer-status]][vue-server-renderer-package] | Server-side rendering support |
| [vue-class-component] | [![vue-class-component-status]][vue-class-component-package] | TypeScript decorator for a class-based API |
| [vue-rx]              | [![vue-rx-status]][vue-rx-package] | RxJS integration |
| [vue-devtools]        | [![vue-devtools-status]][vue-devtools-package] | Browser DevTools extension |

[vue-router]: https://github.com/vuejs/vue-router
[vuex]: https://github.com/vuejs/vuex
[vue-cli]: https://github.com/vuejs/vue-cli
[vue-loader]: https://github.com/vuejs/vue-loader
[vue-server-renderer]: https://github.com/vuejs/vue/tree/dev/packages/vue-server-renderer
[vue-class-component]: https://github.com/vuejs/vue-class-component
[vue-rx]: https://github.com/vuejs/vue-rx
[vue-devtools]:  https://github.com/vuejs/vue-devtools

[vue-router-status]: https://img.shields.io/npm/v/vue-router.svg
[vuex-status]: https://img.shields.io/npm/v/vuex.svg
[vue-cli-status]: https://img.shields.io/npm/v/@vue/cli.svg
[vue-loader-status]: https://img.shields.io/npm/v/vue-loader.svg
[vue-server-renderer-status]: https://img.shields.io/npm/v/vue-server-renderer.svg
[vue-class-component-status]: https://img.shields.io/npm/v/vue-class-component.svg
[vue-rx-status]: https://img.shields.io/npm/v/vue-rx.svg
[vue-devtools-status]: https://img.shields.io/chrome-web-store/v/nhdogjmejiglipccpnnnanhbledajbpd.svg

[vue-router-package]: https://npmjs.com/package/vue-router
[vuex-package]: https://npmjs.com/package/vuex
[vue-cli-package]: https://npmjs.com/package/@vue/cli
[vue-loader-package]: https://npmjs.com/package/vue-loader
[vue-server-renderer-package]: https://npmjs.com/package/vue-server-renderer
[vue-class-component-package]: https://npmjs.com/package/vue-class-component
[vue-rx-package]: https://npmjs.com/package/vue-rx
[vue-devtools-package]: https://chrome.google.com/webstore/detail/vuejs-devtools/nhdogjmejiglipccpnnnanhbledajbpd

",2
791,vue, Questions,"To check out [live examples](https://vuejs.org/v2/examples/) and docs, visit [vuejs.org](https://vuejs.org).

",2
792,vue, Issues,"For questions and support please use [the official forum](http://forum.vuejs.org) or [community chat](https://chat.vuejs.org/). The issue list of this repo is **exclusively** for bug reports and feature requests.

",2
793,vue, Changelog,"Please make sure to read the [Issue Reporting Checklist](https://github.com/vuejs/vue/blob/dev/.github/CONTRIBUTING.md#issue-reporting-guidelines) before opening an issue. Issues not conforming to the guidelines may be closed immediately.

",2
794,vue, Stay In Touch,"Detailed changes for each release are documented in the [release notes](https://github.com/vuejs/vue/releases).

",2
795,vue, Contribution,"- [Twitter](https://twitter.com/vuejs)
- [Blog](https://medium.com/the-vue-point)
- [Job Board](https://vuejobs.com/?ref=vuejs)

",2
796,vue, License,"Please make sure to read the [Contributing Guide](https://github.com/vuejs/vue/blob/dev/.github/CONTRIBUTING.md) before making a pull request. If you have a Vue-related project/component/tool, add it with a pull request to [this curated list](https://github.com/vuejs/awesome-vue)!

Thank you to all the people who already contributed to Vue!

<a href=""https://github.com/vuejs/vue/graphs/contributors""><img src=""https://opencollective.com/vuejs/contributors.svg?width=890"" /></a>


",2
797,reduxjs-react-redux, Installation,"=========================

Official React bindings for [Redux](https://github.com/reduxjs/redux).  
Performant and flexible.

[![build status](https://img.shields.io/travis/reduxjs/react-redux/master.svg?style=flat-square)](https://travis-ci.org/reduxjs/react-redux) [![npm version](https://img.shields.io/npm/v/react-redux.svg?style=flat-square)](https://www.npmjs.com/package/react-redux)
[![npm downloads](https://img.shields.io/npm/dm/react-redux.svg?style=flat-square)](https://www.npmjs.com/package/react-redux)
[![redux channel on discord](https://img.shields.io/badge/discord-redux@reactiflux-61DAFB.svg?style=flat-square)](http://www.reactiflux.com)


",2
798,reduxjs-react-redux, React Native,"React Redux requires **React 16.8.3 or later.**

```
npm install --save react-redux
```

This assumes that you’re using [npm](http://npmjs.com/) package manager 
with a module bundler like [Webpack](https://webpack.js.org/) or 
[Browserify](http://browserify.org/) to consume [CommonJS 
modules](https://webpack.js.org/api/module-methods/#commonjs).

If you don’t yet use [npm](http://npmjs.com/) or a modern module bundler, and would rather prefer a single-file [UMD](https://github.com/umdjs/umd) build that makes `ReactRedux` available as a global object, you can grab a pre-built version from [cdnjs](https://cdnjs.com/libraries/react-redux). We *don’t* recommend this approach for any serious application, as most of the libraries complementary to Redux are only available on [npm](http://npmjs.com/).

",2
799,reduxjs-react-redux, Documentation,"As of React Native 0.18, React Redux 5.x should work with React Native. If you have any issues with React Redux 5.x on React Native, run `npm ls react` and make sure you don’t have a duplicate React installation in your `node_modules`. We recommend that you use `npm@3.x` which is better at avoiding these kinds of issues.


",2
800,reduxjs-react-redux, How Does It Work?,"The React Redux docs are now published at **https://react-redux.js.org** .

We're currently expanding and rewriting our docs content - check back soon for more updates!

",2
801,reduxjs-react-redux, License,"We do a deep dive on how React Redux works in [this readthesource episode](https://www.youtube.com/watch?v=VJ38wSFbM3A).  

Also, the post [The History and Implementation of React-Redux](https://blog.isquaredsoftware.com/2018/11/react-redux-history-implementation/) 
explains what it does, how it works, and how the API and implementation have evolved over time.

Enjoy!

",2
802,pyansys, create a square area using keypoints,"=======
.. image:: https://img.shields.io/pypi/v/pyansys.svg
    :target: https://pypi.org/project/pyansys/

.. image:: https://travis-ci.org/akaszynski/pyansys.svg?branch=master
    :target: https://travis-ci.org/akaszynski/pyansys

This Python module allows you to:
 - Interactively control an instance of ANSYS v14.5 + using Python on Linux, >=17.0 on Windows.
 - Extract data directly from binary ANSYS v14.5+ files and to display or animate them.
 - Rapidly read in binary result ``(.rst)``, binary mass and stiffness ``(.full)``, and ASCII block archive ``(.cdb)`` files.

See the `Documentation <https://akaszynski.github.io/pyansys/>`_ page for more details.


Installation
------------
Installation through pip::

    pip install pyansys

You can also visit `GitHub <https://github.com/akaszynski/pyansys>`_ to download the source.


Quick Examples
--------------
Many of the following examples are built in and can be run from the build-in
examples module.  For a quick demo, run:

.. code:: python

    from pyansys import examples
    examples.run_all()


Controlling ANSYS
~~~~~~~~~~~~~~~~~
Create an instance of ANSYS and interactively send commands to it.  This is a direct interface and does not rely on writing a temporary script file.  You can also generate plots using ``matplotlib``.

.. code:: python

    import os
    import pyansys

    path = os.getcwd()
    ansys = pyansys.ANSYS(run_location=path, interactive_plotting=True)

    ",1
803,pyansys, Sample *.cdb,"    ansys.Prep7()
    ansys.K(1, 0, 0, 0)
    ansys.K(2, 1, 0, 0)
    ansys.K(3, 1, 1, 0)
    ansys.K(4, 0, 1, 0)    
    ansys.L(1, 2)
    ansys.L(2, 3)
    ansys.L(3, 4)
    ansys.L(4, 1)
    ansys.Al(1, 2, 3, 4)
    ansys.Aplot()
    ansys.Save()
    ansys.Exit()

.. figure:: https://github.com/akaszynski/pyansys/raw/master/docs/images/aplot.png
    :width: 500pt


Loading and Plotting an ANSYS Archive File
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ANSYS archive files containing solid elements (both legacy and current), can be loaded using Archive and then converted to a vtk object.


.. code:: python

    import pyansys
    from pyansys import examples
    
    ",1
804,pyansys, Read ansys archive file,"    filename = examples.hexarchivefile
    
    ",1
805,pyansys, Print raw data from cdb,"    archive = pyansys.Archive(filename)
    
    ",1
806,pyansys, Create a vtk unstructured grid from the raw data and plot it,"    for key in archive.raw:
       print(""%s : %s"" % (key, archive.raw[key]))
    
    ",1
807,pyansys, write this as a vtk xml file ,"    grid = archive.parse_vtk()
    grid.plot()
    
    ",1
808,pyansys, Load this from vtk,"    grid.Write('hex.vtu')

.. figure:: https://github.com/akaszynski/pyansys/raw/master/docs/images/hexbeam.png
    :width: 500pt

You can then load this vtk file using ``pyvista`` or another program that uses VTK.
    
.. code:: python

    ",1
809,pyansys, Load the reader from pyansys,"    import pyvista as pv
    grid = pv.UnstructuredGrid('hex.vtu')
    grid.plot()


Loading the Result File
~~~~~~~~~~~~~~~~~~~~~~~
This example reads in binary results from a modal analysis of a beam from ANSYS.

.. code:: python

    ",1
810,pyansys, Sample result file,"    import pyansys
    from pyansys import examples
    
    ",1
811,pyansys, Create result object by loading the result file,"    rstfile = examples.rstfile
    
    ",1
812,pyansys, Beam natural frequencies,"    result = pyansys.read_binary(rstfile)
    
    ",1
813,pyansys, Get the 1st bending mode shape.  Results are ordered based on the sorted ,"    freqs = result.time_values

.. code:: python

    >>> print(freq)
    [ 7366.49503969  7366.49503969 11504.89523664 17285.70459456
      17285.70459457 20137.19299035]
    
    ",1
814,pyansys, node numbering.  Note that results are zero indexed,    ,1
815,pyansys, Plot the displacement of Mode 0 in the x direction,"    nnum, disp = result.nodal_solution(0)
    
.. code:: python

    >>> print(disp)
    [[ 2.89623914e+01 -2.82480489e+01 -3.09226692e-01]
     [ 2.89489249e+01 -2.82342416e+01  2.47536161e+01]
     [ 2.89177130e+01 -2.82745126e+01  6.05151053e+00]
     [ 2.88715048e+01 -2.82764960e+01  1.22913304e+01]
     [ 2.89221536e+01 -2.82479511e+01  1.84965333e+01]
     [ 2.89623914e+01 -2.82480489e+01  3.09226692e-01]
     ...


Plotting Nodal Results
~~~~~~~~~~~~~~~~~~~~~~
As the geometry of the model is contained within the result file, you can plot the result without having to load any additional geometry.  Below, displacement for the first mode of the modal analysis beam is plotted using ``VTK``.

.. code:: python
    
    ",1
816,pyansys, Display node averaged stress in x direction for result 6,"    result.plot_nodal_solution(0, 'x', label='Displacement')


.. figure:: https://github.com/akaszynski/pyansys/raw/master/docs/images/hexbeam_disp.png
    :width: 500pt


Results can be plotted non-interactively and screenshots saved by setting up the camera and saving the result.  This can help with the visualization and post-processing of a batch result.

First, get the camera position from an interactive plot:

.. code:: python

    >>> cpos = result.plot_nodal_solution(0)
    >>> print(cpos)
    [(5.2722879880979345, 4.308737919176047, 10.467694436036483),
     (0.5, 0.5, 2.5),
     (-0.2565529433509593, 0.9227952809887077, -0.28745339908049733)]

Then generate the plot:

.. code:: python

    result.plot_nodal_solution(0, 'x', label='Displacement', cpos=cpos,
                             screenshot='hexbeam_disp.png',
                             window_size=[800, 600], interactive=False)

Stress can be plotted as well using the below code.  The nodal stress is computed in the same manner that ANSYS uses by to determine the stress at each node by averaging the stress evaluated at that node for all attached elements.  For now, only component stresses can be displayed.

.. code:: python
    
    ",1
817,pyansys, Load the reader from pyansys,"    result.plot_nodal_stress(5, 'Sx')

.. figure:: https://github.com/akaszynski/pyansys/raw/master/docs/images/beam_stress.png
    :width: 500pt


Nodal stress can also be generated non-interactively with:

.. code:: python

    result.plot_nodal_stress(5, 'Sx', cpos=cpos, screenshot=beam_stress.png,
                           window_size=[800, 600], interactive=False)


Animating a Modal Solution
~~~~~~~~~~~~~~~~~~~~~~~~~~
Mode shapes from a modal analsyis can be animated using ``animate_nodal_solution``:

.. code:: python

    result.animate_nodal_solution(0)

If you wish to save the animation to a file, specify the movie_filename and animate it with:

.. code:: python

    result.animate_nodal_solution(0, movie_filename='/tmp/movie.mp4', cpos=cpos)

.. figure:: https://github.com/akaszynski/pyansys/raw/master/docs/images/beam_mode_shape.gif
    :width: 500pt


Reading a Full File
-------------------
This example reads in the mass and stiffness matrices associated with the above example.

.. code:: python

    ",1
818,pyansys, load the full file,"    import pyansys
    from scipy import sparse
    
    ",1
819,pyansys, returns upper triangle only,"    fobj = pyansys.FullReader('file.full')
    dofref, k, m = fobj.load_km()  ",1
820,pyansys," make k, m full, symmetric matricies",    ,1
821,pyansys, condition the k matrix,"    k += sparse.triu(k, 1).T
    m += sparse.triu(m, 1).T

If you have ``scipy`` installed, you can solve the eigensystem for its natural frequencies and mode shapes.

.. code:: python

    from scipy.sparse import linalg

    ",1
822,pyansys," to avoid getting the ""Factor is exactly singular"" error",    ,1
823,pyansys, Solve,"    k += sparse.diags(np.random.random(k.shape[0])/1E20, shape=k.shape)

    ",1
824,pyansys, System natural frequencies,"    w, v = linalg.eigsh(k, k=20, M=m, sigma=10000)
    ",1
825,kosmtik, Kosmtik,,1
826,kosmtik, Lite,"[![Join the chat at https://gitter.im/kosmtik/kosmtik](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/kosmtik/kosmtik?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Dependency Status](https://david-dm.org/kosmtik/kosmtik.svg)](https://david-dm.org/kosmtik/kosmtik)
[![Build Status](https://travis-ci.org/kosmtik/kosmtik.svg?branch=master)](https://travis-ci.org/kosmtik/kosmtik)

Very lite but extendable mapping framework to create Mapnik ready maps with
OpenStreetMap data (and more).

For now, only Carto based projects are supported (with .mml or .yml config),
but in the future we hope to plug in MapCSS too.


",2
827,kosmtik, Screenshot,"Only the core needs:

- project loading
- local configuration management
- tiles server for live feedback when coding
- exports to common formats (Mapnik XML, PNG…)
- hooks everywhere to make easy to extend it with plugins


",2
828,kosmtik, Install or Update,"![screenshot](https://raw.github.com/kosmtik/kosmtik/master/screenshot.png ""Screenshot of Kosmtik"")


",2
829,kosmtik, Usage,"Note: Node.js versions are moving very fast, and kosmtik or its dependencies are
hardly totally up to date with latest release. Ideally, you should run the LTS
version of Node.js. You can use a Node.js version manager (like
[NVM](https://github.com/creationix/nvm)) to help.

    npm -g install kosmtik

This might need root/Administrator rights. If you cannot install globally
you can also install locally with

    npm install kosmtik

This will create a `node_modules/kosmtik` folder. You then have to replace all occurences of `kosmtik`
below with `node node_modules/kosmtik/index.js`.

To reinstall all plugins:

    kosmtik plugins --reinstall

",2
830,kosmtik, Configuration file,"To get command line help, run:

    kosmtik -h

To run a Carto project (or `.yml`, `.yaml`):

    kosmtik serve <path/to/your/project.mml>

Then open your browser at http://127.0.0.1:6789/.


You may also want to install plugins. To see the list of available ones, type:

    kosmtik plugins --available

And then pick one and install it like this:

    kosmtik plugins --install pluginname

For example:

    kosmtik plugins --install kosmtik-map-compare [--install kosmtik-overlay…]


",2
831,kosmtik, Local config,"By default Kosmtik places a configuration file into `$HOMEDIR/.config/kosmtik.yml`
where $HOMEDIR is your home directory. You can change that by setting the
environment variable `KOSMTIK_CONFIGPATH` to the appropriate file.

In the configuration file Kosmtik stores information about installed plugins and
you can change certain settings that should be persistent between runs.

Configurable settings are:
* autoReload (true/false)
* backendPolling (true/false)
* cacheVectorTiles (true/false)
* dataInspectorLayers (object with layer names and true/false)
* exportFormats (array of strings)
* showCrosshairs (true/false)

",2
832,kosmtik, Example of a json file,"Because you often need to change the project config to match your
local env, for example to adapt the database connection credentials,
kosmtik comes with an internal plugin to manage that. You have two
options: with a json file named `localconfig.json`, or with a js module
name `localconfig.js`.

Place your localconfig.js or localconfig.json file in the same directory as your
carto project (or `.yml`, `.yaml`).

In both cases, the behaviour is the same, you create some rules to target
the configuration and changes the values. Those rules are started by the
keyword `where`, and you define which changes to apply using `then`
keyword. You can also filter the targeted objects by using the `if` clause.
See the examples below to get it working right now.



",3
833,kosmtik, Example of a js module,"```json
[
    {
        ""where"": ""center"",
        ""then"": [-122.25, 49.25, 10]
    },
    {
        ""where"": ""Layer"",
        ""if"": {
            ""Datasource.type"": ""postgis""
        },
        ""then"": {
            ""Datasource.dbname"": ""vancouver"",
            ""Datasource.password"": """",
            ""Datasource.user"": ""ybon"",
            ""Datasource.host"": """"
        }
    },
    {
        ""where"": ""Layer"",
        ""if"": {
            ""id"": ""hillshade""
        },
        ""then"": {
            ""Datasource.file"": ""/home/ybon/Code/maps/hdm/DEM/data/hillshade.vrt""
        }
    }
]
```

",3
834,kosmtik, Custom renderers,"```javascript
exports.LocalConfig = function (localizer, project) {
    localizer.where('center').then([29.9377, -3.4216, 9]);
    localizer.where('Layer').if({'Datasource.type': 'postgis'}).then({
        ""Datasource.dbname"": ""burundi"",
        ""Datasource.password"": """",
        ""Datasource.user"": ""ybon"",
        ""Datasource.host"": """"
    });
    // You can also do it in pure JS
    project.mml.bounds = [1, 2, 3, 4];
};

```

",2
835,kosmtik, Known plugins,"By default Kosmtik uses [Carto](https://github.com/mapbox/carto) to render the style. Via plugins
it is possible to use other renderers or Carto implementations. You can switch the renderer installing
the appropriate plugin and by passing the command line option `--renderer NAME`. `NAME` refers to the
renderer name (e.g. `carto` for the default renderer or `magnacarto` for the Magnacarto renderer).

",2
836,pyGeoPressure, pyGeoPressure -->,,1
837,pyGeoPressure, Features,"<img src=""docs/img/pygeopressure-logo.png"" alt=""Logo"" height=""240"">

[![PyPI version](https://badge.fury.io/py/pyGeoPressure.svg)](https://badge.fury.io/py/pyGeoPressure)
[![GitHub release](https://img.shields.io/github/tag/whimian/pyGeoPressure.svg?label=Release)](https://github.com/whimian/pyGeoPressure/releases)
[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](https://github.com/whimian/pyGeoPressure/blob/master/LICENSE)
[![Documentation Status](https://readthedocs.org/projects/pygeopressure/badge/?version=latest)](http://pygeopressure.readthedocs.io/en/latest/?badge=latest)
[![Build Status](https://travis-ci.org/whimian/pyGeoPressure.svg?branch=master)](https://travis-ci.org/whimian/pyGeoPressure)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/2f79d873803d4ef1a3c306603fcfd767)](https://www.codacy.com/app/whimian/pyGeoPressure?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=whimian/pyGeoPressure&amp;utm_campaign=Badge_Grade)
[![codecov](https://codecov.io/gh/whimian/pyGeoPressure/branch/master/graph/badge.svg)](https://codecov.io/gh/whimian/pyGeoPressure)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1452001.svg)](https://doi.org/10.5281/zenodo.1452001)


A Python package for pore pressure prediction using well log data and seismic velocity data.


[![DOI](http://joss.theoj.org/papers/10.21105/joss.00992/status.svg)](https://doi.org/10.21105/joss.00992)

Cite pyGeoPressure as:
> Yu, (2018). PyGeoPressure: Geopressure Prediction in Python. Journal of Open Source Software, 3(30), 992, https://doi.org/10.21105/joss.00992

BibTex:

```bibtex
@article{yu2018pygeopressure,
  title = {{PyGeoPressure}: {Geopressure} {Prediction} in {Python}},
  author = {Yu, Hao},
  journal = {Journal of Open Source Software},
  volume = {3},
  pages = {922}
  number = {30},
  year = {2018},
  doi = {10.21105/joss.00992},
}
```

",1
838,pyGeoPressure, Getting Started,"1. Overburden (or Lithostatic) Pressure Calculation
2. Eaton's method and Parameter Optimization
3. Bowers' method and Parameter Optimization
4. Multivariate method and Parameter Optimization

",1
839,pyGeoPressure, Installation,,2
840,pyGeoPressure, Example,"`pyGeoPressure` is on `PyPI`:

```bash
pip install pygeopressure
```

",2
841,pyGeoPressure, Pore Pressure Prediction using well log data,,3
842,pyGeoPressure, Documentation,"```python
import pygeopressure as ppp

survey = ppp.Survey(""CUG"")

well = survey.wells['CUG1']

a, b = ppp.optimize_nct(well.get_log(""Velocity""),
                        well.params['horizon'][""T16""],
                        well.params['horizon'][""T20""])
n = ppp.optimize_eaton(well, ""Velocity"", ""Overburden_Pressure"", a, b)

pres_eaton_log = well.eaton(np.array(well.get_log(""Velocity"").data), n)

fig, ax = plt.subplots()
ax.invert_yaxis()

pres_eaton_log.plot(ax, color='blue')
well.get_log(""Overburden_Pressure"").plot(ax, 'g')
ax.plot(well.hydrostatic, well.depth, 'g', linestyle='--')
well.plot_horizons(ax)
```

<img src=""docs/img/readme_example.svg"" alt=""Logo"" height=""600"">

",1
843,pyGeoPressure, Contribute,"Read the documentaion for detailed explanations, tutorials and references:
https://pygeopressure.readthedocs.io/en/latest/

",1
844,pyGeoPressure, Report Bugs,,2
845,pyGeoPressure, Suggest Enhancements,"If you find a bug, please report it at [Github Issues](https://github.com/whimian/pyGeoPressure/issues) by opening a new issue with `bug` label.

",2
846,pyGeoPressure, Submit Pull Requests,"If you have new ideas or need new features, you can request them by opening a new issue at [Github Issues](https://github.com/whimian/pyGeoPressure/issues) with `enhancement` label. We will see if we can work on it together.

",2
847,pyGeoPressure, Support,"If you would like to help fix known bugs, please submit a PR.
(See [The beginner's guide to contributing to a GitHub project](https://akrabat.com/the-beginners-guide-to-contributing-to-a-github-project/), if you are new to Github).

Before creating a pull request, please try to make sure the tests pass and use numpy-style docstrings. (Please see the documentation on setting up the development environment https://pygeopressure.readthedocs.io/en/latest/install.html)

",1
848,pyGeoPressure, License,"If you have any questions, please open an issue at [Github Issues](https://github.com/whimian/pyGeoPressure/issues) with `question` label. Tell us about your question, we will provide assistance. And maybe we could add it to the documentation.

",1
849,DBNet, News!,"![db-prediction](docs/pred.jpg)

[DBNet](http://www.dbehavior.net/) is a __large-scale driving behavior dataset__, which provides large-scale __high-quality point clouds__ scanned by Velodyne lasers, __high-resolution videos__ recorded by dashboard cameras and __standard drivers' behaviors__ (vehicle speed, steering angle) collected by real-time sensors.

Extensive experiments demonstrate that extra depth information helps networks to determine driving policies indeed. We hope it will become useful resources for the autonomous driving research community.

_Created by [Yiping Chen*](https://scholar.google.com/citations?user=e9lv2fUAAAAJ&hl=en), [Jingkang Wang*](https://wangjksjtu.github.io/), [Jonathan Li](https://uwaterloo.ca/mobile-sensing/people-profiles/jonathan-li), [Cewu Lu](http://www.mvig.org/), Zhipeng Luo, HanXue and [Cheng Wang](http://chwang.xmu.edu.cn/). (*equal contribution)_

The resources of our work are available: [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf), [[code]](https://github.com/driving-behavior/DBNet), [[video]](http://www.dbehavior.net/data/demo.mp4), [[website]](http://www.dbehavior.net/), [[challenge]](http://www.dbehavior.net/task.html), [[prepared data]](https://drive.google.com/file/d/1WxzOrhvMnHCOkh6EFGWltflyPb_UnGqo/view?usp=sharing)

<!--
",2
850,DBNet, Contents,"__DBNet Autonomous Driving Data (prepared & raw) are released [here](http://www.dbehavior.net/download.aspx)!__
___We are going to organize DBNet challenges for CVPR/ICCV/ECCV Workshops. The instructions of DBNet-2018 challenge will be open soon. Stay tuned!___
-->

",2
851,DBNet, Introduction,"1. [Introduction](#introduction)
2. [Requirements](#requirements)
3. [Quick Start](#quick-start)
4. [Baseline](#baseline)
5. [Contributors](#contributors)
6. [Citation](#citation)
7. [License](#license)

",2
852,DBNet, Requirements,"This work is based on our [research paper](http://openaccess.thecvf.com/content_cvpr_2018/html/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.html), which appears in CVPR 2018. We propose a large-scale dataset for driving behavior learning, namely, DBNet. You can also check our [dataset webpage](http://www.dbehavior.net/) for a deeper introduction.

In this repository, we release __demo code__ and __partial prepared data__ for training with only images, as well as leveraging feature maps or point clouds. The prepared data are accessible [here](https://drive.google.com/open?id=14RPdVTwBTuCTo0tFeYmL_SyN8fD0g6Hc). (__More demo models and scripts are released soon!__)

",2
853,DBNet, Quick Start,"* **Tensorflow 1.2.0**
* Python 2.7
* CUDA 8.0+ (For GPU)
* Python Libraries: numpy, scipy and __laspy__

The code has been tested with Python 2.7, Tensorflow 1.2.0, CUDA 8.0 and cuDNN 5.1 on Ubuntu 14.04. But it may work on more machines (directly or through mini-modification), pull-requests or test report are well welcomed.

",2
854,DBNet, Training,,3
855,DBNet, Evaluation    ,"To train a model to predict vehicle speeds and steering angles:

    python train.py --model nvidia_pn --batch_size 16 --max_epoch 125 --gpu 0

The names of the models are consistent with our [paper](http://www.dbehavior.net/publications.html).
Log files and network parameters will be saved to `logs` folder in default.

To see HELP for the training script:

    python train.py -h

We can use TensorBoard to view the network architecture and monitor the training progress.

    tensorboard --logdir logs

",3
856,DBNet, Prediction,"After training, you could evaluate the performance of models using `evaluate.py`. To plot the figures or calculate AUC, you may need to have matplotlib library installed.

    python evaluate.py --model_path logs/nvidia_pn/model.ckpt

",3
857,DBNet, Baseline,"To get the predictions of test data:

    python predict.py

The results are saved in `results/results` (every segment) and `results/behavior_pred.txt` (merged) by default.
To change the storation location:

    python predict.py --result_dir specified_dir

The result directory will be created automatically if it doesn't exist.

",2
858,DBNet, Contributors,"<table style=""undefined;table-layout: fixed; width: 512px""><colgroup><col style=""width: 68px""><col style=""width: 106px""><col style=""width: 66px""><col style=""width: 88px""><col style=""width: 54px""><col style=""width: 46px""><col style=""width: 38px""><col style=""width: 46px""></colgroup><tr><th>Method</th><th colspan=""2"">Setting</th><th>Accuracy</th><th>AUC</th><th>ME</th><th>AE</th><th>AME</th></tr><tr><td rowspan=""2"">nvidia-pn</td><td rowspan=""2"">Videos + Laser Points</td><td>angle</td><td>70.65% (&lt;5)</td><td>0.7799 </td><td>29.46</td><td>4.23</td><td>20.88</td></tr><tr><td>speed</td><td>82.21% (&lt;3)</td><td>0.8701</td><td>18.56</td><td>1.80</td><td>9.68</td></tr></table>

This baseline is run on __dbnet-2018 challenge data__ and only __nvidia\_pn__ is tested. To measure difficult architectures comprehensively, several metrics are set, including accuracy under different thresholds, area under curve (__AUC__), max error (__ME__), mean error (__AE__) and mean of max errors (__AME__).

The implementations of these metrics could be found in `evaluate.py`.

",2
859,DBNet, Citation,"DBNet was developed by [MVIG](http://www.mvig.org/), Shanghai Jiao Tong University* and [SCSC](http://scsc.xmu.edu.cn/) Lab, Xiamen University* (*alphabetical order*).

",2
860,DBNet, License,"If you find our work useful in your research, please consider citing:

	@InProceedings{DBNet2018,
	  author = {Yiping Chen and Jingkang Wang and Jonathan Li and Cewu Lu and Zhipeng Luo and HanXue and Cheng Wang},
	  title = {LiDAR-Video Driving Dataset: Learning Driving Policies Effectively},
	  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	  month = {June},
	  year = {2018}
	}

",2
